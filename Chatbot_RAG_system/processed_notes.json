[
  {
    "text": "3.1. Linear Basis Function Models\n- Linear basis function models are extensions of simple linear regression.\n- a linear function of the input variables xi, and this imposes significant limitations on the model. We therefore extend the class of models by considering\nlinear combinations of fixed nonlinear functions of the input variables\n- Instead of working directly with the input variables (𝑥1,x2,…), these models transform inputs using functions called basis functions (𝜙𝑗(𝑥)).",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk0",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "This form remains linear in the parameters , even if basis is nonlinear and the model can capture complex relationships in the data.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk1",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Examples of Basis Functions:\nGaussian:  where μj​ controls the center and s controls the spread.\nSigmoidal , ploynomial",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk2",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.1.1 Maximum likelihood and least squares\n- it expect that the noise data t is gaussion?\n- Maximizing the log-likelihood with respect to w is equivalent to minimizing the sum-of-squares error function ED(w).\n- The precision parameter 𝛽 (or its inverse, the noise variance) can also be estimated, which is the residual variance of the target values around the model's predictions.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk3",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.1.2: Geometry of Least Squares\nThe least-squares solution has a natural interpretation in an N-dimensional space, represents the target values as a vector.\nThe error minimization process (least-squares) corresponds to finding the projection of 𝑡 onto 𝑆 ensuring that 𝑦 is the closest point to 𝑡 in this subspace.\nThe least-squares solution achieves this by making the residual (difference between 𝑡 and 𝑦) orthogonal to 𝑆",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk4",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "The residuals are the vertical distances from the points to the line. The least-squares solution ensures that these residuals are perpendicular to the line, which geometrically corresponds to finding the closest line to all points.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk5",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.1.3: Sequential Learning\n- Batch Learning: Involves processing the entire dataset at once, which can be computationally expensive for large datasets.\n- Sequential Learning: Processes data one point at a time, updating the model incrementally.\n- Stochastic Gradient Descent (SGD)\nThe sequential learning method is based on stochastic gradient descent (SGD), which minimizes the error function by updating the parameter vector 𝑤 iteratively for subset of data:",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk6",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.1.4: Regularized Least Squares\nadding a regularization term to an error function in order to control over-fitting\n- known in the machine learning literature as weight decay because in sequential learning algorithms, it encourages weight values to decay towards zero, unless supported by the data.\n- Alternative Regularization Terms: More general forms, such as \nM\n∑\n𝑗=1 ∣𝑤𝑗∣𝑞,\ncan be used for regularization.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk7",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "M\n∑\n𝑗=1 ∣𝑤𝑗∣𝑞,\ncan be used for regularization.\nFor q=1, the method is called lasso, which promotes sparsity by driving some weights to zero.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk8",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.1.5: Multiple Outputs\nIn some applications, instead of predicting a single target variable 𝑡, you might need to predict multiple outputs simultaneously. These target variables are represented as a vector 𝑡 with 𝐾>1 components.\n- y(x,w)=WTϕ(x),where:\n𝑦(𝑥,𝑤) is a 𝐾-dimensional output vector. \n𝑊 is an 𝑀×𝐾 parameter matrix. \n𝜙(𝑥) is an 𝑀-dimensional vector of basis functions.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk9",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.2: The Bias-Variance Decomposition\nExpected Squared Loss: The goal is to minimize the expected squared loss between predictions 𝑦(𝑥) and the true regression function ℎ(𝑥)  first term measures the accuracy of predictions, while the second term represents the intrinsic noise in the data.\n- The squared loss is broken into three components: Expected Loss=(Bias)2+Variance+Noise.\nBias: Measures how far the average prediction is from the true function ℎ(𝑥).",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk10",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Variance: Reflects the sensitivity of the model to different training data sets.\nNoise: Represents the irreducible error due to randomness in the data.\n- There is a trade-off between bias and variance:\nFlexible models (e.g., high-degree polynomials) have low bias but high variance.\nRigid models (e.g., simple linear functions) have high bias but low variance.\nThe optimal model balances bias and variance to minimize total error.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk11",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.3. Bayesian Linear Regression\n- Adding a regularization term to the log likelihood function means the effective model complexity can then be controlled by the value of the regularization coefficient,\n\n- We turn to a Bayesian treatment of linear regression, which will avoid the over-fitting problem of maximum likelihood, and which will also lead to automatic methods of determining model complexity using the training data alone.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk12",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.3.1 Parameter distribution\n-  Bayesian methods, which provide a principled way to incorporate prior knowledge and handle uncertainty in parameter estimates\nnoise precision parameter β as a known constant.\n- In Bayesian linear regression, the parameters 𝑤 of the model are treated as random variables.\n- A prior distribution is assigned to 𝑤, reflecting our initial belief about its values before observing any data.\n- A Gaussian prior is commonly used: 𝑝(𝑤)=𝑁(𝑤∣𝑚0,𝑆0),",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk13",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "where 𝑚0 is the prior mean and 𝑆0 is the prior covariance matrix.\n-  prior is updated to a posterior distribution using Bayes' theorem: p(w∣t)∝p(t∣w)p(w),\nwhere  p(t∣w) is the likelihood of the data given w.\nThe posterior distribution is also Gaussian due to the conjugate properties of Gaussian priors and likelihoods\n- The posterior mean 𝑚𝑁 and covariance 𝑆𝑁 are computed as where Φ is the design matrix, 𝑡 is the vector of observed targets, and 𝛽 is the noise precision.\nSpecial Cases:",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk14",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Special Cases:\n- If the prior is broad (S0−1→0), the posterior mean reduces to the maximum likelihood solution.\n- When no data is available (N=0), the posterior reverts to the prior.\n- Sequential Updates: Bayesian learning allows for sequential updates: the posterior after observing one dataset serves as the prior for the next dataset.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk15",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.3.2: Predictive Distribution\n- The main goal is to predict the target value 𝑡 for a new input 𝑥, using the Bayesian framework. This involves marginalizing over the posterior distribution of the model parameters.\n- he predictive distribution is given by: \n\tintegrating Likelihood of the target variable given the model parameters. with Posterior distribution of the parameters.\n\tBecause both the likelihood and posterior are Gaussian, the predictive distribution is also Gaussian:",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk16",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "As more data points are observed, the posterior becomes more confident (narrower), and the second term in the variance diminishes.\n\tIn the limit of infinite data, the uncertainty arises solely from the noise.\n\tthe model's predictive uncertainty is higher in regions with fewer data points and reduces as more data is observed.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk17",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.3.3: Equivalent Kernel?\nThe equivalent kernel provides an interpretation of Bayesian linear regression in terms of kernel methods.\nIt describes how predictions are formed as a weighted combination of the training data's target values.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk18",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.4: Bayesian Model Comparison\nBayesian model comparison, which evaluates models using probabilities, providing a principled way to select among competing models based on observed data. Key points are:\nPosterior Model Probability\n1- The posterior probability of a model 𝑀𝑖 given data 𝐷 is:\n\tp(Mi): Prior probability, representing initial preference for the model. \n\t𝑝(𝐷∣𝑀𝑖): Model evidence or marginal likelihood, measures how well the model explains the data.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk19",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "2- Model Evidence\nThe model evidence 𝑝(𝐷∣𝑀𝑖) is computed by marginalizing over the model parameters 𝑤:\nThis integral balances:\n\tFit to the data: How well the model matches the observations.\n\tComplexity penalty: Penalizes overly complex models to avoid overfitting.\n\n3. Bayes Factor\nThe ratio of evidences for two models is called the Bayes factor It quantifies the relative evidence for two models based on the observed data.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk20",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4. Model Averaging\nRather than selecting a single model, predictions can be made by averaging over all models, weighted by their posterior probabilities\n\n5- Model Complexity Trade-off\nSimple models may underfit, providing poor data fit.\nComplex models may overfit, spreading probability across many data sets and penalizing evidence.\nThe optimal model balances complexity and fit.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk21",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "6-  General Insights\nBayesian model comparison avoids overfitting by incorporating a complexity penalty.\nPredictions and model evaluations are based on the training data, eliminating the need for a separate validation set.\nThe method is sensitive to prior choices and assumptions.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk22",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.6: Limitations of Fixed Basis Functions\n1. Dependency on Basis Functions Linear models rely on a fixed set of basis functions which transform the input 𝑥 into a new representation: Key limitations arise due to the choice and properties of these basis functions.\n\n2. Limitations\n\nModel Complexity:\nThe complexity of the model is determined by the number of basis functions 𝑀 A small 𝑀 may lead to underfitting, while a large 𝑀 may result in overfitting or numerical instability.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk23",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Choice of Basis Functions:\nThe performance of the model depends critically on the type of basis functions used (e.g., polynomials, Gaussians, sigmoids).\nPoorly chosen basis functions may fail to capture important data features.\n\nFixed Nature:\nFixed basis functions do not adapt to the data, which can limit their flexibility and effectiveness, especially for complex datasets.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk24",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Scalability:\nIn high-dimensional input spaces, the number of basis functions required grows rapidly, leading to computational challenges (the curse of dimensionality).\n\nExtrapolation Issues:\nFixed basis functions, such as localized Gaussians, may generalize poorly outside the region covered by the data, resulting in overconfidence in predictions in unobserved regions.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk25",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Feature Engineering Dependency:\nFixed basis function models often require extensive pre-processing or feature engineering to construct effective basis functions tailored to the problem.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk26",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "3.1.3: Sequential Learning\nSequential learning, based on stochastic gradient descent, updates model parameters incrementally with each data point, making it efficient for large datasets and real-time applications. While it offers adaptability and reduced computational demands, careful tuning of the learning rate is essential for stability and convergence.",
    "embedding": null,
    "source": "3.1. Linear Basis Function Models.txt",
    "chunk_id": "3.1. Linear Basis Function Models_chunk27",
    "meta_data": {
      "filename": "3.1. Linear Basis Function Models.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1 Discriminant Functions\nThis section introduces discriminant functions, which assign an input vector 𝑥 to one of 𝐾 classes 𝐶𝑘 based on decision boundaries. The focus is on linear discriminants, where decision surfaces are hyperplanes in the input space.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk0",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1.1 Two classes\ny(x) = wTx + w0\nw is called a weight vector, and w0 is a bias (not to be confused with bias in the statistical sense).\n- The negative of the bias is sometimes called a threshold. \n\t The orientation of the paper is entirely determined by this stick (vector 𝑤), while  𝑤0  decides where the paper is located relative to the origin.\nAn input vector (x) is assigned to (class C1) if y(x) = 0 and to class C2 otherwise.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk1",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1.2 Multiple classes\nNow consider the extension of linear discriminants to K >2 classes. We might be tempted be to build a K-class discriminant by combining a number of two-class discriminant functions.\nConsider the use of K−1 classifiers each of which solves a two-class problem of separating points in a particular class Ck from points not in that class. This is known as a one-versus-the-rest classifier",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk2",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "An alternative is to introduce K(K − 1)/2 binary discriminant functions, one for every possible pair of classes. This is known as a one-versus-one classifier. Eachpoint is then classified according to a majority vote amongst the discriminant functions.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk3",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "The section addresses a challenge in multi-class classification (𝐾>2), where combining multiple two-class discriminants (e.g., \"one-vs-rest\" or \"one-vs-one\") can result in ambiguous regions in the input space. To resolve this, a single unified 𝐾-class discriminant approach is proposed.\nThis means 𝑥 is classified into the class whose discriminant function 𝑦𝑘(𝑥) has the largest value.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk4",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1.3 Least Squares for Classification\nApplies the least-squares method (used for regression) to classification:\nMinimize a sum-of-squares error function to compute class assignments.\nNot ideal for classification due to:\nPoor approximation of probabilities.\nSensitivity to outliers.\nIneffective separation for complex datasets.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk5",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1.4 Fisher’s Linear Discriminant\nFisher’s Linear Discriminant is introduced as a method to reduce the dimensionality of data while maximizing the separation between classes. It projects high-dimensional data onto a lower-dimensional space (typically 1D for two-class problems) such that the separation between the classes is maximized, making classification easier.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk6",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Goal: Find a direction 𝑤 in the 𝐷-dimensional space such that, when the data is projected onto this direction:\nThe means of the projected classes are far apart (maximizing between-class separation).\nThe spread (variance) of the projected points within each class is minimized (minimizing within-class overlap).\n\nProjection of Data\nEach data point 𝑥 is projected onto the line defined by \n𝑤: y=w(T)x.\nThe means of the projected points for the two classes are:\n𝑚1′=𝑤(𝑇)𝑚1, 𝑚2′=𝑤(𝑇)𝑚2",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk7",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Once the data is projected onto 𝑤, the classes can be separated using a threshold on the projected values: 𝑦=𝑤(𝑇)𝑥.\nThe threshold can be chosen based on class distributions or by modeling the class-conditional densities of the projected data.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk8",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1.5 Relation to Least Squares\nFisher's discriminant can be viewed as a special case of least squares with specific target coding.\nIt offers better class separation than least squares.\nFisher’s criterion can be viewed as a specific case of least squares when a modified target coding scheme is used.\nThe least-squares solution provides both the direction 𝑤 (same as Fisher’s result) and the bias 𝑤0 for classification.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk9",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "However, unlike Fisher's method, least squares is sensitive to outliers and does not explicitly maximize the separation of class means.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk10",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1.6 Fisher’s Discriminant for Multiple Classes\nExtends Fisher’s criterion to K>2 classes. \nProjects data into a (K−1)-dimensional space to maximize class separability.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk11",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.1.7 Perceptron Algorithm\nThe perceptron algorithm is an early method for solving two-class classification problems using a linear discriminant model. It works by iteratively updating the weight vector 𝑤 to correctly classify training data points.\nTarget Coding:\nt=+1 for class C 1\nt=−1 for class 𝐶2\nThe perceptron criterion minimizes the error associated with misclassified points:\nwhere 𝑀 is the set of misclassified points.\nCorrectly classified points contribute zero to the error.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk12",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Misclassified points are penalized based on how far they are from being correctly classified.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk13",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Convergence:\nThe Perceptron Convergence Theorem states:\nIf the data is linearly separable, the perceptron algorithm will find a separating hyperplane in a finite number of steps.\nHowever, the convergence time can be significant if the data is close to being non-separable.\n\nNon-Separable Data:\nIf the data is not linearly separable, the algorithm never converges and continues updating indefinitely.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk14",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Dependence on Initialization:\nThe solution depends on the initial choice of weights and the order of data presentation.\n\nNo Probabilistic Interpretation:\nThe perceptron does not provide probabilistic outputs (e.g., class probabilities).",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk15",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.2 Probabilistic Generative Models\nThese models assume that data is generated by a specific probabilistic process and explicitly model the class-conditional distributions p(x∣Ck) and class priors 𝑝(𝐶𝑘). Predictions are made by using Bayes' theorem to compute the posterior probabilities 𝑝(𝐶𝑘∣𝑥)\n\nGenerative Approach:\n\nModels how data 𝑥 is generated for each class 𝐶k by specifying:\np(x∣Ck): Class-conditional densities.\np(Ck): Prior probabilities of each class.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk16",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Posterior probabilities are computed using Bayes' theorem\nFor two classes in terms of the log-odds\nFor 𝐾-classes, the posterior probability is given by the softmax function",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk17",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.2.1 Continuous Inputs\nIf the class-conditional densities are modeled as Gaussians\n- If all classes share the same covariance matrix Σ\n\tPosterior probabilities are logistic sigmoid functions of a linear function of 𝑥 (linear decision boundaries).\nFor K>2, decision boundaries are linear hyperplanes.\n- If classes have different covariance matrices Σ𝑘\n\tThe decision boundaries become quadratic functions of x (quadratic discriminant analysis).",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk18",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.2.2 Maximum Likelihood\nModel parameters (e.g., μk,Σ,p(Ck) are estimated using maximum likelihood:\nFor Gaussian p(x∣Ck ), this involves computing:\nClass means 𝜇𝑘\nCovariance matrix Σ\nClass priors p(Ck) as the fraction of data points in each class.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk19",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "The maximum likelihood method estimates the parameters of generative models by maximizing the likelihood of the training data. For Gaussian class-conditional densities with shared covariance, this results in linear decision boundaries. While efficient and interpretable, this approach heavily relies on assumptions about the data distribution and is sensitive to outliers.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk20",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.2.3 Discrete Inputs\nFor discrete features (e.g., binary or categorical data):\nUse a naive Bayes assumption, where features are conditionally independent given the class.\nf.y.i Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk21",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.2.4\nExponential Family:\nIf p(x∣Ck ) belongs to the exponential family (e.g., Gaussian, Poisson):\nPosterior probabilities are still logistic sigmoid (for two classes) or softmax functions (for multiple classes) of a linear function of 𝑥.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk22",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.2 general\nProbabilistic generative models describe how data is generated for each class and use Bayes' theorem for classification. Gaussian models result in linear or quadratic decision boundaries depending on whether the covariance is shared. For discrete data, the naive Bayes assumption simplifies the model. While powerful and interpretable, their performance depends heavily on accurate modeling of 𝑝(𝑥∣𝐶𝑘).",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk23",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.3 Probabilistic Discriminative Models\nprobabilistic discriminative models, which directly model the posterior probability  without requiring the explicit modeling of class-conditional densities.\n\nDiscriminative Approach:\nInstead of modeling p(x∣Ck), discriminative models directly parameterize p(Ck∣x), often using a generalized linear model.\nParameters are learned by maximum likelihood estimation based on the training data.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk24",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.3.1 Fixed Basis Functions\nThe input vector 𝑥 can be transformed into a feature vector ϕ(x) using fixed nonlinear basis functions.\nLinear Decision Boundaries:\nIn the transformed feature space ϕ(x), the decision boundaries are linear.\nThese correspond to nonlinear boundaries in the original input space 𝑥.\nHowever, we first make a fixed nonlinear transformation of the inputs using a\nvector of basis functions φ(x). The resulting decision boundaries will be linear in",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk25",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "the feature space φ, and these correspond to nonlinear decision boundaries in the\noriginal x space",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk26",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.3.2 Logistic Regression\nLogistic regression optimizes the separation of classes in terms of probabilities, unlike least squares.\nThe parameters w are estimated by maximizing the likelihood of the training data.\nThe error function (negative log-likelihood) \nLogistic regression directly models p(Ck∣x), requiring fewer parameters than generative models like Gaussian classifiers.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk27",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.3.3 Iterative Reweighted Least Squares (IRLS)\nLogistic regression does not have a closed-form solution due to the nonlinearity of the sigmoid function.\nThe Newton-Raphson method is used to optimize the error function iteratively.\n\tThis involves solving a sequence of weighted least-squares problems.\n\nusing:\n∇E(w): Gradient of the error function,\n𝐻: Hessian matrix (second derivative of ∇E(w))",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk28",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.3.4 Multiclass Logistic Regression (Summary)\nThis section extends logistic regression to handle classification problems with more than two classes (K>2) using the softmax function\nDecision Rule: Assign x to the class with the highest posterior probability\nThe parameters {𝑤𝑘} are learned using maximum likelihood estimation.\nThe error function (negative log-likelihood\nOptimization\nNo closed-form solution exists.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk29",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Optimization\nNo closed-form solution exists.\nOptimization is performed iteratively using techniques like gradient descent or Newton-Raphson.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk30",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 30,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.3.5 Probit Regression \nProbit Model: The posterior probability of class 𝐶1 is modeled using the cumulative Gaussian (probit) function\nwhile Logistic regression uses the logistic sigmoid function. Probit regression uses the probit (Gaussian CDF) function, which has a similar S-shaped curve.\nParameters w are learned via maximum log-likelihood",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk31",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 31,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.3.6 Canonical Link Functions\ncanonical link functions used in generalized linear models (GLMs), which relate the mean of the response variable to a linear combination of predictors.\nLink Function: In GLMs, the relationship between the expected value of the response variable and the linear predictor is defined by the link function\nCanonical Link Function: A link function is canonical if it is derived directly from the natural parameter of the exponential family distribution.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk32",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 32,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Multiclass Logistic Regression generalizes binary logistic regression to 𝐾-class problems using the softmax function.\nProbit Regression uses the cumulative Gaussian (probit) function instead of the logistic sigmoid and is more suited to certain datasets.\nCanonical Link Functions provide a natural and mathematically efficient way to relate predictors to response variables in GLMs, simplifying computations and aligning with exponential family distributions.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk33",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 33,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.4 The Laplace Approximation\nThe Laplace approximation is a method for approximating complex integrals, commonly used in Bayesian inference when exact solutions are intractable. \nThe Laplace approximation simplifies Bayesian inference by approximating the posterior distribution p(w∣t) as a Gaussian centered at 𝑤MAP. It is widely used in classification models, particularly for logistic regression, where exact Bayesian inference is computationally challenging",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk34",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 34,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.4.1 Model Comparison and BIC\nThe Laplace approximation enables the evaluation of model evidence, and the  BIC provides a computationally efficient approximation for comparing models. BIC balances model fit and complexity, favoring simpler models that explain the data well. While widely used, it assumes large datasets and Gaussian posteriors, which may not hold in all scenarios.",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk35",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 35,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "4.5 Bayesian Logistic Regression\nBayesian logistic regression provides a probabilistic interpretation and incorporates regularization naturally\nIn Bayesian logistic regression, the goal is to compute the posterior distribution over the parameters 𝑤 given the data:",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk36",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 36,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Bayesian logistic regression incorporates priors to provide a probabilistic framework for classification, improving interpretability and regularization. While the posterior distribution is intractable, approximation methods like the Laplace approximation make inference feasible. This approach is particularly valuable in scenarios where uncertainty quantification and robust regularization are important",
    "embedding": null,
    "source": "4.1 Discriminant Functions.txt",
    "chunk_id": "4.1 Discriminant Functions_chunk37",
    "meta_data": {
      "filename": "4.1 Discriminant Functions.txt",
      "chunk_index": 37,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.1. Feed-forward Network Functions\ny(x,w) = f\n(M\nsum wjφj(x))\nj=1    \nUnlike traditional linear models with fixed basis functions, neural networks allow adaptive basis functions, enabling them to learn from data more effectively.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk0",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Neural networks consist of multiple layers of neurons:\nInput layer: Receives input features.\nHidden layer(s): Applies nonlinear transformations.\nOutput layer: Produces the final prediction.\nEach layer computes a weighted sum of inputs, followed by a nonlinear activation function.\n\nActivation functions include sigmoid, tanh, and softmax, depending on the problem type.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk1",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Properties:\nUniversal Approximation Theorem: A two-layer neural network with enough hidden units can approximate any continuous function.\n\n5.1.1 Weight-space Symmetries\nWeight-space symmetries: Multiple configurations of weights can produce the same network output.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk2",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "2. Types of Weight-space Symmetries\n- Hidden Unit Sign-Flipping Symmetry\nIf we take a hidden unit j and change the sign of all the weights feeding into it, the activation a_j also flips its sign\nHowever, we can compensate for this sign change by flipping the sign of the corresponding second-layer weights: Wkj(2)→−Wkj(2)",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk3",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "This results in the same final output, meaning that the two sets of weights are functionally equivalent. Since each hidden unit can have its weights flipped independently, there are 2^M equivalent weight configurations for a network with M hidden units.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk4",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "-Hidden Unit Permutation Symmetry\nThe order of the hidden units does not matter, as long as the connections are adjusted accordingly.\nSuppose we swap the weights of two hidden units, say unit 1 and unit 2:\nSwap all first-layer weights connected to these units.\nSwap all second-layer weights leading out of these units.\nSince the output remains unchanged, we again have an equivalent weight configuration.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk5",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "With M hidden units, there are M! (M factorial) ways to permute them, meaning an additional M! symmetric weight configurations.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk6",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Why Weight-space Symmetries Matter\n(a) Implications for Optimization\nSince there are multiple equivalent solutions, gradient-based optimization methods like stochastic gradient descent (SGD) can converge to any of these symmetric minima.\nThe optimizer may explore different regions of weight-space that correspond to the same function.\n(b) Bayesian Neural Networks\nIn Bayesian approaches (discussed in Section 5.7), symmetries in weight space must be accounted for when computing model evidence.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk7",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "(c) Pruning and Interpretability\nWeight symmetries can make it harder to interpret the learned weights of a network because the same function can be represented in different ways.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk8",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.2: Network Training \nTraining a neural network means finding the best weights w to minimize an error function E(w), which measures how well the network’s predictions match the target outputs. This is typically done using gradient-based optimization methods",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk9",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "The choice of error function (or loss function) depends on the type of problem being solved:\n-Regression Problems This leads to the sum-of-squares error function\n- Classification Problems: \nThe choice of error function is based on the likelihood function\nBinary Classification : The cross-entropy error function (negative log-likelihood)\nMulticlass Classification: The network uses the softmax function for output also Cross-entropy loss",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk10",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "The Error Surface and Local Minima\nThe error function E(w) defines a surface over weight space.\nTraining involves finding a minimum of this surface.\nDifficulties in optimization:\nLocal Minima: Some weight settings lead to small but not the absolute lowest error.\nSaddle Points: Points where the gradient is zero but it’s neither a minimum nor maximum.\nPlateaus and Flat Regions: The error function changes slowly, making learning slow.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk11",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Since exactly solving \n∇E(w)=0 is infeasible, we use iterative methods:\n\n(A) Gradient Descent (Steepest Descent)\nThe simplest method updates weights using the gradient of the error function: w (t+1) =w (t) −η∇E(w)\nwhere η is the learning rate.\n\n(B) Stochastic Gradient Descent (SGD)\nInstead of computing the gradient over the entire dataset, we update weights after each data point",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk12",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "(C) Mini-Batch Gradient Descent\nUses a small batch of data points at each step, balancing between stability and efficiency.\n\n(D) More Advanced Methods\nMomentum: Helps the network keep moving in the right direction, avoiding slowdowns in plateaus.\nAdaptive Learning Rates: Methods like Adam, RMSprop, and Adagrad adjust learning rates dynamically.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk13",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Convergence and Practical Considerations\nWeight Initialization: Randomized small values to prevent neurons from becoming identical.\nLearning Rate Scheduling: Decreasing 𝜂 over time to stabilize training.\nEarly Stopping: Stops training when performance stops improving on a validation set to prevent overfitting.\nRegularization (Weight Decay): Adds a term like λ∣∣w∣∣2 to the error function to prevent overfitting",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk14",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.3: Error Backpropagation\nThe backpropagation algorithm is a fundamental method for training feed-forward neural networks. It efficiently computes the gradient of the error function with respect to the network's weights by propagating errors backward through the network. This allows us to use gradient-based optimization methods like Stochastic Gradient Descent (SGD) to update the weights.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk15",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.3.1 Evaluation of Error-function Derivatives\nThe goal is to compute the gradient of the error function with respect to the weights efficiently.\n- The gradient can be computed recursively using the chain rule.\n- The weight gradient is given by: ∂𝐸/∂𝑤𝑗𝑖=𝛿𝑗𝑧𝑖\nKey observation: The weight update depends on the error signal 𝛿𝑗 at the output end and the activation 𝑧𝑖  at the input end.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk16",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.3.2 A Simple Example\nTo illustrate backpropagation, we consider a two-layer neural network with:\nSigmoid hidden units: h(a)=tanh(a)\nLinear output units: y k=ak\n​Sum-of-squares error function\nBackward Pass\nCompute the error term at the output layer: 𝛿𝑘=𝑦𝑘−𝑡𝑘\nompute the error term at the hidden layer:\n𝛿𝑗=(1−𝑧𝑗2)∑ 𝑤𝑘𝑗(2)𝛿𝑘\n         𝑘 \nCompute the weight gradients:\nFirst-layer weights:∂E/∂Wji(1)= 𝛿jxi\nSecond-layer weights::∂E/∂Wkj(2)= 𝛿kzj",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk17",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.3.3 Efficiency of Backpropagation\nBackpropagation is computationally very efficient compared to numerical differentiation.\n Computational Complexity O(W)\n- Since the network has many weights, backpropagation is much faster.\n\n5.3.4 The Jacobian Matrix\nThe Jacobian matrix measures how the outputs of a neural network change with respect to inputs.\nwhere each entry represents how much the output 𝑦𝑘 changes when input 𝑥𝑖 is perturbed",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk18",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.4: The Hessian Matrix\n It is the second derivative of the error function with respect to the network's weights, providing insights into local minima, saddle points, and convergence speed\nAlthough the Hessian can improve training efficiency and model uncertainty estimation, computing and inverting it directly is expensive. Various approximations exist to make it practical.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk19",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.4.1 Diagonal Approximation\nSince the Hessian is a large matrix (𝑊×𝑊, where W is the number of weights), we often use a diagonal approximation, keeping only the second derivatives along the diagonal.\n- Off-diagonal terms (which show interactions between weights) are ignored.\n- The Hessian is cheap to compute because we only calculate individual second derivatives.\n- The inverse of a diagonal matrix is trivial, making this useful for optimization methods like Newton’s method.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk20",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.4.2 Outer Product Approximation (Levenberg-Marquardt Approximation)\nThis approximation assumes that the Hessian can be estimated using the outer product of gradients, leading to a simplified but effective estimation \n​\n5.4.3 Inverse Hessian Approximation\nThe Hessian’s inverse is important for many applications, including Bayesian inference and optimization. Direct inversion is computationally expensive, so approximations are used.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk21",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "The inverse Hessian helps determine optimal learning rate adjustments and model uncertainty.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk22",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.4.4 Eigenvalue Analysis of the Hessian\n🔹 Why Eigenvalues Matter?\nThe eigenvalues of the Hessian determine the curvature of the error function.\nLarge eigenvalues → sharp minima (high curvature) → fast convergence but potential instability.\nSmall eigenvalues → flat regions (low curvature) → slow convergence.\n🔹 Key Properties:\nThe Hessian is positive definite if all eigenvalues are positive, meaning the point is a local minimum.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk23",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "The Hessian is negitave definite if all eigenvalues are positive, meaning the point is a local maxima.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk24",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "If some eigenvalues are negative, the point is a saddle point (not a minimum).\nIf some eigenvalues are zero, there are flat directions, leading to slow learning.\n🔹 Practical Implications:\nLarge differences between the smallest and largest eigenvalues (high condition number) cause slow optimization.\nEigenvalue analysis helps in selecting adaptive learning rates (larger for flat regions, smaller for sharp minima).",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk25",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.4.5 The Hessian and Generalization\nThe Hessian provides insights into overfitting and model complexity.\n🔹 Connection to Model Complexity:\nA flat minimum (small eigenvalues) means the model is robust to small changes → better generalization.\nA sharp minimum (large eigenvalues) means small weight changes cause large errors → more prone to overfitting.\n🔹 Hessian-Based Regularization\nRegularization techniques like weight decay or Bayesian learning use Hessian properties to prevent overfitting.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk26",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Smaller eigenvalues in the Hessian suggest the network has learned useful features rather than memorizing noise.\n🔹 Bayesian View:\nIn Bayesian inference, the Hessian helps define a Gaussian approximation to the posterior over weights.\nThis leads to uncertainty quantification, allowing confidence estimates for predictions.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk27",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.4.6 Fast Approximation Methods\nBecause computing the full Hessian is expensive, several approximations are used in practice.\n🔹 Quasi-Newton Methods\nBFGS (Broyden-Fletcher-Goldfarb-Shanno) approximates the inverse Hessian iteratively.\nRequires less computation and is widely used in optimization.\n🔹 Krylov Subspace Methods\nApproximates dominant eigenvalues of the Hessian, useful for second-order optimization.\nUsed in Hessian-free optimization for deep learning.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk28",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "🔹 Stochastic Estimation of the Hessian\nUses mini-batches to approximate Hessian terms efficiently.\nBalances accuracy and computational cost.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk29",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5: Regularization in Neural Networks\nRegularization is a key technique in neural network training to prevent overfitting and improve generalization to unseen data. Overfitting occurs when a model becomes too complex and memorizes the training data rather than learning general patterns.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk30",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 30,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5.1. Parameter Regularization (Weight Decay)\n🔹 Why is Regularization Needed?\nNeural networks with many parameters can fit training data well but may perform poorly on new data.\nRegularization methods add constraints on the model parameters to prevent overfitting.\n🔹 L2 Regularization (Weight Decay)\nThe most common form of regularization is L2 regularization, which adds a penalty on the squared values of the weights: Ereg =E+ λ/2∑wj2",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk31",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 31,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "​λ is the regularization parameter (controls the penalty strength).\nL1 Regularization (Sparsity)\nInstead of squaring the weights, L1 regularization penalizes their absolute values:\n𝐸reg =𝐸+𝜆∑∣𝑤𝑗∣\nEffect: Encourages sparse weights, meaning many weights become zero.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk32",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 32,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5.2. Early Stopping\n🔹 Idea: Stop Training at the Right Time\nInstead of modifying the objective function, early stopping prevents overfitting by stopping training before the model memorizes noise.\nThe network is trained while monitoring validation loss:\nIf validation loss starts increasing while training loss keeps decreasing, stop training.\n🔹 Practical Implementation\nSplit data into training set and validation set.\nTrain the network and track validation loss at each epoch.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk33",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 33,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Stop training when validation loss starts increasing.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk34",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 34,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5.3. Dataset Augmentation\n🔹 Key Idea: Create More Training Data\nOverfitting happens when there’s too little data. A simple fix is to artificially generate more training samples.\n🔹 Methods for Data Augmentation\nFor images:\nRandom cropping, flipping, rotation, zooming, color adjustments.\nFor text:\nSynonym replacement, paraphrasing, back translation.\nFor speech/audio:\nTime stretching, pitch shifting, adding noise.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk35",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 35,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5.3. Dataset Augmentation\n🔹 Key Idea: Create More Training Data\nOverfitting happens when there’s too little data. A simple fix is to artificially generate more training samples.\n🔹 Methods for Data Augmentation\nFor images:\nRandom cropping, flipping, rotation, zooming, color adjustments.\nFor text:\nSynonym replacement, paraphrasing, back translation.\nFor speech/audio:\nTime stretching, pitch shifting, adding noise.\n🔹 Why It Works?",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk36",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 36,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "🔹 Why It Works?\nHelps the model generalize better by learning from a more diverse dataset.\nMakes the model invariant to small changes in the input.\n5.5.4. Noise in the Inputs and Weights\n🔹 Injecting Noise to Improve Generalization\nAdding random noise to the inputs or weights forces the model to become robust.\n🔹 Methods\nAdding noise to inputs (input corruption):\nx ′=x+Gaussian noise\nEncourages robustness to minor variations in input data.\nSimilar to data augmentation.\nAdding noise to weights:",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk37",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 37,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Adding noise to weights:\nDuring training, small random values are added to the weights.\nPrevents the model from relying on specific neurons too much.\n🔹 Dropout (A Special Case of Noise)\nRandomly disables neurons during training.\nPrevents neurons from becoming too dependent on each other",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk38",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 38,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5.5. Ensemble Methods\n- Instead of relying on a single trained model, train multiple models and average their predictions.\nTypes of Ensemble Methods\nBagging (Bootstrap Aggregating): Train multiple independent networks on slightly different data subsets. Example: Random Forest (for decision trees).\nAveraging multiple trained models: Train multiple models and take the average of their predictions.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk39",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 39,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Dropout as an Ensemble: Dropout can be seen as training many sub-networks, which are then combined.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk40",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 40,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5.6. Convolutional Neural Networks (CNNs) as Regularizers\n🔹 CNNs Impose Structural Constraints\nFully connected networks have too many parameters, leading to overfitting.\nCNNs naturally reduce the number of parameters by sharing weights in local regions.\n🔹 Why CNNs Help?\nLocal Receptive Fields: Neurons only process small patches of input.\nWeight Sharing: The same filter slides across the image, reducing parameters.\nTranslation Invariance: Helps detect patterns regardless of location.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk41",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 41,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.5.7. Tangential Regularization\n🔹 Regularization Based on Data Manifold\nIn real-world data, meaningful variations often lie on a lower-dimensional manifold.\nTangential regularization encourages the network to be invariant to small transformations along this manifold.\n🔹 Implementation\nSmall transformations of the input should not change the output significantly.\nExample: If a rotated image still represents the same object, the network should predict the same label.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk42",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 42,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.6: Mixture Density Networks (MDNs)\nMixture Density Networks (MDNs) combine neural networks with mixture models to model complex, multimodal conditional probability distributions. Unlike traditional neural networks that predict a single output, MDNs can model situations where multiple outcomes are possible for a given input, making them ideal for tasks with inherent uncertainty or ambiguity.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk43",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 43,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "An MDN predicts a probability distribution over possible outputs rather than a single deterministic output. It consists of:\nA neural network that outputs the parameters of a mixture model.\nA mixture model, typically a Gaussian Mixture Model (GMM), to represent the conditional probability distribution.\nnput Layer: Processes input features as in standard neural networks.\nHidden Layers: Extract features and patterns.\nOutput Layer: Produces parameters for the mixture model, including:",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk44",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 44,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Mixing coefficients 𝜋𝑘 : Probabilities that sum to 1.\nMeans 𝜇𝑘 : Centers of the Gaussian components.\nVariances 𝜎𝑘2 : Spread of each Gaussian component.\nThey output a probability distribution rather than a single value, allowing them to handle multimodal data distributions effectively.\nTraining involves maximizing the likelihood of the observed data under the mixture model, typically using gradient-based methods.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk45",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 45,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.7: Bayesian Neural Networks (BNNs)\nBayesian Neural Networks (BNNs) provide a probabilistic approach to neural network learning by placing probability distributions over network weights rather than using fixed values. This helps model uncertainty in predictions, making BNNs useful for tasks requiring confidence estimation, such as medical diagnosis and robotics",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk46",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 46,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.7.1. Inference in Bayesian Neural Networks\n🔹 What is Bayesian Inference in Neural Networks?\nInstead of finding a single optimal set of weights w, Bayesian inference treats the weights as random variables with a probability distribution.\nThe goal is to compute the posterior distribution over weights given the data\n\np(w) is the prior distribution over weights.\np(D∣w) is the likelihood (how well the network fits the data).\np(D) is the evidence (normalization factor).",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk47",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 47,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "5.7.2. Laplace Approximation and Posterior Distribution\n🔹 Laplace Approximation: A Simple Bayesian Approach\nInstead of computing the exact posterior, we approximate it with a Gaussian centered around the maximum a posteriori (MAP) estimate 𝑤MAP\n​Steps in the Laplace Approximation\nFind 𝑤MAP , which maximizes the posterior\nCompute the Hessian matrix H of the negative log-posterior at 𝑤MAP",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk48",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 48,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "​Approximate the posterior as  the posterior distribution is modeled as a Gaussian centered at 𝑤MAP with covariance inversely proportional to the Hessian.\n \n​5.7.3. Predictive Distribution and Bayesian Model Averaging\n🔹 The Predictive Distribution\nInstead of predicting a single output 𝑦∗ for a given input 𝑥∗, we compute the full predictive distribution:",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk49",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 49,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "Bayesian Model Averaging (BMA)\nStandard neural networks use one weight configuration.\nBNNs average predictions over multiple possible weight configurations, leading to better generalization\n​\nSummary\nBNNs provide uncertainty estimates by treating weights as probability distributions.\nLaplace Approximation simplifies Bayesian inference but assumes Gaussian distributions.\nBayesian Model Averaging improves prediction reliability by considering multiple possible weight values.",
    "embedding": null,
    "source": "5.1. Feed-forward Network Functions.txt",
    "chunk_id": "5.1. Feed-forward Network Functions_chunk50",
    "meta_data": {
      "filename": "5.1. Feed-forward Network Functions.txt",
      "chunk_index": 50,
      "created_time": "2025-06-21T22:46:13.252326"
    }
  },
  {
    "text": "6.1: Dual Representations\nIn dual representations, we express linear regression and classification models in terms of kernel functions rather than explicit feature mappings. This allows us to compute solutions in high-dimensional spaces efficiently. Instead of working directly with parameters w, we reformulate the problem using a Gram matrix (K), which consists of kernel function evaluations between training points.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk0",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "a linear regression model where the parameters w are determined by minimizing a regularized sum-of-squares error function:\n         N\nJ(w)= 1/2∑(wTϕ(xn)−tn)^2+λ/2 wTw",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk1",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Here, λ is a regularization parameter that prevents overfitting, and Φ(x) is the feature mapping.\nBy setting the gradient ∇J(w) = 0, the optimal w can be expressed as a linear combination of the basis functions:\n𝑤=∑𝑎𝑛*𝜙(𝑥𝑛)\n  𝑛=1\nThis representation shifts the focus from w to the new coefficient vector a, leading to a dual formulation of the problem.\n\nBy defining the Gram matrix 𝐾 We can express the least-squares solution entirely in terms of the kernel function",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk2",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Setting the gradient ∇J(a) = 0, we obtain The prediction for a new input x\n\n\n6.2. Constructing Kernels\nSince the kernel function k(x, x') is central to dual representations, we need a systematic way to construct valid kernels. The kernel must satisfy certain mathematical properties to ensure it represents an inner product in some feature space.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk3",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Ways to Construct Kernels\n-Feature Space Mapping Approach\nDefine an explicit feature transformation φ(x) and compute the kernel \n- Direct Kernel Construction\nInstead of defining φ(x) explicitly, we construct k(x, x') directly.\nA valid kernel must ensure that the Gram matrix K is positive semi-definite for all training sets.\n- Common Kernel Functions\nLinear Kernel:Eqvalent to a simple dot product in input space.\n\nPolynomial Kernel (Degree M):Captures interactions of features up to degree M.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk4",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Rial Basis Function (RBF) or Gaussian Kernel: easures similarity based on Euclidean distance, allowing infinite-dimensional feature mappings.\n\nSigmoid Kernel: inspired by neural networks but not always valid.\n\n-Building New Kernels from Existing Ones\nIf k1(x, x') and k2(x, x') are valid kernels, then the following operations also produce valid kernels:\nScaling, Addition,Multiplication, Exponentiation",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk5",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "summary of 6.1,6.2 \n-Dual representations reformulate models using kernel functions, allowing efficient computations in high-dimensional spaces.\n-Valid kernel functions must ensure the Gram matrix (K) is positive semi-definite.\n-Kernel engineering techniques help adapt models to different types of data without explicitly computing feature mappings",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk6",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.3. Radial Basis Function (RBF) Networks\nRBF networks are neural network models that use radial basis functions as their activation functions. Unlike standard multi-layer perceptrons (MLPs), which use sigmoidal or ReLU activations, RBF networks rely on localized, distance-based activations centered around key data points.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk7",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- The model consists of a weighted sum of radial basis functions, where each function depends only on the Euclidean distance between an input and a center:\n     M\ny(x)=∑​wh(∥x−μj∥)\n    j=1\nμj are the centers of the basis functions.\nh(∥x−μj∥) is typically a Gaussian function: h(∥x−μj∥)=exp((− ∥x−μj∥ ^2)/2𝜎62)",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk8",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "-Originally developed for exact interpolation, meaning they fit data exactly. However, this is not ideal in noisy scenarios, so modern versions include regularization.\nRBF networks are closely related to kernel methods, particularly the Gaussian kernel, which we saw in Section 6.2.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk9",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- The centers 𝜇𝑗 can be chosen from:\nTraining data points (one per point) → computationally expensive.\nClustering methods like K-means → computationally efficient.\nOrthogonal least squares (OLS) → selects the most influential data points iteratively.\n\n- Training is fast, but predictions can be slow because each test point involves evaluating all basis functions.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk10",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.3.1 Nadaraya-Watson Model\nThe Nadaraya-Watson model provides a probabilistic interpretation of kernel-based regression. Instead of explicitly solving for parameters, it estimates the function directly using weighted averages of training data.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk11",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "-The model predicts the output for a new input x as a weighted sum of training labels\n- The kernel function k(x, x_n) determines the weight of each training point.\n- The weights sum to 1, ensuring smooth estimates, typical choice for g(x) is a Gaussian function, similar to the RBF network’s basis functions.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk12",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Gaussian Processes (GPs) in Section 6.4, which unify kernel-based regression under a Bayesian framework. GPs take kernel regression a step further by not just estimating a function but also modeling uncertainty in predictions.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk13",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4. Gaussian Processes: A Probabilistic View of Kernel Methods\nFrom Kernel Regression to Gaussian Processes\nKernel regression (6.3.1) assumes a fixed function estimate, but it does not provide a confidence measure for predictions.\nGaussian Processes (GPs) extend this by treating functions as random variables, where predictions come with uncertainty quantification.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk14",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Instead of estimating a single function, a GP defines a probability distribution over all possible functions that fit the training data",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk15",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4.1 Linear Regression Revisited: The Link to Gaussian Processes\nTo understand GPs, we revisit linear regression but from a probabilistic perspective:\nLinear regression model:y(x)=wTϕ(x)\nPreviously, we estimated w using least squares.\nInstead, we now place a prior on w, making it a random variable.\nIf w follows a Gaussian prior, then y(x) follows a Gaussian distribution: p(y)=N(y∣0,K)\nThe covariance matrix K is computed using a kernel function k(x,x′).\nThis transforms our view of regression:",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk16",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This transforms our view of regression:\nInstead of optimizing w, we infer the distribution over functions using GPs.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk17",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4.2 Gaussian Processes for Regression\nKey Differences from Kernel Regression\nKernel regression provides only point estimates, whereas GPs provide full distributions with mean and variance.\nGP predictions have uncertainty quantification, which is useful in real-world applications like Bayesian optimization, active learning, and reinforcement learning.\nFor a new test point 𝑥∗,the GP predictive distribution is:𝑝(𝑦∗∣𝑋,𝑦,𝑥∗)=𝑁(𝜇(𝑥∗),𝜎^2(𝑥∗))\nwhere:\n𝜇(𝑥∗) is the mean prediction.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk18",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "where:\n𝜇(𝑥∗) is the mean prediction.\n𝜎^2(𝑥∗) is the uncertainty (variance) of the prediction.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk19",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4.3 Learning the Hyperparameters of Gaussian Processes\nGPs rely on a kernel function k(x,x′) with hyperparameters that control function behavior (e.g., smoothness).\n\nTo find optimal hyperparameters, we maximize the marginal likelihood\nwhere\nFirst term: How well the data fits the model.\nSecond term: Complexity penalty (model regularization).\nGradient-based methods (e.g., conjugate gradient, L-BFGS) optimize the kernel hyperparameters.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk20",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4.4 Automatic Relevance Determination (ARD)\nGPs can automatically identify which input dimensions are important via ARD.\n\nWe assign different length scales ℓ𝑖 to each input dimension:\nIf ℓ𝑖 is large, the function varies slowly with respect to 𝑥𝑖, meaning 𝑥𝑖 is not important.\nIf ℓ𝑖 is small,𝑥𝑖 strongly influences predictions.\nThis prunes irrelevant dimensions automatically in high-dimensional problems.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk21",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4.5 Gaussian Processes for Classification\nUnlike regression, classification outputs discrete labels instead of continuous values.\nGP classification transforms the GP output using a sigmoid function: p(y=1∣x)=σ(f(x))\nwhere f(x) is a Gaussian Process.\nSince the sigmoid function makes the likelihood non-Gaussian, exact inference is intractable.\nThus, we use approximations:\nLaplace Approximation\nExpectation Propagation (EP)\nVariational Inference",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk22",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4.6 Laplace Approximation for GP Classification\nThe Laplace Approximation is one way to approximate the non-Gaussian posterior:\nFind the mode of the posterior p(f∣X,y).\nApproximate the posterior using a Gaussian centered at the mode.\nUse this Gaussian to compute predictions.\nAlthough simple, it can be inaccurate in complex classification tasks. Expectation Propagation (EP) is often preferred.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk23",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6.4.7 Connections Between Gaussian Processes and Other Models\nGPs are closely linked to other machine learning models:",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk24",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "-Kernel Ridge Regression\tGPs reduce to ridge regression in the limit of zero noise variance.\n- Bayesian Linear Regression GPs are a generalization of Bayesian linear regression.\n- Neural Networks\tAn infinite-width neural network converges to a GP.\n- Support Vector Machines (SVMs)\tGPs and SVMs both use kernel functions, but SVMs maximize a margin, while GPs model a distribution over functions.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk25",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The connection to infinite neural networks is particularly exciting—modern research shows that deep GPs can act as Bayesian deep learning models.\n\n\nfinal Connections: From Dual Representations to Gaussian Processes\nDual Representations (6.1) → Kernel Methods (6.2)\n\nDuality allows us to reformulate machine learning models using kernels.\nKernel Methods (6.2) → Radial Basis Function Networks (6.3)",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk26",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "RBF networks are a neural network equivalent of kernel-based learning.\nRBF Networks (6.3) → Kernel Regression (6.3.1)\n\nNadaraya-Watson kernel regression provides a probabilistic foundation.\nKernel Regression (6.3.1) → Gaussian Processes (6.4)\n\nGPs extend kernel regression by modeling distributions over functions.\nGP Regression (6.4.2) → GP Classification (6.4.5)\n\nBy using sigmoid or softmax functions, GPs can be adapted for classification.\nGPs (6.4.7) → Infinite Neural Networks",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk27",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "A deep GP is equivalent to an infinite-width neural network.\nKey Takeaways\nGaussian Processes unify kernel-based learning in a Bayesian framework.\nThey predict not just function values but also uncertainty.\nHyperparameters can be learned using marginal likelihood optimization.\nGPs generalize many classical ML methods, including neural networks.",
    "embedding": null,
    "source": "6.1 Dual Representations.txt",
    "chunk_id": "6.1 Dual Representations_chunk28",
    "meta_data": {
      "filename": "6.1 Dual Representations.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Logical Computations with Neurons\nThe artificial neuron activates its output when more than a certain number of its inputs are active You can imagine how these networks can be combined to compute complex logical\nexpressions",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk0",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The Perceptron \ncalled a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). The inputs and output are numbers (instead of binary on/off values),\nThreshold logic unit: an artificial neuron which computes a weighted sum of its inputs then applies a step function\n\nA single TLU can be used for simple linear binary classification. It computes a linear\ncombination of the inputs, and if the result exceeds a threshold, it outputs the positive\nclass.",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk1",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "A Perceptron is simply composed of a single layer of TLUs,7 with each TLU connected\nto all the inputs. When all the neurons in a layer are connected to every neuron in the\nprevious layer (i.e., its input neurons), the layer is called a fully connected layer, or a\ndense layer. The inputs of the Perceptron are fed to special passthrough neurons\ncalled input neurons: they output whatever input they are fed. All the input neurons",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk2",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "form the input layer. Moreover, an extra bias feature is generally added (x0 = 1): it is\ntypically represented using a special type of neuron called a bias neuron, which outputs\n1 all the time.",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk3",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Computing the outputs of a fully connected layer\nhW, b X = ϕ XW + b\nIn this equation:\n• As always, X represents the matrix of input features. It has one row per instance\nand one column per feature.\n• The weight matrix W contains all the connection weights except for the ones\nfrom the bias neuron. It has one row per input neuron and one column per artificial\nneuron in the layer.\n• The bias vector b contains all the connection weights between the bias neuron",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk4",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "and the artificial neurons. It has one bias term per artificial neuron.\n• The function ϕ is called the activation function: when the artificial neurons are\nTLUs, it is a step function (but we will discuss other activation functions shortly).\nSo",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk5",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Hebb’s rule Cells that fire together, wire together”; that is, the connection weight between two neurons tends to increase when they fire simultaneously",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk6",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Perceptron learning rule (weight update) \nwi, j next step = wi,j + η (yj − y'j)xi\n• wi, j is the connection weight between the ith input neuron and the jth output\nneuron.\n• xi is the ith input value of the current training instance.\n• y j is the output of the jth output neuron for the current training instance.\n• yj is the target output of the jth output neuron for the current training instance.\n• η is the learning rate.",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk7",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "problem : the fact that they\nare incapable of solving some trivial problems\nsolution : It turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a Multilayer Perceptron (MLP\n\nThe Multilayer Perceptron and Backpropagation\nAn MLP is composed of one (passthrough) input layer, one or more layers of TLUs,\ncalled hidden layers, and one final layer of TLUs called the output layer",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk8",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "When an ANN contains a deep stack of hidden layers,9 it is called a deep neural network\n(DNN). The field of Deep Learning studies DNNs, and more generally models containing deep stacks of computations",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk9",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "backpropagation algorithm\nis able to compute the gradient of the network’s error with regard to every single\nmodel parameter. In other words, it can find out how each connection weight and\neach bias term should be tweaked in order to reduce the error. Once it has these gradients,\nit just performs a regular Gradient Descent step, and the whole process is\nrepeated until the network converges to the solution.",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk10",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Regression MLPs\nwhen building an MLP for regression, you do not want to use any activation function for the output neurons so they are free to output any range of values.\nThe loss function to use during training is typically the mean squared error, but if you\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\nerror instead",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk11",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Classification MLPs\nMLPs can also be used for classification tasks. For a binary classification problem,\nyou just need a single output neuron using the logistic activation function: the output\nwill be a number between 0 and 1, which you can interpret as the estimated probability\nof the positive class MLPs can also easily handle multilabel binary classification tasks",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk12",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "If each instance can belong only to a single class, out of three or more possible classes, then you need to have one output neuron per class, and you should use the softmax activation function for the whole output layer to ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1 . This is called multiclass\nclassification.",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk13",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Fine-Tuning Neural Network Hyperparameters\nThe flexibility of neural networks is also one of their main drawbacks: there are many\nhyperparameters to tweak\n\nOne option is to simply try many combinations of hyperparameters and see which\none works best on the validation set (or use K-fold cross-validation).\nThe exploration may last many hours, depending on the hardware, the size of the\ndataset, the complexity of the model, and the values of n_iter and cv.",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk14",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Number of Hidden Layers\nFor many problems, you can begin with a single hidden layer and get reasonable\nresults. An MLP with just one hidden layer can theoretically model even the most\ncomplex functions, provided it has enough neurons. But for complex problems, deep\nnetworks have a much higher parameter efficiency than shallow ones: they can model\ncomplex functions using exponentially fewer neurons than shallow nets, allowing",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk15",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "them to reach much better performance with the same amount of training data.\nIn summary, for many problems you can start with just one or two hidden layers and\nthe neural network will work just fine. For more complex problems,\nyou can ramp up the number of hidden layers until you start overfitting the training\nset. Very complex tasks",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk16",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Number of Neurons per Hidden Layer\nThe number of neurons in the input and output layers is determined by the type of\ninput and output your task requires.\nAs for the hidden layers, it used to be common to size them to form a pyramid, with\nfewer and fewer neurons at each layer\nJust like the number of layers, you can try increasing the number of neurons gradually\nuntil the network starts overfitting. But in practice, it’s often simpler and more",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk17",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "efficient to pick a model with more layers and neurons than you actually need, then\nuse early stopping and other regularization techniques to prevent it from overfitting.\nVincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants”",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk18",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Learning rate\nThe learning rate is arguably the most important hyperparameter. In general, the\noptimal learning rate is about half of the maximum learning rate\nOne way to find a good learning rate is to train the model for a few hundred iterations,\nstarting with a very low learning rate (e.g., 10-5) and gradually increasing\nit up to a very large value (e.g., 10). This is done by multiplying the learning rate\nby a constant factor at each iteration (e.g., by exp(log(106)/500) to go from 10-5 to",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk19",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "10 in 500 iterations).",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk20",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Optimizer\nChoosing a better optimizer than plain old Mini-batch Gradient Descent (and\ntuning its hyperparameters) is also quite important",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk21",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Batch size\nThe batch size can have a significant impact on your model’s performance and\ntraining time. The main benefit of using large batch sizes is that hardware accelerators\nlike GPUs can process them efficiently\none strategy is to try to\nuse a large batch size, using learning rate warmup, and if training is unstable or\nthe final performance is disappointing, then try using a small batch size instead",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk22",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Activation function\nWe discussed how to choose the activation function earlier in this chapter: in\ngeneral, the ReLU activation function will be a good default for all hidden layers.\nFor the output layer, it really depends on your task.",
    "embedding": null,
    "source": "ch1.txt",
    "chunk_id": "ch1_chunk23",
    "meta_data": {
      "filename": "ch1.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "ch14 \n- The Architecture of the Visual Cortex\nthey showed that many neurons in\nthe visual cortex have a small local receptive field, meaning they react only to visual stimuli located in a limited region of the visual field This powerful architecture is able to detect all sorts of complex patterns in any area of the visual field\n\nThese studies of the visual cortex inspired the neocognitron,4 introduced in 1980, which gradually evolved into what we now call convolutional neural networks",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk0",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Convolutional Layers\nThis architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on.\n\nIn order for a layer to have the same height and width as the previous layer, it is common\nto add zeros around the inputs, as shown in the diagram. This is called zero padding.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk1",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Filters\nA neuron’s weights can be represented as a small image the size of the receptive field called filters \nvertical line filter and horizontal line filter; \nThus, a layer full of neurons using the same filter outputs a feature map, which highlights the areas in an image that activate the filter the most.\n\n- Stacking Multiple Feature Maps\nin reality a convolutional layer has multiple filters and outputs one feature map per filter, so it is more accurately represented in\n3D",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk2",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Input images are also composed of multiple sublayers: one per color channel. There are typically three: red, green, and blue (RGB). Grayscale images have just one channel \n- Memory Requirements\nAnother problem with CNNs is that the convolutional layers require a huge amount of RAM. This is especially true during training, because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk3",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Pooling Layers\nTheir goal is to subsample (i.e., shrink) the input image reduce the computational load\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\noutputs of a limited number of neurons in the previous layer, located within a small\nrectangular receptive field. You must define its size, the stride, and the padding type,\njust like before. However, a pooling neuron has no weights; all it does is aggregate the",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk4",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "inputs using an aggregation function such as the max or mean.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk5",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- CNN Architectures\nTypical CNN architectures stack a few convolutional layers (each one generally followed\nby a ReLU layer), then a pooling layer, then another few convolutional layers\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\nwith more feature maps), thanks to the convolutional layers (see Figure 14-11). At the",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk6",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\nsoftmax layer that outputs estimated class probabilities).",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk7",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- LeNet-5 architecture\ninput - conv - pool - conv - avg pool - conv - fc - fc\n\n- AlexNet \ninput - conv - max pool - conv - conv - conv - max pool - fc -fc -fc\n \n- GoogLeNet\ninput - conv- max pool - norm - conv - norm - max pool - max pool - avg pool - drop out - fc - SoftMax",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk8",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Classification and Localization",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk9",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Localizing an object in a picture can be expressed as a regression task,to predict a bounding box around the object, a common approach is to predict the horizontal and vertical coordinates of the object’s center, as well as its height and width. This means we have four numbers to predict. It does not require much change to the model; we just need to add a second dense output layer with four units (typically on top of the global average pooling layer), and it can be trained using the MSE loss",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk10",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Transfer Learning\nLeveraging Pretrained Models: Transfer learning involves using a pre-trained model (e.g., ResNet-50) that has already learned to detect features from a large dataset like ImageNet. The images need to be resized to the model's expected input size (e.g., 224x224 pixels for ResNet-50) and preprocessed appropriately.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk11",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Fine-tuning: Typically, the weights of the pre-trained layers are initially frozen to prevent damaging the learned features. After training the top layers, all layers can be unfrozen, and the model is trained further with a much lower learning rate. This process allows for achieving high accuracy on new datasets.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk12",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Object Detection\nThe task of classifying and localizing multiple objects in an image is called object detection.\ncommon approach was to take a CNN that was trained to classify and locate a single object, then slide it across the image,",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk13",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This technique is fairly straightforward, but as you can see it will detect the same object multiple times, at slightly different positions. Some post-processing will then be needed to get rid of all the unnecessary bounding boxes. A common approach for this is called non-max suppression.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk14",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "1- First, you need to add an extra objectness output to your CNN, to estimate the probability that a flower is indeed present in the image (alternatively, you could add a “no-flower” class Then get rid of all the bounding boxes for which the objectness score is below some threshold: this will drop all the bounding boxes that don’t actually contain a flower.\n2- Find the bounding box with the highest objectness score, and get rid of all the other bounding boxes that overlap a lot with it",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk15",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3. Repeat step two until there are no more bounding boxes to get rid of.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk16",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "-Fully Convolutional Networks (FCNs)\nFCNs are primarily used for semantic segmentation, a visual task where each pixel in an image is classified according to the object class it belongs to\n Traditional CNNs, which often include fully connected layers at the end for classification, tend to lose spatial resolution as the image passes through layers with strides greater than 1 \nFCNs address this by converting a pre-trained CNN into a fully convolutional network. This involves:",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk17",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Replacing the final dense (fully connected) layers with convolutional layers.\nAdding upsampling layers (e.g., transposed convolutional layers) to recover the spatial resolution lost during the downsampling process in earlier layers",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk18",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- You Only Look Once (YOLO)\nYOLO is a popular object detection system. Object detection involves identifying and localizing multiple objects within an image by drawing bounding boxes around them and classifying each object.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk19",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "OLO falls under the category of single-shot detection models. Unlike older methods that might slide a CNN across an image at different positions and scales, single-shot detectors process the entire image in one pass to directly predict bounding boxes and class probabilities simultaneously. This makes them very fast.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk20",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Semantic Segmentation\neach pixel is classified according to the class of the object it belongs to\n. The main challenge is the gradual loss of spatial resolution in traditional CNNs due to layers with strides greater than 1. \nA common and effective solution to overcome the spatial resolution loss is to transform a pre-trained CNN (which might have been originally designed for image classification) into a Fully Convolutional Network (FCN).",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk21",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "An FCN is a network composed entirely of convolutional and pooling layers, without any dense (fully connected) layers at the end.\nTo recover the lost spatial resolution, FCNs incorporate upsampling layers. These layers (often implemented using transposed convolutions, also known as \"deconvolutions\" or \"fractionally-strided convolutions\") effectively enlarge the feature maps back to the original image dimensions or a desired output resolution.",
    "embedding": null,
    "source": "ch14.txt",
    "chunk_id": "ch14_chunk22",
    "meta_data": {
      "filename": "ch14.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "ch15\n- Recurrent Neurons and Layers\nrecurrent neural network looks very much like a\nfeedforward neural network, except it also has connections pointing backward.\nYou can easily create a layer of recurrent neurons. At each time step t, every neuron receives both the input vector x(t) and the output vector from the previous time step y(t–1),\nEach recurrent neuron has two sets of weights: one for the inputs x(t) and the other for the outputs of the previous time step, y(t–1).",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk0",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Memory Cells\nSince the output of a recurrent neuron at time step t is a function of all the inputs from previous time steps, you could say it has a form of memory.\nA part of a neural network that preserves some state across time steps is called a memory cell\n\n- Input and Output Sequences\nAn RNN can simultaneously take a sequence of inputs and produce a sequence of outputs This type of sequence-to-sequence network is useful for predicting time series",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk1",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "you could feed the network a sequence of inputs and ignore all outputs\nexcept for the last one sequence-to-vector network\n\nyou could feed the network the same input vector over and over again at\neach time step and let it output a sequence (vector-to-sequence network) \nthe input could be an image and the output could be a caption for that image.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk2",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Lastly, you could have a sequence-to-vector network, called an encoder, followed by a vector-to-sequence network, called a decoder\nexample, this could be used for translating a sentence from one language\nto another. You would feed the network a sentence in one language, the\nencoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in another language. This two-step model, called an Encoder–Decoder",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk3",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Training RNNs \nTo train an RNN, the trick is to unroll it through time (like we just did) and then\nsimply use regular backpropagation (see Figure 15-5). This strategy is called backpropagation\nthrough time (BPTT).",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk4",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Forecasting a Time Series\nsingle value per time step, so these are univariate time series\nmultiple values per time step so it is a multivariate time series\nA typical task is to predict future values, which is called forecasting.\nto predict missing values from the past. This is called imputation",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk5",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Baseline Metrics\nBefore we start using RNNs, it is often a good idea to have a few baseline metrics, or else we may end up thinking our model works great when in fact it is doing worse than basic models.\n\nThis is called naive forecasting, and it is sometimes surprisingly difficult to outperform.\n\nAnother simple approach is to use a fully connected network. Since it expects a flat list of features for each input, we need to add a Flatten layer.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk6",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Implementing a Simple RNN\nWe do not need to specify the length of the input sequences (unlike in the previous model), since a recurrent neural network can process any number of time steps\n\n- Deep RNNs\nImplementing a deep RNN with tf.keras is quite simple: just stack recurrent layers\nIf you compile, fit, and evaluate this model, you will find that it reaches an MSE of\n0.003. We finally managed to beat the linear model!",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk7",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Forecasting Several Time Steps Ahead\nThe first option is to use the model we already trained, make it predict the next value,\nthen add that value to the inputs (acting as if this predicted value had actually occurred),\nand use the model again to predict the following value, and so on,\n\nAs you might expect, the prediction for the next step will usually be more accurate than the predictions for later time steps, since the errors might accumulate",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk8",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The second option is to train an RNN to predict all 10 next values at once. We can still use a sequence-to-vector model, but it will output 10 values instead of 1. However,\nwe first need to change the targets to be vectors containing the next 10 values",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk9",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "we can train it to forecast the next 10 values at each and every time step. In other words, we can turn this sequence-to-vector RNN into a sequence-to-sequence RNN. The advantage of this technique is that the loss will contain a term for the output of the RNN at each and every time step, not just the output at the last time step. This means there will be many more error gradients flowing through the model, and they won’t have to flow only through time; they will also flow from the output of",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk10",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "time; they will also flow from the output of each time step. This will both stabilize and speed up training.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk11",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Handling Long Sequences\nFighting the Unstable Gradients Problem\ngood parameter initialization, faster optimizers, dropout, and so on. However, nonsaturating activation functions (e.g., ReLU) may not help as much here; in fact, they may actually lead the RNN to be even more unstable during training\nBecause the same weights are used",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk12",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Because the same weights are used\nat every time step, the outputs at the second time step may also be slightly increased, and those at the third, and so on until the outputs explode—and a nonsaturating activation function does not prevent that. You can reduce this risk by using a smaller learning rate, but you can also simply use saturating activation function like the hyperbolic tangent -tanh- \nIn much the same way, the",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk13",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "In much the same way, the\ngradients themselves can explode. If you notice that training is unstable, you may want to monitor the size of the gradients (e.g., using TensorBoard) and perhaps use Gradient Clipping.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk14",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Batch Normalization cannot be used as efficiently with RNNs",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk15",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Another form of normalization often works better with RNNs: Layer Normalization.\nThis idea was introduced by Jimmy Lei Ba et al. in a 2016 paper:4 it is very similar to\nBatch Normalization, but instead of normalizing across the batch dimension, it normalizes\nacross the features dimension. One advantage is that it can compute the\nrequired statistics on the fly, at each time step, independently for each instance. This",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk16",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "also means that it behaves the same way during training and testing (as opposed to\nBN), and it does not need to use exponential moving averages to estimate the feature\nstatistics across all instances in the training set",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk17",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "With these techniques, you can alleviate the unstable gradients problem and train an RNN much more efficiently. Now let’s look at how to deal with the short-term memory problem.\n\n- Tackling the Short-Term Memory Problem\nafter a while, the RNN’s state contains virtually no trace of the first inputs. This can be a showstopper\nTo tackle this problem, various types of cells with long-term\nmemory have been introduced.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk18",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "LSTM cells\nThe Long Short-Term Memory (LSTM) cell was\nLSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster, and it will detect long-term dependencies in the data.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk19",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "If you don’t look at what’s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split into two vectors: h(t) and c(t) (“c” stands for “cell”). You can think of h(t) as the short-term state and c(t) as the long-term state",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk20",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Now let’s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state c(t–1) traverses the network from left to right, you can see that it first goes through a forget gate, dropping some memories, and then it adds some new memories via the addition operation (which adds the memories that were selected by an input gate). The result c(t) is sent straight out, without any further transformation.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk21",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "straight out, without any further transformation. So, at each time step, some memories are dropped and some memories are added. Moreover, after the addition operation, the long-term state is copied and passed through the tanh function, and then the result is filtered by the output gate. This produces the short-term state h(t) (which is equal to the cell’s output for this time step, y(t)).",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk22",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "—The forget gate (controlled by f(t)) controls which parts of the long-term state should be erased.\n—The input gate (controlled by i(t)) controls which parts of g(t) should be added to the long-term state.\n—Finally, the output gate (controlled by o(t)) controls which parts of the longterm state should be read and output at this time step, both to h(t) and to y(t).",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk23",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "In short, an LSTM cell can learn to recognize an important input (that’s the role of the input gate), store it in the long-term state, preserve it for as long as it is needed (that’s the role of the forget gate), and extract it whenever it is needed. This explains why these cells have been amazingly successful at capturing long-term patterns in time series, long texts, audio recordings, and more.",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk24",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "GRU cells\nThe GRU cell is a simplified version of the LSTM cell,\n• Both state vectors are merged into a single vector h(t).\n• A single gate controller z(t) controls both the forget gate and the input gate.\n• There is no output gate; the full state vector is output at every time step. However, there is a new gate controller r(t) that controls which part of the previous state will be shown to the main layer (g(t)).",
    "embedding": null,
    "source": "ch15.txt",
    "chunk_id": "ch15_chunk25",
    "meta_data": {
      "filename": "ch15.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "ch2 \n-The Vanishing/Exploding Gradients Problems\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\nlayers’ connection weights virtually unchanged, and training never converges to a\ngood solution. We call this the vanishing gradients problem",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk0",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "In some cases, the opposite\ncan happen: the gradients can grow bigger and bigger until layers get insanely\nlarge weight updates and the algorithm diverges. This is the exploding gradients problem\n\nIn some cases, the opposite\ncan happen: the gradients can grow bigger and bigger until layers get insanely\nlarge weight updates and the algorithm diverges. This is the exploding gradients problem,",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk1",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "-Glorot and He Initialization\nThey point out that we need the signal to flow properly in both\ndirections: in the forward direction when making predictions, and in the reverse\ndirection when backpropagating gradients. We don’t want the signal to die out, nor\ndo we want it to explode and saturate",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk2",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "For the signal to flow properly, the authors\nargue that we need the variance of the outputs of each layer to be equal to the variance\nof its inputs,2 and we need the gradients to have equal variance before and after\nflowing through a layer in the reverse direction\nIt is actually not possible to guarantee both\nunless the layer has an equal number of inputs and neurons but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk3",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "layer must be initialized randomly as described in Equation 11-1, where fanavg = (fanin\n+ fanout)/2. This initialization strategy is called Xavier initialization or Glorot initialization",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk4",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "By default, Keras uses Glorot initialization with a uniform distribution. When creating\na layer, you can change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\"\n\n-Nonsaturating Activation Functions\nit turns out that other activation functions behave much better in deep neural networks—\nin particular, the ReLU activation function, mostly because it does not saturate\nfor positive values",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk5",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem\nknown as the dying ReLUs during training, some neurons effectively “die,” meaning\nthey stop outputting anything other than 0. In some cases, you may find that half of\nyour network’s neurons are dead,if you used a large learning rate.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk6",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "A neuron dies when its weights get tweaked in such a way that the weighted sum of its\ninputs are negative for all instances in the training set. When this happens, it just\nkeeps outputting zeros, and Gradient Descent does not affect it anymore because the\ngradient of the ReLU function is zero when its input is negative.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk7",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "to solve that we use leaky ReLU. LeakyReLUα(z) = max(αz, z) α defines how much the function “leaks”: it is the\nslope of the function for z < 0 and is typically set to 0.01. This small slope ensures that\nleaky ReLUs never die;",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk8",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "parametric leaky ReLU (PReLU), where α is\nauthorized to be learned during training (instead of being a hyperparameter, it\nbecomes a parameter that can be modified by backpropagation\nPReLU was reported to strongly outperform ReLU on large image datasets, but\non smaller datasets it runs the risk of overfitting the training set.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk9",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Arné Clevert et al.6 proposed a new activation\nfunction called the exponential linear unit (ELU) that outperformed all the ReLU\nvariants in the authors’ experiments: training time was reduced, and the neural network\nperformed better on the test set.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk10",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The ELU activation function looks a lot like the ReLU function, with a few major\ndifferences:\n• It takes on negative values when z < 0, which allows the unit to have an average\noutput closer to 0 and helps alleviate the vanishing gradients problem\n• It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n• If α is equal to 1 then the function is smooth everywhere, including around z = 0,\nwhich helps speed up Gradient Descent since it does not bounce as much to the",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk11",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "left and right of z = 0.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk12",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The main drawback of the ELU activation function is that it is slower to compute\nthan the ReLU function and its variants",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk13",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Klambauer et al. introduced the Scaled ELU (SELU)\nactivation function: as its name suggests, it is a scaled variant of the ELU activation\nfunction.  if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize: the output of each layer will tend to\npreserve a mean of 0 and standard deviation of 1 during training, which solves the\nvanishing/exploding gradients problem.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk14",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "vanishing/exploding gradients problem.\nThere are, however, a few conditions for self-normalization to happen",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk15",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "• The input features must be standardized (mean 0 and standard deviation 1).\n• Every hidden layer’s weights must be initialized with LeCun normal initialization.\n• The network’s architecture must be sequential.\n• The paper only guarantees self-normalization if all layers are dense,\n\nSo, which activation function should you use for the hidden layers\nof your deep neural networks? Although your mileage will vary, in\ngeneral SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n> logistic.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk16",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Batch Normalization\nThe technique consists of\nadding an operation in the model just before or after the activation function of each\nhidden layer.\nsimply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling,\nthe other for shifting.\nif you add a BN layer as the very first layer you do not need to standardize your training\nset\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk17",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "each input’s mean and standard deviation. It does so by evaluating the mean and standard\ndeviation of the input over the current mini-batch",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk18",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "How it works:\n•During training, the BN operation zero-centers and normalizes the inputs of the layer using the mean (μB) and standard deviation (σB) calculated over the current mini-batch (B). A small smoothing term (ε) is added to the denominator to avoid division by zero.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk19",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "•After standardizing, it scales (γ) and shifts (β) the result using two new parameter vectors learned per layer. These parameters allow the model to learn the optimal scale and mean for the layer's inputs. The output of the BN operation is a rescaled and shifted version of the inputs.\n•During training, the parameters γ and β are learned through regular backpropagation.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk20",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "•At test time, instead of using the mini-batch statistics (μB and σB), BN uses estimated \"final\" input means (μ) and standard deviations (σ). These final statistics are typically estimated during training using a moving average of the layer's input means and standard deviations. This is necessary because test-time predictions might be for individual instances or small, unreliable batches.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk21",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "It significantly reduces the vanishing gradients problem, so much so that even saturating activation functions like tanh and logistic can be used effectively.\n•It makes the network much less sensitive to the weight initialization technique.\n•It allows the use of much larger learning rates, substantially speeding up the learning process.\n•BN helps networks converge much faster, requiring fewer epochs to reach the same performance.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk22",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "•It acts as a regularizer, reducing the need for other regularization techniques like dropout.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk23",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Drawbacks and considerations:\n•BN adds some complexity to the model.\n•There is a runtime penalty during inference due to the extra computations. However, this can often be avoided after training by fusing the BN layer with the previous layer.\n•Training might seem slower per epoch with BN, but the faster convergence usually means the total training time is reduced.\n•Batch Normalization is tricky to use in recurrent neural networks",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk24",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Gradient Clipping\nclip the gradients during backpropagation so that they never exceed some threshold. This is\ncalled Gradient Clipping.12 This technique is most often used in recurrent neural networks\n\n- Reusing Pretrained Layers\nfind an existing neural network that accomplishes a similar task to the one you are trying to tackle then reuse the lower layers of this network. This technique is called transfer learning.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk25",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The output layer of the original model should usually be replaced because it is most\nlikely not useful at all for the new task\n\nTry freezing all the reused layers first (i.e., make their weights non-trainable so that\nGradient Descent won’t modify them), then train your model and see how it performs.\nThen try unfreezing one or two of the top hidden layers to let backpropagation\ntweak them and see if performance improves",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk26",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Unsupervised Pretraining\nIn unsupervised training, a model is trained on the unlabeled data (or on\nall the data) using an unsupervised learning technique, then it is fine-tuned for the final\ntask on the labeled data using a supervised learning technique; the unsupervised part\nmay train one layer at a time as shown here, or it may train the full model directly",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk27",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Faster Optimizers\n1.Momentum Optimization:\n◦Inspired by a bowling ball rolling down a slope, this method doesn't just consider the local gradient but also the \"momentum\" from previous gradients.\n◦It adds a momentum vector to the weight updates.\n◦The gradient is used for acceleration, not speed.\n◦A hyperparameter β (momentum, typically 0.9) is used to simulate friction and prevent momentum from growing too large.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk28",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "◦If the gradient is constant, the terminal velocity (maximum update size) is the gradient multiplied by the learning rate and 1/(1–β). With β = 0.9, it can go 10 times faster than Gradient Descent.\n◦It helps escape plateaus faster and can roll past local optima.\n◦Momentum can cause overshooting and oscillations, but the friction (β < 1) helps reduce this.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk29",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.Nesterov Accelerated Gradient (NAG):\n◦A variation of momentum optimization, often faster.\n◦It measures the gradient slightly ahead in the direction of the momentum vector.\n◦This small tweak is usually more accurate as the momentum generally points towards the optimum.\n◦It helps reduce oscillations and converges faster than regular momentum.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk30",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 30,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3.AdaGrad:\n◦Addresses the elongated bowl problem by scaling down the gradient vector along the steepest dimensions.\n◦It accumulates the square of the gradients for each parameter in a vector s.\n◦The update step scales down the gradient by the square root of s plus a small smoothing term ε.\n◦This results in an adaptive learning rate that decays faster for steeper dimensions.\n◦It requires less tuning of the initial learning rate.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk31",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 31,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "◦AdaGrad often performs well on simple problems but can stop too early when training neural networks because the learning rate becomes too small.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk32",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 32,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "4.RMSProp:\n◦Fixes AdaGrad's problem of stopping too early by accumulating only the gradients from recent iterations using exponential decay.\n◦The first step accumulates squares of gradients into s using a decay rate β (typically 0.9).\n◦The update step is similar to AdaGrad, scaling the gradient by the square root of s plus ε.\n◦It almost always performs better than AdaGrad. It was a preferred optimizer before Adam.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk33",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 33,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "5.Adam and Nadam Optimization:\n◦Adam (adaptive moment estimation) combines ideas from momentum optimization and RMSProp.\n◦It keeps track of an exponentially decaying average of past gradients (like momentum, the \"first moment\") and an exponentially decaying average of past squared gradients (like RMSProp, the \"second moment\").\n◦Steps 1 and 2 in its algorithm compute these decaying averages (m and s).\n◦Steps 3 and 4 are bias correction steps to account for m and s being initialized at 0.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk34",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 34,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "◦Step 5 updates the parameters using the decaying averages.\n◦The momentum decay β1 is typically 0.9, and the scaling decay β2 is typically 0.999. The smoothing term ε is usually 10–7.\n◦Adam is an adaptive learning rate algorithm and requires less tuning of the learning rate η (often defaults to 0.001).\n◦Nadam is Adam optimization with the Nesterov trick. It often converges slightly faster than Adam.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk35",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 35,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "◦Adaptive methods like RMSProp, Adam, and Nadam converge fast but can sometimes lead to solutions that generalize poorly on certain datasets. The source suggests trying plain Nesterov Accelerated Gradient if performance is disappointing with adaptive methods",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk36",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 36,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Learning Rate Scheduling\nyou can do better than a constant learning rate: if you start with a large learning\nrate and then reduce it once training stops making fast progress, you can reach a\ngood solution faster than with the optimal constant learning rate.",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk37",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 37,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Avoiding Overfitting Through Regularization\n- ℓ1 and ℓ2 Regularization\nℓ1 regularization if you\nwant a sparse model (with many weights equal to 0). Here is how to apply ℓ2 regularization\nto a Keras layer’s connection weights, using a regularization factor of 0.01",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk38",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 38,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "-Dropout\nat every training step, every neuron (including the\ninput neurons, but always excluding the output neurons) has a probability p of being\ntemporarily “dropped out,” meaning it will be entirely ignored during this training\nstep, but it may be active during the next step \nThe hyperparameter p is called the dropout rate, and it is typically set between 10% and 50%:",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk39",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 39,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "-Max-Norm Regularization\nfor each neuron, it constrains the weights w of the incoming\nconnections such that ∥ w ∥2 ≤ r, where r is the max-norm hyperparameter and ∥ · ∥2\nis the ℓ2 norm.\nit is typically implemented by computing ∥w∥2 after each training\nstep and rescaling w if needed (w ← w r/‖ w ‖2).",
    "embedding": null,
    "source": "ch2.txt",
    "chunk_id": "ch2_chunk40",
    "meta_data": {
      "filename": "ch2.txt",
      "chunk_index": 40,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "ch3 ch4\n🔄 TensorFlow vs PyTorch: Feature-by-Feature Comparison\nConcept \tTensorFlow 2.x \t\t\t\t\tPyTorch Equivalent\nTensors\t\ttf.Tensor, tf.Variable\t\t\t\ttorch.Tensor (mutable by default)\nCustom Loss\tSubclass keras.losses.Loss or use a function\tSubclass nn.Module or use a function\nCustom Metric\tSubclass keras.metrics.Metric\t\t\tManual class or functional logic\nCustom Layer\tSubclass keras.layers.Layer\t\t\tSubclass nn.Module\nCustom Model\tSubclass keras.Model\t\t\t\tSubclass nn.Module",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk0",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Gradient Tape\ttf.GradientTape()\t\t\t\tloss.backward() + torch.autograd.grad()\nOptimizer Step\toptimizer.apply_gradients()\t\t\toptimizer.step()\nTraining Loop\tmodel.fit() (or custom loop)\t\t\tAlways manual loop\nSaving Models\tmodel.save(), HDF5 format\t\t\ttorch.save(model.state_dict())",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk1",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Chapter 13 Topic\t\tTensorFlow\t\tPyTorch Equivalent\nDataset class\t\t\ttf.data.Dataset\t\tCustom torch.utils.data.Dataset\nMapping / Preprocessing\t\tdataset.map(fn)\t\ttransform argument in Dataset\nShuffling\t\t\tshuffle(buffer_size)\tshuffle=True in DataLoader\nBatching\t\t\tbatch(batch_size)\tbatch_size= in DataLoader\nRepeating\t\t\trepeat()\t\tManual loop, or ConcatDataset\nPrefetching\t\t\tprefetch(N)\t\tprefetch_factor=N in DataLoader\nMultithreading\t\t\tnum_parallel_calls\tnum_workers= in DataLoader",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk2",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "1. Custom Loss Functions\nYou can define loss functions as Python functions or by subclassing tf.keras.losses.Loss.\n\n2. Custom Training Loops\nUse tf.GradientTape() to compute gradients manually.\nAllows full control over each step of training (forward pass, loss calculation, backpropagation, optimizer step).\n\n3. Custom Models\nSubclass tf.keras.Model to define models with custom behavior.\nImplement the call() method for the forward pass.",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk3",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "4. Custom Layers\nSubclass tf.keras.layers.Layer to define reusable layers (e.g., a residual block).\nUseful for architectures like ResNets.\n\n5. Custom Metrics\nSubclass tf.keras.metrics.Metric or use stateful logic to track performance across batches.\nExample: custom Mean Absolute Error implementation.\n\n6. Callbacks\nYou can use or write callbacks (e.g., EarlyStopping, ModelCheckpoint) to plug into training even if you use a custom loop.",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk4",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7. Using @tf.function\nYou can speed up performance by converting your Python functions to TF Graphs with @tf.function.\nCompatible with custom training loops and models.\n\nit equips you with maximum control and flexibility:\nPerfect for researchers, complex architectures, or nonstandard training loops.\nYou gain a solid understanding of how Keras works under the hood, which helps you debug, optimize, and experiment effectively",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk5",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "chapter 13 \nDeep Learning projects often involve large datasets that can’t fit into memory.\nEfficient data input pipelines\nUse tf.data.Dataset for scalable pipelines.\nUse torch.utils.data.Dataset (custom) and DataLoader\n- Supports reading, shuffling, batching, prefetching, and mapping.",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk6",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Data preprocessing (on-the-fly or ahead of time)\nUse map() to normalize, tokenize, or parse CSV/TFRecords.\n- Can use standard layers (Normalization, TextVectorization, etc.) or custom ones.\nDefine a transform inside your Dataset class.\n- Can use torchvision.transforms or custom Python functions.",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk7",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Encoding categorical/text features\nIn TensorFlow:\nLoad multiple CSV files using TextLineDataset + interleave.\nFor large-scale binary input: use TFRecord + parse_single_example.\nIn PyTorch:\nLoad CSV files with pandas or csv, and wrap them in a custom Dataset.",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk8",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "4. Shuffling, Batching, Repeating\nConcept\t\tTensorFlow\t\t\tPyTorch\nShuffle\t\tdataset.shuffle(buffer_size)\tshuffle=True in DataLoader\nRepeat\t\tdataset.repeat()\t\tManual loop or ConcatDataset\nBatch\t\tdataset.batch(batch_size)\tbatch_size= in DataLoader\nPrefetch\tdataset.prefetch(N)\t\tprefetch_factor + num_workers",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk9",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Preprocessing Strategies\nStrategy\t\tPros\t\t\t\t\tCons\nOffline \t\tpreprocessing (e.g., pandas, NumPy)\tFast at training time\tHard to keep sync across deployment\ntf.data.map() \t\tFlexible, modular\t\t\tRepeated each epoch (unless cached)\nInside the model\t(preprocessing layers)\t\t\tPortable (gets saved with the model)\tNot all ops are differentiable\nTF Transform (TFX)\tWrite once, reuse across t\t\training and serving\tMore complex setup\n\nPyTorch doesn’t have a TF Transform equivalent yet.",
    "embedding": null,
    "source": "ch3 ch4.txt",
    "chunk_id": "ch3 ch4_chunk10",
    "meta_data": {
      "filename": "ch3 ch4.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "chapter 11 sampling methods\nsampling method involves selecting a subset of individuals or observations from a larger population to collect data and make inferences about the entire population",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk0",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.1.1 Standard Distributions (Transformation Method)\nGenerate samples from a known distribution p(y) using uniform samples.\nGenerate a random number z∼Uniform(0,1)\nYou want to generate random samples from a probability distribution p(y).\nBut instead of directly sampling from p(y), you use uniform random numbers between 0 and 1 and transform them to follow the desired distribution.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk1",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This is called the inverse transform sampling method.\nstep by step explanation\n1-Generate a random number z from a uniform distribution between 0 and 1.\n2- Use the CDF of your target distribution  𝐹(𝑦)=∫𝑝(𝑦′)𝑑𝑦′\ntells you the probability that a sample is less than or equal to y.\nThis function maps a value of 𝑦 to a value between 0 and 1.\n*we use cdf because we can't compute the pdf in one point \n3- Step 3: Invert the CDF",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk2",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3- Step 3: Invert the CDF\nNow you do the reverse: you have a value z∈(0,1), and you want to find the value of \n𝑦 such that: F(y)=z Compute y=F−1(z)\nThis gives you a value 𝑦 such that a random sample from p(y) would have had a cumulative probability of 𝑧\n4- Step 4: Output y∼p(y).\nThe value 𝑦 you get is distributed according to p(y).\nBy repeating this process, you can generate as many samples from p(y) as you need.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk3",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Pros:\nExact samples.\nSimple and fast for standard distributions.\n❌ Cons:\nRequires invertible CDF F ^−1, which may not exist or be tractable.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk4",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. Rejection Sampling\nThe rejection sampling framework allows us to sample from relatively complex\ndistributions, subject to certain constraints. We begin by considering univariate distributions\nand discuss the extension to multiple dimensions subsequently.\nSuppose we wish to sample from a distribution p(z) that is not one of the simple,\nstandard distributions considered so far, and that sampling directly from p(z) is difficult.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk5",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Furthermore suppose, as is often the case, that we are easily able to evaluate\np(z) for any given value of z, up to some normalizing constant Z, so that\np(z) =1/Z p'(z) \nwhere \u0004p(z) can readily be evaluated, but Zp is unknown.\nIn order to apply rejection sampling, we need some simpler distribution q(z),\nsometimes called a proposal distribution, from which we can readily draw samples.\nnext introduce a constant k whose value is chosen such that kq(z) \u0002 \u0004p(z) for",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk6",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "all values of z. The function kq(z) is called the comparison function and is illustrated\nfor a univariate distribution in Figure 11.4. Each step of the rejection sampler\ninvolves generating two random numbers. First, we generate a number z0 from the\ndistribution q(z). Next, we generate a number u0 from the uniform distribution over\n[0, kq(z0)]. This pair of random numbers has uniform distribution under the curve",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk7",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "of the function kq(z). Finally, if u0 > \u0004p(z0) then the sample is rejected, otherwise\nu0 is retained.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk8",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.1.3 Adaptive rejection sampling\nIn many instances where we might wish to apply rejection sampling, it proves\ndifficult to determine a suitable analytic form for the envelope distribution q(z). An\nalternative approach is to construct the envelope function on the fly based on measured\nvalues of the distribution p(z) (Gilks and Wild, 1992). Construction of an\nenvelope function is particularly straightforward for cases in which p(z) is log concave,",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk9",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "in other words when ln p(z) has derivatives that are nonincreasing functions\nof z.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk10",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The function ln p(z) and its gradient are evaluated at some initial set of grid\npoints, and the intersections of the resulting tangent lines are used to construct the\nenvelope function. Next a sample value is drawn from the envelope distribution.\nThis is straightforward because the log of the envelope distribution is a succession\nof linear functions, and hence the envelope distribution itself comprises a piecewise\nexponential distribution of the form",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk11",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "exponential distribution of the form\nq(z) = kiλi exp {−λi(z − zi−1)} zi−1 < z \u0001 zi.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk12",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Once a sample has been drawn, the usual rejection criterion can be applied. If the\nsample is accepted, then it will be a draw from the desired distribution. If, however,\nthe sample is rejected, then it is incorporated into the set of grid points, a new tangent\nline is computed, and the envelope function is thereby refined. As the number of\ngrid points increases, so the envelope function becomes a better approximation of\nthe desired distribution p(z) and the probability of rejection decreases",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk13",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.1.4 Importance sampling\nimportance sampling provides a framework for approximating expectations directly\nbut does not itself provide a mechanism for drawing samples from distribution\np(z).\nThe finite sum approximation to the expectation, given by (11.2), depends on\nbeing able to draw samples from the distribution p(z). Suppose, however, that it is\nimpractical to sample directly from p(z) but that we can evaluate p(z) easily for any\ngiven value of z.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk14",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Importance sampling is a clever trick to estimate expectations (averages) with respect to a complex probability distribution p(z), without needing to sample directly from it.\nin simple words \"If I accidentally sampled 𝑧 from q(z), how much should I adjust (weight) this sample to make it represent what I would have gotten from p(z)?\"",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk15",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.1.5 Sampling-importance-resampling\nThe rejection sampling method discussed in Section 11.1.2 depends in part for\nits success on the determination of a suitable value for the constant k. For many\npairs of distributions p(z) and q(z), it will be impractical to determine a suitable",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk16",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "value for k in that any value that is sufficiently large to guarantee a bound on the\ndesired distribution will lead to impractically small acceptance rates.\nAs in the case of rejection sampling, the sampling-importance-resampling (SIR)\napproach also makes use of a sampling distribution q(z) but avoids having to determine\nthe constant k. There are two stages to the scheme. In the first stage,\nL samples z(1), . . . , z(L) are drawn from q(z). Then in the second stage, weights",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk17",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "w1, . . . , wL are constructed using (11.23). Finally, a second set of L samples is\ndrawn from the discrete distribution (z(1), . . . , z(L)) with probabilities given by the\nweights (w1, . . . , wL).\nThe resulting L samples are only approximately distributed according to p(z),\nbut the distribution becomes correct in the limit L → ∞.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk18",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "in another words a useful algorithm that builds on importance sampling but outputs samples that approximate your target distribution p(z, Turn weighted samples from importance sampling into unweighted samples approximately from p(z).\nSIR = Importance Sampling + Resampling",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk19",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.1.6 Sampling and the EM algorithm\nsampling methods can be used to approximate the E step of the EM algorithm for models in which the E step cannot be performed analytically. Consider a model with hidden variables\nZ, visible (observed) variables X, and parameters θ. The function that is optimized\nwith respect to θ in the M step is the expected complete-data log likelihood,",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk20",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "particular instance of the Monte Carlo EM algorithm, called stochastic EM,\narises if we consider a finite mixture model, and draw just one sample at each E step.\nHere the latent variable Z characterizes which of the K components of the mixture\nis responsible for generating each data point. In the E step, a sample of Z is taken\nfrom the posterior distribution p(Z|X, θold) where X is the data set. This effectively",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk21",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "makes a hard assignment of each data point to one of the components in the mixture.\nIn the M step, this sampled approximation to the posterior distribution is used to\nupdate the model parameters in the usual way.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk22",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.2 – Markov Chain Monte Carlo (MCMC)\nTo sample from complex, high-dimensional distributions (like posteriors) where other methods (rejection, importance sampling) fail due to inefficiency.\nConstruct a Markov chain whose stationary (long-term) distribution is p(z), and generate samples by running the chain.\n\tWe no longer require samples to be independent.\n\tOver time, the chain will generate samples from the correct target distribution.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk23",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Use a proposal distribution q(z ′∣z^(τ)) that suggests the next sample based on the current one. This forms a Markov chain, where each sample depends only on the previous one.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk24",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.2.1 – Markov Chains\n🔹 Definitions:\nA Markov chain is a sequence  where each z^(t+1)\n  depends only on z(t) .\nDefined by transition probabilities T(z'∣z).\n🔹 Invariant (Stationary) Distribution:\nA distribution p*(z) is invariant if:p *(z′)= ∑T(z′∣z)⋅p*(z)\n\n🔹 Ergodicity:\nThe chain converges to the stationary distribution regardless of the starting point.\nRequires the chain to be irreducible (can reach any state) and aperiodic.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk25",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.2.2 – The Metropolis-Hastings Algorithm\n🔹 Generalization of Metropolis Algorithm:\nDesigned to work with asymmetric proposal distributions q(z ′∣z).\n🔹 Algorithm Steps:\nGiven current state 𝑧, propose z'∼q(z′∣z).\nAccept 𝑧′ with probability: 𝐴(𝑧′,𝑧)=min(1,𝑝(𝑧′)𝑞(𝑧∣𝑧')/𝑝(𝑧)𝑞(𝑧′∣𝑧))\n(Only requires unnormalized p(z))\nIf accepted: z^(τ+1)=z′\nIf rejected: 𝑧^(τ+1)=z\n\nEnsures that the target distribution p(z) is invariant.\nSatisfies detailed balance.\nWorks even if 𝑍𝑝(normalizing constant) is unknown",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk26",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Concept\tDescription\nMCMC\tUses Markov chains to sample from complex distributions.\nMarkov Chain\tSequence where each sample depends only on the previous one.\nStationary Distribution\tThe long-run distribution the chain converges to.\nDetailed Balance\tStrong condition ensuring stationarity.\nMetropolis-Hastings\tMCMC method that handles asymmetric proposals and works with unnormalized distributions.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk27",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.3 – Gibbs Sampling\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) algorithm that generates samples from a complex joint distribution 𝑝(𝑧1,…,𝑧𝑀) by sampling each variable one at a time, conditioned on the others.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk28",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "You update z1 using the old values of all other variables.\nFor 𝑧2, you use the new value of 𝑧1, but still the old values for the rest.\nAnd so on, until you’ve updated all 𝑀 variables.\nThis process is repeated for many iterations, and the sequence of z's converges to samples from the joint distribution p(z).",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk29",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.4 – Slice Sampling\nGenerate samples from a distribution p(z) without requiring a step size (like Metropolis-Hastings) or knowledge of full conditional distributions (like Gibbs).\nInstead of sampling directly from p(z), slice sampling introduces an auxiliary variable 𝑢 and samples uniformly from the area under the curve of p'(z), the unnormalized distribution.\nIt transforms the 1D problem into sampling uniformly from a region in 2D space.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk30",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 30,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.5 – Hybrid Monte Carlo (HMC)\nTraditional MCMC methods (like Metropolis-Hastings and Gibbs sampling) struggle in high dimensions or when the target distribution is highly correlated. Their main weakness is the random walk behavior, which leads to slow exploration of the distribution.\n\nHMC fixes this by simulating physical dynamics that move smoothly through high-probability regions of the space",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk31",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 31,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "HMC introduces auxiliary momentum variables and simulates Hamiltonian dynamics (from physics) to propose new samples. These dynamics use gradients of the log-probability to guide movement, allowing large, efficient jumps that reduce random walk behavior.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk32",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 32,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.5.1 – Hamiltonian Dynamics\nWe want to simulate dynamics governed by Hamilton’s equations:\nThe first equation tells us how the position 𝑧𝑖 changes over time: it follows the momentum.\nThe second equation updates the momentum 𝑟𝑖: it’s pulled by the negative gradient of the potential energy, or equivalently the gradient of the log-probability.\n\nThis system conserves total energy H(z,r), and so if simulated accurately, we move through the space without rejecting proposals.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk33",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 33,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "But we can’t simulate these equations exactly, so we approximate them using a numerical integration method.\nA popular integrator that preserves energy reasonably well is the leapfrog method.\nIt works as follows (for time step ϵ):\n1- Half-step momentum update:\n2- Full-step position update:\n3- Another half-step momentum update\nThe leapfrog steps maintain reversibility and volume preservation, which are needed for valid MCMC sampling.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk34",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 34,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.5.2 – Hybrid Monte Carlo Algorithm\nNow we combine Hamiltonian dynamics with the Metropolis accept/reject step to correct for discretization error.\nHMC Algorithm (Full Steps)\nGiven the current sample 𝑧:\nSample momentum: r∼N(0,M)\nSimulate dynamics: Use the leapfrog method to simulate the trajectory (z,r)→(z′,r′) for L steps of size 𝜖.\nAccept/reject step: Accept 𝑧′ with probability: A=min(1,exp(−H(z′,r′)+H(z,r)))",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk35",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 35,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This corrects any error introduced by leapfrog (since 𝐻 isn’t perfectly conserved).\nIf accepted: z^(τ+1)=z′; else: stay at z^(τ)\n  Why HMC Works Well\nThe dynamics guide proposals using the gradient of the log-probability.\nProposals move along smooth trajectories — no random walk!\nEfficiently explores correlated, high-dimensional spaces.\nTuning Parameters\n𝜖 (step size): too big → energy error → low acceptance rate. Too small → high computation cost.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk36",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 36,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "𝐿 (number of leapfrog steps): too small → short moves. Too big → loops back and wastes effort.\n𝑀 (mass matrix): can be tuned to match the shape of the distribution (like preconditioning in optimization).",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk37",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 37,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.6 – Estimating the Partition Function\nAs we have seen, most of the sampling algorithms considered in this chapter require\nonly the functional form of the probability distribution up to a multiplicative",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk38",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 38,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "constant. 1/ZG exp (−G(z)) =∑T(z(l), z) the value of the normalization constant ZE, also known as the partition function, is not needed in order to draw samples from p(z). However, knowledge of the value of ZE can be useful for Bayesian model comparison since it represents the model evidence (i.e., the probability of the observed data given the model), and so it is of interest to consider how its value might be obtained. We assume that direct",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk39",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 39,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "evaluation by summing, or integrating, the function exp(−E(z)) over the state space\nof z is intractable.\nFor model comparison, it is actually the ratio of the partition functions for two\nmodels that is required. Multiplication of this ratio by the ratio of prior probabilities\ngives the ratio of posterior probabilities, which can then be used for model selection\nor model averaging.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk40",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 40,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.1 – Basic Sampling Algorithms\nTransformation method: Turning uniform samples into samples from other distributions.\n\nRejection sampling: Simple but inefficient in high dimensions.\n\nImportance sampling: Useful when you can't sample from the target distribution directly.\n\nSIR (Sampling Importance Resampling): Combines importance sampling with resampling.\n\nMonte Carlo EM: Uses sampling in EM when the E-step is intractable.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk41",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 41,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.2 – Markov Chain Monte Carlo (MCMC)\nConcept: Generates dependent samples from complex distributions via Markov chains.\n\nMetropolis-Hastings:\n\nAccept/reject based on a ratio.\n\nWorks with asymmetric proposals.\n\nErgodicity & detailed balance: Guarantees convergence to the target distribution.\n\n11.3 – Gibbs Sampling\nSpecial case of M-H where updates are from full conditional distributions.\n\nEfficient if conditional distributions are known and easy to sample.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk42",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 42,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "11.4 – Slice Sampling\nAdapts step size dynamically.\n\nUses auxiliary variable to sample uniformly under the distribution curve.\n\n11.5 – Hybrid Monte Carlo (a.k.a. Hamiltonian Monte Carlo)\nSimulates physical dynamics to avoid random walk behavior.\n\nEfficient in high-dimensional continuous spaces using gradient information.",
    "embedding": null,
    "source": "chapter 11 sampling methods.txt",
    "chunk_id": "chapter 11 sampling methods_chunk43",
    "meta_data": {
      "filename": "chapter 11 sampling methods.txt",
      "chunk_index": 43,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Gaussian Distribution \n- normal distribution \n- the distribution that maximizes the entropy is the Gaussian\n\tentropy as a measure of \"spread\" or \"uncertainty.\" The Gaussian distribution is the smoothest and least structured way to allocate \tprobability given fixed mean and variance. It is like saying, \"If I know only the average value and how much values deviate from it, \tthe most unbiased assumption is that the data follows a bell-shaped curve.\"",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk0",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This is akin to finding the \"least biased\" distribution given the known constraints.\n\tThe result means that, under the given constraints, the Gaussian distribution is the \"most random\" or \"least informative\" \tdistribution, making it the one with the highest entropy.\n-geometrical form- \n- The Gaussian distribution is shaped by its mean and variance (or covariance in higher dimensions).\n- In 1D, it forms a bell curve; in 2D, it forms a bell-shaped surface with elliptical contours; in",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk1",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "n-dimensions, it forms an n-dimensional \"bell.\"\n- The shape reflects uncertainty: wider regions of the curve/surface correspond to higher variability.\n-limitation of Gaussian-\n- the Gaussian cannot capture asymmetry in data and may fail to model distributions with significant skewness.\n- When outliers are present, the mean and variance (parameters of the Gaussian) are heavily affected, leading to poor model performance",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk2",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- It cannot model heavy-tailed distributions (e.g., those seen in financial data where extreme events are common), leading to underestimation of the probability of extreme events.\n- The Gaussian is fully described by only two parameters: mean 𝜇 and variance 𝜎2 It lacks flexibility to model more complex data shapes.\n- It cannot model discrete or categorical data directly",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk3",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.1 Conditional Gaussian Distributions\nIf a vector x follows a multivariate Gaussian distribution, its conditional distribution (given a subset of variables) is also Gaussian.\nX=(X1,X2) follows a joint Gaussian distribution\n- his formula gives the mean of X1 conditional on 𝑥2=x2 It is a linear combination of the mean of𝑋1 and the difference between the observed 𝑥2 and the mean of X2.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk4",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- This gives the variance of 𝑋1 given 𝑋2 . It depends on the variance of 𝑋1the covariance between 𝑋1and 𝑋2, and the variance of x2\n- The conditional Gaussian distribution describes how one variable is distributed when we know something about another related variable.\n- This conditional distribution is still Gaussian, meaning the \"shape\" of the distribution does not change, only the location and spread are modified by the conditioning variable.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk5",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.2 Marginal Gaussian Distributions\nIf a joint Gaussian distribution p(xa,xb) exists, the marginal distribution p(xa) is also Gaussian\nMarginal Gaussian distributions refer to the distribution of a subset of variables in a multivariate Gaussian distribution. By marginalizing over the other variables (integrating them out), the resulting marginal distribution will also be Gaussian, with a mean and covariance derived from the original multivariate distribution.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk6",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.3 Bayes’ Theorem for Gaussian Variables\nBayes' Theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis based on new evidence\nWhen combining Gaussian priors with Gaussian likelihoods, the posterior is also Gaussian.\n- The likelihood function models how probable the observed data is\n- The prior distribution represents our belief about the parameters before observing the data.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk7",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Gaussian Prior x Gaussian Likelihood → Gaussian Posterior: When combining a Gaussian prior with a Gaussian likelihood, the resulting posterior distribution is also Gaussian.\nPosterior Mean: The posterior mean is a weighted average of the prior mean and the data (observation), with weights inversely proportional to the variances (or precisions).",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk8",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Posterior Variance: The posterior variance is smaller than both the prior variance and the likelihood variance, reflecting the fact that observing the data reduces uncertainty.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk9",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.4 Maximum Likelihood for the Gaussian\nGiven a dataset{x 1,x2,…,xn} of n independent and identically distributed (i.i.d.) observations, the likelihood function L(μ,σ2) is the joint probability of observing all the data points under the assumption that each data point follows the Gaussian distribution",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk10",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.5 Sequential Estimation -time-",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk11",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Sequential estimation is a powerful technique that allows for the continuous estimation of parameters, such as the mean and variance, as new data becomes available. When the data is assumed to follow a Gaussian distribution, sequential estimation methods, like the Kalman filter and Welford’s method, allow for efficient and real-time updates of these parameters. The Gaussian distribution's properties, especially the closed-form expressions for updating the mean and variance, make it an ideal",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk12",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "updating the mean and variance, make it an ideal candidate for sequential estimation in many applications.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk13",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.6 Bayesian Inference for the Gaussian\nBayesian inference treats the parameters themselves as random variables, allowing for the inclusion of uncertainty in the model. This is particularly useful when you have prior information or belief about the distribution of the parameters before observing the data.\nSteps in Bayesian Inference for Gaussian Distribution",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk14",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "1-Prior Assumptions: You start with a prior belief about the mean 𝜇 and variance 𝜎2 which could be based on historical data or domain knowledge.\n\n2-Update with Data: As new data points 𝑥1,x2,..,𝑥𝑛 are observed, the likelihood function is updated, and the prior is combined with the likelihood to produce the posterior distribution.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk15",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3- Posterior Distribution: The posterior distribution reflects the updated belief about the parameters after observing the data. This distribution can then be used to make inferences about the parameters, such as calculating the posterior mean and posterior variance for 𝜇 and 𝜎2",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk16",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Bayesian inference for Gaussian distributions is a robust approach for estimating the parameters of a Gaussian distribution while incorporating prior beliefs and observed data. It provides not only point estimates but also uncertainty around those estimates through the posterior distribution. This makes it particularly powerful when dealing with real-world data where uncertainty is intrinsic, and prior knowledge can be useful in refining the estimates.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk17",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.7 Student’s t-Distribution\nThe Student's t-distribution is a probability distribution that arises when estimating the mean of a normally distributed population with a small sample size and unknown population standard deviation.\n- shape \n\tThe t-distribution is similar to the standard normal (Gaussian) distribution, but it has heavier tails. This means that extreme values (outliers) are more likely to occur compared to a normal distribution",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk18",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "with larger sample sizes, the estimation of the population variance becomes more accurate.\n- mean \n\tThe mean of the t-distribution is 0, assuming the underlying population is normally distributed.\n- variance \n\tThe variance of the t-distribution is \n𝑑𝑓\n−\ndf−2,\n\twhich is larger than 1 for degrees of freedom greater than 2. As the degrees of freedom increase, the variance \tapproaches 1, the same as the standard normal distribution.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk19",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- The Student's t-distribution can be seen as a generalization of the normal distribution in the sense that it becomes normal as the sample size grows. \n- The t-distribution is widely used in hypothesis testing, confidence intervals, and robust regression analysis.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk20",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.8 Periodic Variables\nProblem:\n\tGaussian distributions cannot model periodic data (e.g., angles, time of day).\n\tPeriodic variables require special treatment to ensure periodicity.\nIn cases where a variable exhibits periodic behavior, the covariance function (or kernel) used in the Gaussian process can include a periodic component. For example, a periodic kernel such as the Rational Quadratic kernel or Periodic kernel is used to model periodic variations in the data.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk21",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.3.9 Mixtures of Gaussians\nCombines multiple Gaussian distributions to capture complex data structures.\nmixture model is a probabilistic model that assumes the data is generated from a combination of several distributions. For periodic variables, a Gaussian mixture model (GMM) could be extended to model periodic behavior by using multiple Gaussian components that capture the periodic structure of the data.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk22",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.4: The Exponential Family\nThe exponential family is a broad class of probability distributions that is fundamental in statistics and machine learning. These distributions are highly structured, making them computationally convenient for both theoretical and practical purposes.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk23",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Key Properties of the Exponential Family:\nη: the natural (canonical) parameter.T(x): sufficient statistics of the data.\nA(η): log-partition function ensuring normalization.\nh(x): base measure independent of η\nThe exponential family simplifies Bayesian inference, as it often leads to conjugate priors.\nSufficient statistics reduce data to fixed dimensions, independent of the sample size.\nApplications:",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk24",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Applications:\nThe exponential family underpins many statistical models, including generalized linear models (GLMs) and graphical models.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk25",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.4.1 Maximum Likelihood and Sufficient Statistics\nSufficient Statistics:\nDefinition: Functions T(x) that summarize data without losing information about 𝜂\nThe MLE depends solely on the sufficient statistics, reducing computational complexity.\nFor large N , the log-likelihood is dominated by A(η)\n\n2.4.2 Conjugate Priors\nA prior p(η) is conjugate to the likelihood if the posterior p(η∣x) has the same functional form as (η).\nPosterior updates are straightforward\nSimplifies Bayesian inference.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk26",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.4.3 Noninformative Priors\nDefinition:\nA noninformative prior is chosen to have minimal influence on the posterior, representing ignorance about the parameters.\nCommon in hypothesis testing and where prior knowledge is unavailable.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk27",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.5 Nonparametric Methods (for small data)\nNonparametric methods provide a flexible framework for estimating probability distributions and modeling data without assuming a specific parametric form (e.g., Gaussian or exponential). These methods adapt to the complexity of the data\nThey estimate the density or predictive function directly from the data.\nNonparametric methods are particularly useful in cases where prior knowledge about the data distribution is limited.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk28",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.5.1 Kernel Density Estimators (KDE)",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk29",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "K(u): Kernel function, which is symmetric and integrates to 1.\nGaussian , Epanechnikov\nThe choice of kernel influences the estimator’s smoothness but has less impact than the bandwidth.\nh: Bandwidth (smoothing parameter), controlling the kernel's width\nCritical Role:\nSmall : Captures fine details but may lead to overfitting.\nLarge h: Smooths the data excessively, risking underfitting.\nSmoothness:\nKDE produces a continuous and differentiable density function.\nConvergence:",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk30",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 30,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Convergence: \nAs N→∞, KDE converges to the true density function.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk31",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 31,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2.5.2 Nearest-Neighbor Methods\nNearest-neighbor (NN) methods use the proximity of data points to make predictions or estimate density. They are intuitive and straightforward, relying on the assumption that nearby points in the feature space share similar properties.\nComparison of KDE and NN Methods:\nKDE:\nProduces a continuous density estimate.\nRelies on bandwidth ℎ, which is fixed across the dataset.\nNN:\nProvides density estimates based on local neighborhood sizes.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk32",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 32,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Adaptable to local data structures but computationally intensive.\nBoth methods are complementary, and their choice depends on the dataset characteristics and the application requirements.",
    "embedding": null,
    "source": "Gaussian Distribution ch2.txt",
    "chunk_id": "Gaussian Distribution ch2_chunk33",
    "meta_data": {
      "filename": "Gaussian Distribution ch2.txt",
      "chunk_index": 33,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Mixture Models and the EM Algorithm \nThis chapter introduces mixture models, a powerful statistical tool used for modeling complex probability distributions. The core idea behind mixture models is the use of latent variables, which allow us to express complicated distributions in terms of simpler components.\nLatent variables are variables that can only be inferred indirectly through a mathematical model from other observable variables that can be directly observed or measured.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk0",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.1 K-Means Clustering\n1. Introduction to Clustering\nThe goal of clustering is to group a set of data points into distinct categories or \"clusters\" based on similarity. this section, introduces K-means clustering, a widely used unsupervised learning algorithm for partitioning data into 𝐾 clusters.\nGiven Data:\nA dataset {x1,x2,...,xN} containing N observations, where each 𝑥𝑛​ is a D-dimensional Euclidean vector.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk1",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The goal is to divide these data points into 𝐾 distinct clusters, assuming 𝐾 is known beforehand.\nThe assumption is that points in the same cluster should be closer to each other than to points in different clusters.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk2",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. Defining Cluster Centers (𝜇𝑘 )\nTo formalize the clustering concept, the author introduces prototypes or cluster centers (𝜇𝑘) for each of the 𝐾 clusters.\nThe 𝜇𝑘 values represent the centroids (mean points) of each cluster.\nThe goal is to assign each data point to the cluster with the nearest 𝜇𝑘 and adjust 𝜇𝑘  iteratively to minimize an objective function.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk3",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3. Objective Function for K-Means (Distortion Measure)\nThe objective function in K-means is defined as:\nJ= ∑  ∑ rnk∥xn−μk∥^2\n  n=1k=1\nwhere: \n- 𝑟𝑛𝑘∈{0,1} is a binary assignment variable (1-of-K coding).\n- If data point 𝑥𝑛  belongs to cluster k, then 𝑟𝑛𝑘=1, otherwise 𝑟𝑛𝑘=0\n- The goal of K-means is to minimize 𝐽, which represents the total squared Euclidean distance between each data point and its assigned cluster center.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk4",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "4. The K-Means Algorithm (Two-Step Iterative Process)\n1. The K-means algorithm iteratively minimizes 𝐽 using two alternating steps:\nAssignment Step (E-Step equivalent):\nEach data point is assigned to the cluster whose center 𝜇𝑘 is closest in Euclidean distance\nrnk={ 1, if k=arg min j ∥xn−μj∥^2 otherwise 0\nThis step is equivalent to classifying each data point based on nearest-neighbor rules.\n2. Update Step (M-Step equivalent):",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk5",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. Update Step (M-Step equivalent):\nThe cluster centers 𝜇𝑘 are updated based on the new assignments by computing the mean of all data points assigned to that cluster\nEach cluster center is updated to be the mean (centroid) of all points assigned to it.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk6",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "5. Convergence and Properties of K-Means\nThese two steps are repeated iteratively until assignments no longer change (or a maximum number of iterations is reached).\nThe algorithm is guaranteed to converge because each iteration reduces the objective function J.\nHowever, convergence may be to a local minimum, not necessarily the global optimum.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk7",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "6. Limitations of K-Means\nsome key limitations of K-means:\nSensitivity to initialization: Poor initialization can lead to slow convergence or suboptimal solutions.\n\t\t      A common heuristic is to initialize as 𝜇𝑘 a random subset of the data points.\n\nNot robust to outliers: K-means minimizes squared distances, which means it is highly affected by outliers.\n\t\t\tK-medoids is an alternative method that reduces this sensitivity.\nK-medoids :",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk8",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Fixed K assumption: The number of clusters 𝐾 must be pre-defined, which may not always be known beforehand.\nElbo method \nAssumes spherical clusters: K-means works well when clusters are compact and well-separated, but it fails for complex cluster shapes.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk9",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.1.1 Image Segmentation and Compression\n1. Image Segmentation\nDefinition: Image segmentation is the process of dividing an image into meaningful regions (e.g., objects or similar textures).\nK-means Approach:\nEach pixel in an image can be treated as a data point in a 3-dimensional RGB space.\nThe algorithm clusters pixels based on color similarity.\nPixels in the same cluster are assigned the same mean color (𝜇𝑘 ), reducing the number of unique colors in the image.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk10",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This allows for segmentation based purely on color intensity, without considering spatial relationships.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk11",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. Image Compression Using K-Means (Vector Quantization)\nAnother key application of K-means is in lossy data compression.\nTraditional Images:\nA digital image consists of 𝑁 pixels, each represented by an RGB triplet (24 bits per pixel).\n\nCompression Idea:\nInstead of storing all pixel values, we store only the 𝐾 cluster centers and assign each pixel to its closest center.\nThis reduces the number of unique colors in the image.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk12",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Bit Savings Calculation:\nWithout compression: 24N bits needed.\nWith K-means: 𝑁log2 K+24K bits needed.\nCompression ratio Nlog2 K+24K/24N \n-p.s the log base is 2 \nHigher 𝐾 means better quality but lower compression. Optimal 𝐾 balances both.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk13",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.2 Mixtures of Gaussians\n1. Introduction to Gaussian Mixture Models (GMMs)\nA Gaussian Mixture Model (GMM) is an extension of the single Gaussian distribution that allows modeling complex, multimodal data distributions. Unlike a single Gaussian, which assumes the data is generated from one normal distribution, a GMM assumes that data points are drawn from multiple Gaussian components.\nEach Gaussian component is parameterized by:\nMean (𝜇𝑘 ): The center of the Gaussian.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk14",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Mean (𝜇𝑘 ): The center of the Gaussian.\nCovariance matrix (Σ𝑘 ): Defines the shape and spread of the Gaussian distribution.\nMixing coefficient (𝜋𝑘): The probability that a randomly chosen data point belongs to component 𝑘​",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk15",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. Role of Latent Variables in GMMs\nTo formalize the mixture model, we introduce latent variables 𝑧, which indicate which Gaussian component generated each data point.\n𝑧 follows a categorical distribution (one-hot encoded), meaning: 𝑝(𝑧𝑘=1)=𝜋𝑘\nThe conditional distribution of 𝑥 given 𝑧𝑘 =1 is Gaussian: p(x∣zk=1)=N(x∣μk,Σk)\nThe joint distribution of 𝑥 and 𝑧 is: p(x,z)=p(z)p(x∣z)\nThe marginal distribution of 𝑥 is obtained by summing over all possible 𝑧\n \n3. Responsibilities in GMMs",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk16",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3. Responsibilities in GMMs\nThe responsibility 𝛾(𝑧𝑘) is the posterior probability that a data point 𝑥 belongs to Gaussian component 𝑘, given by Bayes' theorem\nThis term plays a key role in the Expectation-Maximization (EM) algorithm, as it determines the degree to which each data point influences each Gaussian component.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk17",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.2.1 Maximum Likelihood Estimation (MLE) for GMMs\nSingularities in Maximum Likelihood for GMMs\nUnlike a single Gaussian, maximizing the likelihood for a GMM presents a major issue: singularities.\nIf one Gaussian collapses onto a single data point, its variance Σ𝑘 approaches zero, causing the likelihood to diverge to infinity.\nThis happens because the probability density of a Gaussian is inversely proportional to its variance, meaning as Σk →0, p(x) explodes.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk18",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "To prevent this, heuristics such as regularization or Bayesian priors are used.\n Identifiability Issue in GMMs\nSince GMMs allow permutation of components, any set of parameters can be arranged in K! different ways that produce the same likelihood.\nThis makes parameter interpretation ambiguous, though it does not affect the quality of density estimation.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk19",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.2.2 Expectation-Maximization (EM) for Gaussian Mixtures\n1. Why Use EM?\nThe log-likelihood function contains a log-sum, making direct maximization intractable.\nInstead, EM iteratively improves the likelihood in two steps:\nE-step: Compute the expected complete-data log-likelihood.\nM-step: Maximize this expectation with respect to model parameters.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk20",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. E-Step (Expectation Step)\nUsing the current parameter estimates (𝜋𝑘,𝜇𝑘,Σ𝑘), compute the responsibilities This represents the soft assignment of each data point to each Gaussian component.\n\n3. M-Step (Maximization Step)\nGiven the responsibilities 𝛾(𝑧𝑛𝑘), update the parameters:\nMean update (weighted average of data points assigned to cluster 𝑘)\nCovariance update (weighted covariance matrix)\nMixing coefficient update (fraction of data points assigned to cluster 𝑘)",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk21",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "4. Convergence of EM\nEM guarantees an increase in log-likelihood at each step.\nThe algorithm stops when the change in log-likelihood is below a threshold.\nHowever, EM only finds local optima, meaning different initializations can lead to different results.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk22",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.3 An Alternative View of the EM\nThe EM algorithm maximizes the likelihood function when the model contains unobserved (latent) variables.\nInstead of working with the marginal likelihood p(X∣θ), which involves a log-sum, EM considers the complete-data likelihood p(X,Z∣θ), where 𝑍 represents latent variables.\n\n2. General Framework of EM\nGiven:\nObserved data: X={x1,x2,...,xN}\nLatent variables:  Z={z1,z2,...,zN}\nModel parameters: 𝜃",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk23",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The log-likelihood function is:\nln p(X∣θ)=ln ∑p(X,Z∣θ)\nSince summation is inside the log, direct maximization is difficult. Instead, EM works as follows:\n\nE-Step (Expectation Step)\nCompute the expected log-likelihood over the latent variables, given the current estimate of the parameters: Q(θ,θold)= ∑p(Z∣X,θold)lnp(X,Z∣θ)\nwhere p(Z∣X,θold) is the posterior probability of the latent variables.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk24",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "M-Step (Maximization Step)\nFind the new parameters that maximize the expected log-likelihood:\nθnew =arg maxQ(θ,θ,old)\nEach iteration ensures that the likelihood never decreases.\n\nEM for Maximum A Posteriori (MAP) Estimation\nIf a prior p(θ) is introduced, EM can be used for MAP estimation instead of MLE.\nThe M-step maximizes: Q(θ,θold)+lnp(θ)\n\tThis avoids overfitting and prevents singularities in GMMs.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk25",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.3.1 Gaussian Mixtures Revisited\nHere, the author re-examines Gaussian Mixture Models (GMMs) using the alternative EM interpretation.\n\nComplete-Data Log-Likelihood\nIf we had full knowledge of which Gaussian component generated each data point, the log-likelihood would simplify to: ln p(X,Z|μ,Σ,π) =∑ ∑ znk {ln πk + lnN(xn|μk,Σk)}\n\t\t\t\t\tn=1k=1\nwhere 𝑧𝑛𝑘 is a binary variable indicating the component assignment",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk26",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Expectation Step (E-Step)\nCompute the expected value of 𝑧𝑛𝑘\n​\nMaximization Step (M-Step)\nThe new parameter estimates μk, ∑k, πk\n\nKey Takeaways\nEM is equivalent to MLE for GMMs, but the use of latent variables simplifies the optimization.\nThe responsibilities softly assign data points to Gaussian components",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk27",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.3.2 Relation to K-Means\n compares K-means and EM, highlighting their similarities and differences:\n1. Similarities\nBoth algorithms assign data points to clusters.\nBoth involve iterative optimization with alternating steps.\n2. Differences\nK-means uses hard assignments (𝑟𝑛𝑘∈{0,1}), while EM uses soft assignments (𝛾(𝑧𝑛𝑘).\n\nK-means minimizes squared Euclidean distance, while EM maximizes log-likelihood.\n\nK-means assumes equal variance for clusters, while EM estimates full covariance matrices.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk28",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3. K-Means as a Special Case of EM\nIf we assume Gaussian clusters with equal, small variances, EM reduces to K-means.\n𝛾(𝑧𝑛𝑘)→1(for closest cluster)\nThus, K-means is a limit case of EM for Gaussian mixtures.\n\n9.3.3 Mixtures of Bernoulli Distributions\nextends mixture models to binary data, using Bernoulli Mixture Models.\nEach data point 𝑥𝑛 is a binary vector (𝑥𝑖∈{0,1}).\nThe EM algorithm remains the same but with Bernoulli parameters instead of Gaussian ones.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk29",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.3.4 EM for Bayesian Linear Regression\nThe author applies EM to Bayesian regression, where model parameters have priors.\nLatent variable: The regression weights 𝑤.\nE-step: Compute posterior mean and covariance of 𝑤.\nM-step: Maximize the marginal likelihood to update hyperparameters 𝛼,𝛽",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk30",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 30,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "9.4: The EM Algorithm in General\n1. General Formulation of the EM Algorithm\n- The EM algorithm is a general optimization technique used to find maximum likelihood (ML) estimates for models with latent (unobserved) variables.\n- The goal is to maximize the marginal likelihood\n2. The EM Algorithm: Two-Step Iterative Approach\nThe EM algorithm circumvents the difficulty of optimizing the marginal likelihood by instead maximizing an expected complete-data log-likelihood in two alternating steps:",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk31",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 31,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Step 1: Expectation Step (E-Step)\nCompute the expected complete-data log-likelihood under the posterior distribution of the latent variables p(Z∣X,θold):\n𝑄(θ,θold)= ∑p(Z∣X,θold)ln p(X,Z∣θ)\nThis step fills in the missing information using the posterior over 𝑍.\nStep 2: Maximization Step (M-Step)\nFind the new parameter estimates by maximizing 𝑄(𝜃,𝜃old)\nθ new=arg max Q (θ,θ old)\nThis step updates 𝜃 in a way that increases the likelihood.\nIterate Until Convergence",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk32",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 32,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Iterate Until Convergence\n- The E-step and M-step are repeated until convergence, meaning the parameters 𝜃 stop changing significantly or the log-likelihood stabilizes.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk33",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 33,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3. Why EM Works: Theoretical Justification\nTo understand why EM increases the log-likelihood, the author introduces the Jensen’s Inequality and the variational lower bound.\n\n1. Jensen’s Inequality and the Log-Sum Problem\nThe difficulty in directly maximizing the log-likelihood comes from the log-sum structure\nSince the log function is concave, we can apply Jensen’s inequality:\nln∑𝑤𝑖 𝑓𝑖≥∑𝑤𝑖 ln 𝑓𝑖 ,for weights wi summing to 1\nThis provides a lower bound on the log-likelihood that EM maximizes.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk34",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 34,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. Lower Bound Interpretation (Variational View)\nDefine a distribution 𝑞(𝑍) over latent variables. \nlnp(X∣θ)= ∑q(Z)ln p(X,Z∣θ)/q(Z) + ∑ q(Z)ln q(Z)/p(Z∣X,θ)\n​-The first term is the variational lower bound (which EM maximizes).\n- The second term is the KL divergence between q(Z) and the true posterior 𝑝(𝑍∣𝑋,𝜃), which is always non-negative.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk35",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 35,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Thus, EM guarantees non-decreasing likelihood because each iteration tightens this lower bound.\n \n4. EM Convergence and Guarantees\n1. EM Increases the Likelihood at Each Iteration\n- At each iteration, the M-step maximizes 𝑄(θ,θold), which is a lower bound on the true log-likelihood.\n- This ensures: ln p(X∣θnew )≥ln p(X∣θ old) meaning EM never decreases the likelihood.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk36",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 36,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "2. EM Does Not Guarantee a Global Maximum\n- The log-likelihood often has multiple local maxima.\n- EM may get stuck in a local maximum depending on initialization.\n\n3. Stopping Criteria for EM\nThe algorithm is considered converged when:\n- The log-likelihood changes very little between iterations.\n- The parameter updates become small.\n- A fixed number of iterations is reached.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk37",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 37,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "5. Extensions and Applications of EM\n1. Generalized EM (GEM)\n- In the standard EM algorithm, the M-step fully maximizes Q(θ).\n- In Generalized EM (GEM), we increase (but do not fully maximize) Q(θ).\n- GEM is useful when exact maximization is difficult, such as in variational inference.\n\n2. EM in Bayesian Inference\nEM can also be used in Bayesian models, where instead of MLE, we maximize a posterior:\nQ(θ,θold)+lnp(θ)\n- This regularizes the estimates and prevents overfitting.",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk38",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 38,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3. Applications of EM\nThe EM algorithm is widely used in machine learning and statistics, including:\n- Gaussian Mixture Models (GMMs) (soft clustering)\n- Hidden Markov Models (HMMs) (speech recognition)\n- Topic modeling (Latent Dirichlet Allocation, LDA)\n- Missing data imputation",
    "embedding": null,
    "source": "Mixture Models and the EM Algorithm.txt",
    "chunk_id": "Mixture Models and the EM Algorithm_chunk39",
    "meta_data": {
      "filename": "Mixture Models and the EM Algorithm.txt",
      "chunk_index": 39,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "probabilistic graphical models.\nThese offer several useful properties:\n1. They provide a simple way to visualize the structure of a probabilistic model\nand can be used to design and motivate new models.\n2. Insights into the properties of the model, including conditional independence\nproperties, can be obtained by inspection of the graph.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk0",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "3. Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk1",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "A graph comprises nodes (also called vertices) connected by links (also known as edges or arcs). In a probabilistic graphical model, each node represents a random variable (or group of random variables), and the links express probabilistic relationships between these variables",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk2",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.1. Bayesian Networks\nBayesian networks (also known as directed graphical models) are a fundamental tool in probabilistic modeling. They allow us to represent complex probability distributions efficiently by leveraging conditional independence and the factorization of the joint distribution. These models use directed acyclic graphs (DAGs) to describe dependencies among random variables.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk3",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "it can also be used to Reducing computational complexity by breaking down the joint probability distribution into simpler conditional distributions.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk4",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Factorization of Joint Distributions in Bayesian Networks\nTo understand how Bayesian networks work, let’s begin with a simple example.\nConsider a joint probability distribution over three variables: p(a,b,c)\nUsing the product rule of probability, we can decompose it as:p(a,b,c)=p(c∣a,b)p(a,b)\nApplying the product rule again: p(a,b,c)=p(c∣a,b)p(b∣a)p(a)",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk5",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This decomposition shows how the joint distribution can be represented as a sequence of conditional probabilities, making it easier to work with.\n   a → b → c\na is the parent of b, and  b is the parent of 𝑐 \nThe missing direct connection between 𝑎 and 𝑐 suggests that 𝑐 is conditionally independent of 𝑎 given 𝑏.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk6",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "General Case: Factorization in Large Networks For a general Bayesian network with 𝐾 variables 𝑥1,𝑥2,…,𝑥𝐾, the joint probability factorizes as:\n𝑝(𝑥1,𝑥2,...,𝑥𝐾)=∏ 𝑝(𝑥𝑘∣𝑝𝑎𝑘)\n              k=1\nwhere 𝑝𝑎𝑘 represents the parent nodes of 𝑥𝑘. This equation captures the factorization property of Bayesian networks",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk7",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Directed Acyclic Graphs (DAGs)\nA Bayesian network must be a Directed Acyclic Graph (DAG), meaning: Directed: The edges have a direction (e.g., arrows point from cause to effect).\nAcyclic: No cycles exist (i.e., you cannot start from a node and follow the arrows to return to the same node).\n\nWhy must it be acyclic?\nWithout cycles, the conditional probabilities can be ordered and properly computed.\nCyclic graphs would require infinitely recursive probability calculations.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk8",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.1.1 Example: Polynomial regression\nTo illustrate Bayesian networks, Bishop provides an example based on Bayesian polynomial regression, introduced in Section 1.2.6. \nThe key variables in this model include:\n𝑤: The vector of polynomial coefficients.\n𝑡=(𝑡1,𝑡2,...,𝑡𝑁): Observed target values\n𝑥=(𝑥1,𝑥2,...,𝑥𝑁): Input data (not treated as random variables).\n𝛼: A hyperparameter representing the precision of the Gaussian prior over w.\n𝜎^2: Noise variance in observations.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk9",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "𝜎^2: Noise variance in observations.\nto explain how the graph has been develop look at page 363\nThe required predictive distribution for\u0001t is then obtained, from the sum rule of probability, by integrating out the model parameters w so that\np(t|x, x, t, α, σ2) ∝∫ p(t, t,w|x, x, α, σ^2) dw\nwhere we are implicitly setting the random variables in t to the specific values observed\nin the data set. The details of this calculation were discussed in Chapter 3.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk10",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.1.2 Generative models\nA generative model explicitly defines how data is generated by sampling from probability distributions.\nAncestral Sampling Algorithm\nTo sample from a Bayesian network:\nStart with a topological ordering of nodes (ensuring parents are sampled before children).\nSample from the prior distributions.\nProgressively sample child nodes given their parent values.\nThis process is called ancestral sampling and is used to generate synthetic data from the model.\neg.    B → G ← F",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk11",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "eg.    B → G ← F\nB (battery state) and 𝐹 (fuel state) influence 𝐺 (fuel gauge reading).\nInitially, B and 𝐹 are independent.\nIf we observe G=0 (fuel gauge shows empty), it creates a dependency between  𝐵and 𝐹.\nThis means that knowing one cause (e.g., battery is dead) reduces the probability of the other cause (e.g., fuel is empty). This is a key property of Bayesian networks.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk12",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.1.3 Discrete Variables\nIntroduction\nBayesian networks can be used to model discrete variables, which take values from a finite set (e.g., categories, states, or class labels). These variables are often represented using a 1-of-K encoding (one-hot encoding), where each discrete state is assigned a binary vector.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk13",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Probability Distribution for a Single Discrete Variable\nA discrete variable 𝑥 with 𝐾 possible states can be described using a multinomial distribution:\np(x∣μ)= ∏ μk^xk \n       k=1     \nwhere:\n𝜇=(𝜇1,𝜇2,...,𝜇𝐾) represents the probabilities of the 𝐾 states.\n𝑥𝑘 is a binary indicator (1 if 𝑥 is in state 𝑘, otherwise 0).\nThe probabilities sum to one: ∑ 𝜇𝑘=1\n                       k=1 \nSince there are 𝐾 probabilities, but they sum to 1, we only need to specify 𝐾−1 independent parameters.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk14",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Modeling Two Discrete Variables\nFor two discrete variables 𝑥1 and 𝑥2 , each with 𝐾 states, the joint probability distribution is given by:\n𝑝(𝑥1,𝑥2∣𝜇)=∏ ∏ 𝜇𝑘𝑙^𝑥1𝑘𝑥2𝑙 \n         k=1𝑙=1\nwhere 𝜇𝑘𝑙 represents the probability of 𝑥1 being in state 𝑘 and 𝑥2 in state 𝑙.\nIf 𝑥1 and 𝑥2 are fully dependent, we need to specify K^2−1 parameters.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk15",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "If 𝑥1 and 𝑥2 are conditionally independent, then the joint probability factorizes: 𝑝(𝑥1,𝑥2)=𝑝(𝑥1)𝑝(𝑥2) and we only need 2(K−1) parameters, reducing the model complexity.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk16",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Reducing Complexity in Multi-Variable Distributions\nWhen modeling multiple discrete variables, the number of parameters grows exponentially in a fully connected graph. To manage complexity, we:\nUse sparse graphs (drop unnecessary edges) – removing edges reduces the number of dependencies.\nUse structured models like chains\nShare parameters (parameter tying) – reuse the same probability distribution for different conditional probabilities.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk17",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Bayesian Models for Discrete Variables\nIn Bayesian networks, we can introduce Dirichlet priors over the parameters 𝜇.\n- Dirichlet distribution is the conjugate prior for the multinomial distribution.\n- A graph with Dirichlet priors has extra parent nodes representing these priors.\nFor example:\n- Without parameter tying: Each conditional distribution 𝑝(𝑥𝑖∣𝑥𝑖−1) has a separate Dirichlet prior.\n- With parameter tying: A single shared Dirichlet prior governs all conditional distributions.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk18",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This allows Bayesian learning, where we can update our beliefs about 𝜇 as we observe data.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk19",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.1.4 Linear-Gaussian Models\nIntroduction\nA Linear-Gaussian Model is a special case of Bayesian networks where:\n- Each node represents a continuous variable.\n- The conditional distributions are Gaussian.\n- The mean of each node is a linear function of its parents.\nLinear-Gaussian models are widely used in:\n- Probabilistic PCA\n- Factor Analysis\n- Kalman Filters and Linear Dynamical Systems",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk20",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Gaussian Nodes in a Directed Graph\nConsider a set of 𝐷 variables x1,x2 ,...,xD in a directed acyclic graph (DAG), where each variable follows a Gaussian distribution:\np(xi∣pai)=N(xi ∑ wijxj +bi,vi)\n            j∈pai\nwhere:\npai are the parents of 𝑥𝑖\nwij are linear coefficients defining how parents influence 𝑥𝑖.\n𝑏𝑖 is a bias term.\n𝑣𝑖 is the variance of the Gaussian.\nThis means each variable is a linear function of its parents plus Gaussian noise.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk21",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Factorization of the Joint Gaussian Distribution\nThe joint distribution of all variables is:\n     D\n𝑝(𝑥)=∏ 𝑝(𝑥𝑖∣𝑝𝑎𝑖)\n    𝑖=1\nSince each term is a Gaussian, the entire distribution is a multivariate Gaussian.\nUsing matrix notation, we can express it as:\nx=Wx+b+ϵ\nwhere:\n- 𝑊 is a lower-triangular matrix (since it’s a DAG).\n- ϵ∼N(0,V) is Gaussian noise.\n- The covariance matrix Σ can be computed recursively.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk22",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Extreme Cases: Fully Connected vs. Fully Disconnected\nFully Disconnected Graph\n- No links between nodes.\n- Each variable is an independent Gaussian:\n𝑝(𝑥𝑖)=𝑁(𝑏𝑖,𝑣𝑖)\n- The covariance matrix is diagonal.\n\nFully Connected Graph\n- Each node depends on all lower-numbered nodes.\n- The covariance matrix is full (general Gaussian).\n\nPartially Connected Graph\n- Some variables are conditionally independent.\n- The covariance matrix has zeros in certain positions.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk23",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Hierarchical Bayesian Models\n- A Gaussian prior on the mean 𝜇 of another Gaussian variable leads to a joint Gaussian distribution.\n- This can be extended to hyperpriors (priors on priors), forming a hierarchical Bayesian model.\n\nFinal Thoughts\nDiscrete Bayesian Networks help with categorical data (e.g., Weather → Mood).\nLinear-Gaussian Models help with continuous data (e.g., Study → Score).\nBoth use graphs to make calculations simpler and more efficient.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk24",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.2 Conditional Independence\nit helps simplify probability calculations and reduces the complexity of models.\nA variable 𝑎 is conditionally independent of 𝑏, given 𝑐, if: p(a∣b,c)=p(a∣c)\nThis means that once we know 𝑐, learning about 𝑏 gives us no extra information about 𝑎.\nex. Imagine a student’s grade (G) depends on both their intelligence (I) and the difficulty of the exam (D):\nI → G ← D \nBefore knowing 𝐺: Intelligence and Difficulty are independent.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk25",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "After seeing 𝐺: If we learn that the student got an A, and we already know the exam was easy, we might infer they are not very intelligent.\nThis means I and D become dependent given G, even though they were independent before.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk26",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.2.1 Three Example Graphs\nExample 1: Tail-to-Tail Structure \na->c<-b This means a and 𝑏 both depend on c.\nJoint Probability Factorization: (a,b,c)=p(a∣c)p(b∣c)p(c)\nIndependence Properties:\nWithout knowing c: a and 𝑏 are mutually exclusive.\nGiven 𝑐: 𝑎 and 𝑏 become independent\nExample in Real Life:\n𝑎 = Person's accent, \nb = Person's clothing style, \n𝑐= Their country of origin.\nBefore knowing their country: Accent and clothing style might seem correlated.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk27",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "After knowing their country: The correlation disappears—both are simply influenced by the country.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk28",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Example 2: Head-to-Tail Structure (Causal Chain)\n  a → c → b\nThis means 𝑎 influences 𝑐, and 𝑐 influences 𝑏.\nFactorization: (a,b,c)=p(a)p(c∣a)p(b∣c)\nIndependence Properties:\nWithout knowing c: 𝑎 and 𝑏 are dependent.\nGiven 𝑐: 𝑎 and 𝑏 become independent",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk29",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Example 3: Head-to-Head Structure (Explaining Away)\n  a → c ← b\nThis means 𝑐 has two possible causes: 𝑎 and 𝑏.\nFactorization:\np(a,b,c)=p(a)p(b)p(c∣a,b)\nIndependence Properties:\nWithout knowing 𝑐: 𝑎 and 𝑏 are independent.\nGiven 𝑐: 𝑎 and 𝑏 become dependent → 𝑎 and 𝑏 influence each other.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk30",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 30,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Example in Real Life (Explaining Away Phenomenon)\nImagine your car won’t start (𝑐). Two possible reasons:\n𝑎= The battery is dead.\n𝑏 = The fuel tank is empty.\nBefore checking the car, these two causes are independent. But once you observe that the car won’t start:\nIf you check and find that the battery is dead, you don’t need to check the fuel—because you already explained the problem.\nIf the battery is fine, you now strongly suspect the fuel is empty.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk31",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 31,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This is called \"explaining away\"—when two independent causes become dependent after observing the effect",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk32",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 32,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.2.2 D-Separation (Directed Separation) (How to Read Independence from a Graph)\nD-Separation Rule:\nTo check if two sets of nodes 𝐴 and 𝐵 are conditionally independent given 𝐶, follow this rule:",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk33",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 33,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "A path is BLOCKED if:\n- There is a head-to-tail (→) or tail-to-tail (←) connection, and the middle node is in 𝐶. \n- There is a head-to-head (→ ←) connection, and neither the middle node nor its descendants are in 𝐶.\nIf ALL paths between 𝐴 and 𝐵 are blocked, then 𝐴 and 𝐵 are conditionally independent given 𝐶.\nex.\na → c → e\n        ↓\n        d\nTo check if 𝑎 and 𝑒 are independent given 𝑐:\nPath: a→c→e → This is blocked since 𝑐 is given.\nPath: a→c→d→e → This is also blocked since 𝑐 is given.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk34",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 34,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "This means if we already know 𝑐, knowing 𝑎 tells us nothing extra about 𝑒.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk35",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 35,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.3 Markov Random Fields (Undirected Graphical Models)\nGraph type undirected graph\nfactorization use potential functions \nbest for mutual dependencies e.g. pixels in an image \nWhy use MRFs?\nSome problems don’t have clear directionality (e.g., in an image, neighboring pixels influence each other, but there is no strict \"cause-effect\" order).\nThey allow more flexibility in defining relationships between variables",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk36",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 36,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.3.1 Conditional Independence in MRFs\nJust like in Bayesian networks, conditional independence helps simplify MRFs.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk37",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 37,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "How do we check independence?\nIn Bayesian Networks: We use D-Separation (checking blocked paths).\nIn MRFs: We use Graph Separation → If removing a set of nodes disconnects two groups, they are conditionally independent.\n A — B — C\n |    |    |\n D — E — F\nAre A and C independent?\nNo, because there is a connection through B.\nAre A and C independent given B?\nYes! If we know 𝐵, knowing 𝐴 tells us nothing new about 𝐶.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk38",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 38,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Markov Blanket in MRFs\nIn a Markov Network, a node is independent of everything else given its neighbors.\n- The Markov Blanket of a node is just its direct neighbors.\n- This is simpler than Bayesian networks, where the Markov Blanket includes parents, children, and co-parents.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk39",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 39,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.3.2 Factorization Properties of MRFs\nSince there are no arrows in MRFs, we can’t use conditional probabilities like in Bayesian networks.\nInstead, we use potential functions (also called compatibility functions).\nHow does an MRF define probabilities?\nInstead of expressing the joint probability as a product of conditionals, we write it as: p(x)= 1/Z ∏𝜓c(xc)\nwhere:\n𝜓𝐶(𝑥𝐶) is a potential function over a clique C.\nZ is the normalization constant to ensure probabilities sum to 1.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk40",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 40,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "ex.  x1 — x2 — x3\nThe joint probability factorizes as:\np(x1,x2,x3) = 1/Z 𝜓(x1,x2)𝜓(x2,x3)\nThis says that the probability depends only on neighboring variables, not on all variables at once.\nWhy is this useful?\nIt lets us model dependencies flexibly.\nUnlike Bayesian Networks, we don’t need to specify a direction—just how variables relate.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk41",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 41,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.3.3 Illustration: Image de-noising\nMarkov Random Fields (MRFs) are used in image de-noising, a common problem in computer vision. The goal is to remove noise from an image while preserving important details, like edges.\nHow Does Image De-noising Work?\n- An image consists of pixels, each having an intensity value (e.g., black & white or color values).\n- Due to noise, some pixel values are incorrect (e.g., grainy or blurred areas).",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk42",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 42,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- The key assumption: A pixel’s true value is similar to its neighbors.\nMRFs are perfect for this because:\n- Each pixel is influenced by its neighbors (mutual dependency, not cause-effect).\n- There is no clear directionality, so an undirected graphical model (MRF) is a better fit than a Bayesian Network.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk43",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 43,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "How MRFs Model an Image\n1- Graph Representation\n- Each pixel is a node in the MRF.\n- Each edge connects a pixel to its neighboring pixels.\n- This means that each pixel depends only on its neighbors, not the entire image.\n2- Probability Model\n- The goal is to recover the \"true\" image (𝑥) from the noisy image (𝑦).\n- We define a probability distribution:\n𝑝(𝑥)=1/∏𝜓𝐶(𝑥𝐶)\n       C\n- 𝜓𝐶(𝑥𝐶) represents how smooth the image is (favoring similar neighboring pixel values).\n- Z is a normalization factor.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk44",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 44,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Z is a normalization factor.\nWe also model the likelihood of the noisy image 𝑦 given 𝑥:\n- p(y∣x)= ∏N(yi∣xi,σ^2)\n         i\n- This assumes the observed noisy pixel 𝑦𝑖 is close to the true pixel 𝑥𝑖 with Gaussian noise.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk45",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 45,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "How De-noising Works in MRFs\nWe estimate 𝑥 (true image) by maximizing the posterior probability:\np(x∣y)∝p(y∣x)p(x)\nThis is done using optimization techniques like:\nIterative algorithms (e.g., Gibbs sampling, Mean Field Approximation).\nBelief propagation to estimate the best values for 𝑥.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk46",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 46,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.3.4 Relation to directed graphs\nRelationship Between Bayesian Networks and MRFs\nHow are Bayesian Networks and MRFs related?\nAny Bayesian Network can be converted into an MRF.\nBut not every MRF can be represented as a Bayesian Network (because some dependencies can't be expressed with a directed graph).",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk47",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 47,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "How do we convert a Bayesian Network to an MRF?\nMoralization: In a Bayesian network, some nodes share a common child (e.g., A → C ← B).\nIn an MRF, we must add a link between these parent nodes (A—B) to make them a clique.\nRemove the arrows, keeping only undirected edges.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk48",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 48,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Example: Convert a Bayesian Network to an MRF\n A → C ← B\nStep 1: Remove arrows → Just keep A—B—C as an undirected graph.\nThe resulting MRF represents the same conditional independence properties, but without directionality.\nBayesian Networks are easier for causal reasoning, while MRFs are better for undirected dependencies\n A — B\n   \\ /\n    C\n\n8.4 Inference in Graphical Models\nInference in graphical models means computing probabilities or expectations of certain variables given observed data.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk49",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 49,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "For example:\n\nBayesian Networks (Directed Graphs): Given symptoms, infer the probability of a disease.\nMarkov Random Fields (MRFs) (Undirected Graphs): Given a noisy image, infer the clean image.\nTypes of Inference\nMarginal Inference: Compute 𝑝(𝑥𝑖) for a single variable.\nConditional Inference: Compute 𝑝(𝑥𝑖∣𝑥𝑗), given another variable.\nMAP (Maximum a Posteriori) Inference: Find the most likely state 𝑥∗ that maximizes p(x).",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk50",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 50,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.4.1 Inference on a Chain\nGraphical Models as Chains\nOne of the simplest structures in graphical models is a chain, where variables depend on each other sequentially:\nx1 → x2 → x3 → x4\nEach node depends only on its neighbor(s). This appears in:\nHidden Markov Models (HMMs) (sequence data).\nMarkov Chains (probability transitions).\n\nWe want to find:Marginal Probability\np(xk)= ∑p(x1,x2,...,xN)\n      𝑥−𝑘\nwhere 𝑥−𝑘  means sum over all variables except 𝑥𝑘.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk51",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 51,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Forward-Backward Algorithm\nForward Pass: Compute messages from left to right.\nBackward Pass: Compute messages from right to left.\n\nFigure 8.38 The marginal distribution\np(xn) for a node xn along the chain is obtained\nby multiplying the two messages\nμα(xn) and μβ(xn), and then normalizing.\nThese messages can themselves\nbe evaluated recursively by passing messages\nfrom both ends of the chain towards\nnode xn.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk52",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 52,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.4.2 Trees\nWhat are Trees?\nA tree is a graph where there is only one path between any two nodes.\n\nExample: A hierarchical model\n        A\n       / \\\n      B   C\n     / \\   \\\n    D   E   F",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk53",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 53,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Why Are Trees Important?\nFast inference: We can compute probabilities efficiently.\nMany real-world models are trees (e.g., decision trees, phylogenetic trees in biology).\nInference on Trees: Belief Propagation\nSimilar to forward-backward on a chain, but works on trees.\nEach node sends messages to its neighbors summarizing probability information.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk54",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 54,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.4.3 Factor Graphs\nWhy Factor Graphs?\nSometimes, directly representing a probability distribution is too complex.\nInstead, we break it into factors.\n\nExample: Bayesian Network\n A → B → C\n(a,b,c)=p(a)p(b|a)p(a|b)\nA factor graph explicitly shows how variables depend on factors:\nCircles (nodes) → Variables.\nSquares (factors) → Conditional probabilities.\n\n8.4.4 The Sum-Product Algorithm\nWhat is the Sum-Product Algorithm?\nA message-passing algorithm used in factor graphs for efficient inference.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk55",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 55,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "How It Works\nEach node sends messages to its neighbors:\n\nSumming out non-relevant variables (to simplify computation).\nMultiplying messages to compute probabilities.\nWhy is this Useful?\nIt’s much faster than brute-force marginalization.\nUsed in error correction codes, speech recognition, and AI\n\n8.4.5 The Max-Sum Algorithm\nWhat is the Max-Sum Algorithm?\nThe Max-Sum Algorithm (also called the Viterbi algorithm in some contexts) is used for Maximum a Posteriori (MAP) inference.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk56",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 56,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "In the Sum-Product Algorithm, we computed marginal probabilities (summing over variables).\nIn the Max-Sum Algorithm, instead of summing, we maximize the probability to find the most likely configuration (MAP estimate).\nWhere is it Used?\nHidden Markov Models (HMMs) → Finding the most likely sequence of hidden states.\nComputer Vision → Finding the best segmentation of an image.\nError Correction → Finding the most probable transmitted message.\nHow It Works",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk57",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 57,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "How It Works\nThe Sum-Product Algorithm computes: p(x)=∑ψ(x,y)\n                                         y",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk58",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 58,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The Max-Sum Algorithm replaces the sum with max\nThis finds the best possible assignment for the variables.\nIt still uses message passing like the Sum-Product Algorithm.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk59",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 59,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.4.6 Exact Inference in General Graphs\nWhy is Exact Inference Hard?\nIn chains and trees, inference is fast using message passing.\nBut in general graphs (with cycles), inference is NP-hard (exponentially slow).\nTo handle general graphs, we use the Junction Tree Algorithm:\nConvert the graph into a tree (a junction tree).\nRun belief propagation on this tree.\nSteps:\nTriangulation: Add extra edges to remove cycles.\nForm Cliques: Group variables into cliques.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk60",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 60,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Form Cliques: Group variables into cliques.\nRun Message Passing on the junction tree.\nCliques : a subset of vertices of an undirected graph such that every two distinct vertices in the clique are adjacent",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk61",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 61,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.4.7 Loopy Belief Propagation\nWhat is Loopy Belief Propagation (LBP)?\nWhen a graph has cycles, exact inference isn’t feasible.\nInstead, we use an approximate method called Loopy Belief Propagation (LBP).\nHow It Works:\nIgnore cycles and still run message passing like in a tree.\nIterate multiple times until probabilities converge.\nWorks well in many cases, even though it's not exact.\nUsed in error correction codes (Turbo codes), image denoising, and deep learning.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk62",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 62,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "If the graph has strong cycles, messages might oscillate instead of converging.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk63",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 63,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "8.4.8 Learning the Graph Structure\nWhy Learn the Graph Structure?\nIn real-world problems, we don’t always know how variables are connected.\nWe must learn the structure from data.\nTwo Approaches to Learning:\nConstraint-Based Learning",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk64",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 64,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Uses statistical tests to find conditional independence.\nExample: If 𝐴 and 𝐶 are independent given 𝐵, we remove the edge A−C.\nScore-Based Learning\nAssigns a score to different graph structures and finds the best one.\nExample: Bayesian Information Criterion (BIC) and Bayesian Score measure how well the graph fits the data.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk65",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 65,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "✔ Chains and trees allow efficient probability calculations using message passing.\n✔ Factor graphs break complex models into simple factors.\n✔ The sum-product algorithm speeds up inference significantly.\n✔ Max-Sum Algorithm → Finds the most probable state instead of marginal probabilities.\n✔ Junction Tree Algorithm → Converts complex graphs into trees for exact inference.\n✔ Loopy Belief Propagation → Approximate inference in graphs with cycles.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk66",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 66,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "✔ Learning Graph Structures → Discovering relationships directly from data.",
    "embedding": null,
    "source": "probabilistic graphical models..txt",
    "chunk_id": "probabilistic graphical models._chunk67",
    "meta_data": {
      "filename": "probabilistic graphical models..txt",
      "chunk_index": 67,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "It is event-driven and uses asynchronous, non-blocking I/O.\nExpress.js is a highly configurable framework for building applications on Node.js.\nIt abstracts lower-level APIs in Node.js by using  HTTP utility methods and middleware.\nBefore you build your first Node.js app, let’s get familiar with the IDE and some key Node.js\nconcepts.\nExpress.js simplifies application development on Node.js.\nThe following features enable you to develop your application quickly:",
    "embedding": null,
    "source": "Reactive programming .txt",
    "chunk_id": "Reactive programming _chunk0",
    "meta_data": {
      "filename": "Reactive programming .txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Public: public assets like image, CSS, and javascript.\nTemplates/views: server-rendered HTML that is sent back to the client in response to\nrequests.\nRoutes: defines endpoints that accept and process client requests.\nServer.js: a file which contains the main application code.\nPackage.json: contains metadata information about the project including dependencies,\nscripts, and so on.",
    "embedding": null,
    "source": "Reactive programming .txt",
    "chunk_id": "Reactive programming _chunk1",
    "meta_data": {
      "filename": "Reactive programming .txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Sparse Kernel Machines\nKernel-based learning algorithms often require evaluating kernel functions for all training data points, leading to computational inefficiencies. Sparse kernel machines address this by using a subset of data points, reducing computational cost.\nThe two main methods discussed:\nSupport Vector Machines (SVMs): Based on maximum margin classification.\nRelevance Vector Machines (RVMs): Bayesian formulation that leads to sparser solutions and provides probabilistic outputs.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk0",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 0,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.1. Maximum Margin Classifiers\nWe begin our discussion of support vector machines by returning to the two-class classification problem using linear models of the form y(x) = wTφ(x) + b\n\nThe training set consists of input vectors xn with corresponding labels 𝑡𝑛∈{−1,1}\nThe classification decision is based on the sign of 𝑦(x).",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk1",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 1,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The goal is to find a hyperplane that maximizes the margin, which is the distance between the closest points (support vectors) and the decision boundary.\n- If the data is linearly separable, there exists at least one hyperplane that classifies all points correctly\n- To solve this constrained optimization, we introduce Lagrange multipliers Setting the derivatives to zero, Substituting 𝑤 into the Lagrangian gives the dual formulation",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk2",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 2,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "** Lagrange multipliers if decision boundary is linear then you can call g(x) if you subtract it and solve it and multiply it to the lagrange we will have the solutions \nThe final classifier is:𝑦(𝑥)=∑𝛼𝑛𝑡𝑛𝑘(𝑥,𝑥𝑛)+𝑏\n\t\t\t    𝑛∈𝑆",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk3",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 3,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "this means that:\nIf 𝛼𝑛>0, then 𝑡𝑛𝑦(𝑥𝑛)=1 (these are support vectors).\nIf 𝛼𝑛=0, then 𝑥𝑛 does not affect the decision boundary",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk4",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 4,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.1.1 Overlapping class distributions\nIn real-world cases, classes often overlap.\nwe introduce slack variables, ξn >= 0 where\nn = 1, . . . , N, with one slack variable for each training data point \nThese are defined by ξn = 0 for data points that are on or inside the correct margin boundary and ξn = |tn − y(xn)| for other points. Thus a data point that is on the decision boundary y(xn) = 0 will have ξn = 1,",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk5",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 5,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The new optimization problem is: \nmin 1/2∥𝑤∥^2+𝐶∑𝜉𝑛\n𝑤,𝑏\nThe C parameter balances:\nMargin size (∥𝑤∥^2)\nMisclassification penalty (∑𝜉𝑛)\nThe dual problem remains similar, but now:0≤𝛼𝑛≤𝐶\nKernel Trick: Instead of explicitly mapping data to high-dimensional feature spaces, SVMs use a kernel function:\n𝑘(𝑥,𝑥′)=𝜙(𝑥)^𝑇𝜙(𝑥′) allowing computations to remain in input space.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk6",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 6,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.1.2: Relation to Logistic Regression\nError Functions in SVM and Logistic Regression\nSVM uses the hinge loss function:\nESV(yt)=[1−yt]+ \nwhere [z] +means max(0, z).\nIf yt≥1, no loss is incurred.\nIf yt<1, the loss increases linearly.\nLogistic Regression uses the logistic loss function:\n𝐸𝐿𝑅(𝑦𝑡)=ln(1+𝑒^−𝑦𝑡)\nThis function is smooth and differentiable.\nUnlike SVM, it gives probabilistic outputs.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk7",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 7,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Unlike SVM, it gives probabilistic outputs.\nThe hinge loss is an approximation of the misclassification error, while logistic loss is a smooth function.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk8",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 8,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Regularization\nBoth methods use regularization to prevent overfitting both typically uses L2 regularization\n\nSVM has no Probabilistic Outputs",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk9",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 9,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.1.3 Multiclass SVMs\nSince SVMs are naturally binary classifiers, multiclass classification is handled by:\nOne-vs-Rest (OvR): Train a separate SVM for each class.\n- using the decisions of the individual classifiers can lead to inconsistent results in which an input is assigned to multiple classes simultaneously.\n- the training setsare imbalanced.\nOne-vs-One (OvO): Train an SVM for each pair of classes.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk10",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 10,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- Another approach is to train K(K−1)/2 different 2-class SVMs on all possible pairs of classes, and then to classify test points according to which class has the highest\nnumber of ‘votes’,\nError-correcting Output Codes: Combine binary classifiers with redundancy.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk11",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 11,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "- The K classes themselves are represented as particular sets of responses from the two-class classifiers chosen, and together with a suitable decoding scheme, this gives robustness to errors and to ambiguity in the outputs of the individual classifiers.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk12",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 12,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.1.4 SVMs for regression\nStandard Regression: This approach does not enforce sparsity, meaning all data points contribute to the prediction.\n\nSVR Solution: Instead of minimizing squared errors, SVR introduces an ε-insensitive loss function, allowing some flexibility in the error",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk13",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 13,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The ε-Insensitive Loss Function\nIn SVR, we define a \"tube\" of width ε around the target values.\nPredictions within this ε-margin incur zero error.\nOnly points outside the ε-boundary contribute to the loss.\nThis means:\nIf the prediction y(x) is within 𝜖 of 𝑡, no penalty is applied.\nIf the error exceeds 𝜖, it grows linearly (instead of quadratically like in least squares).",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk14",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 14,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The error function for support vector regression can then be written as\nC∑(ξn +ξn) +1/2||w||2\nn=1\nwhere:\n𝜉𝑛 and 𝜉𝑛 are slack variables for points above and below the tube.\n𝐶 controls the trade-off between margin width and error penalty\n\nAn alternative ν-SVR method fixes the fraction of points that lie outside the margin instead of setting ε manually.\nwhere ν controls:\nThe fraction of support vectors.\nThe fraction of points violating the ε-boundary.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk15",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 15,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.1.5 Computational learning theory\nHistorically, support vector machines have largely been motivated and analysed\nusing a theoretical framework known as computational learning theory, also sometimes\ncalled statistical learning theory The goal of the PAC framework is to understand how large a data set needs to be in order to give good generalization. It also gives bounds for the computational cost of learning, although we do not consider these here.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk16",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 16,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Suppose that a data set D of size N is drawn from some joint distribution p(x, t)\nwhere x is the input variable and t represents the class label, and that we restrict\nattention to ‘noise free’ situations in which the class labels are determined by some\n(unknown) deterministic function t = g(x). In PAC learning we say that a function\nf (x;D), drawn from a space F of such functions on the basis of the training set\nD, has good generalization if its expected error rate is below some pre-specified",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk17",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 17,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "threshold \u000e, so that\nEx,t [I (f (x;D) != t)] < 𝜖",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk18",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 18,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "in other words they strongly over-estimate\nthe size of data sets required to achieve a given generalization performance. For this\nreason, PAC bounds have found few, if any, practical applications.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk19",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 19,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.2. Relevance Vector Machines\nis a Bayesian sparse kernel technique for regression and classification that shares many of the characteristics of the SVM whilst avoiding its principal limitations. Additionally, it typically leads to much sparser models resulting in correspondingly faster performance on test data whilst maintaining comparable generalization error",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk20",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 20,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.2.1 RVM for regression\nRVM is based on a linear model similar to standard regression, but with a modified prior distribution that enforces sparsity.\np(t|x,w, β) = N(t|y(x), β^−1)\nwhere β = σ−2 is the noise precision (inverse noise variance), and the mean is given by a linear model of the form\ny(x) =∑wiφi(x) = wTφ(x)\n     i=1\nwith fixed nonlinear basis functions φi(x), which will typically include a constant term so that the corresponding weight parameter represents a ‘bias’.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk21",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 21,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Bayesian Framework: Introducing Sparsity\nKey idea: Instead of using a fixed regularization term, RVM places a prior on the weights: 𝑝(𝑤∣𝛼)=∏𝑁(𝑤𝑖∣0,𝛼𝑖^−1)\nEach weight 𝑤𝑖 has an individual precision parameter 𝛼𝑖.\nThis automatically prunes irrelevant basis functions (leading to sparsity).\nMany 𝛼𝑖 values go to infinity, forcing corresponding 𝑤𝑖 to be zero",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk22",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 22,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Given the prior and the likelihood function, we compute the posterior distribution for the weights\nThe hyperparameters 𝛼 and 𝛽 are learned using Type-II Maximum Likelihood (Evidence Approximation).\nRVM provides uncertainty estimates, making it useful for probabilistic inference.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk23",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 23,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "7.2.2: Analysis of Sparsity in RVM\nThe sparsity mechanism in RVM arises due to the Bayesian automatic relevance determination (ARD) framework, which selectively removes basis functions that contribute little to the model.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk24",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 24,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Why Does Sparsity Occur in RVM?\nIn SVMs, sparsity occurs because only support vectors have nonzero weights.\nIn RVMs, sparsity is stronger because the Bayesian framework allows automatic pruning of many weights 𝑤𝑖 to zero.\nThis happens because each weight 𝑤𝑖 has its own precision parameter 𝛼𝑖 n the prior\nWhen optimizing 𝛼 using evidence maximization, many 𝛼𝑖 values become very large (→∞), forcing the corresponding weights 𝑤𝑖 to zero.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk25",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 25,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "If a basis function 𝜙𝑖(𝑥) is poorly aligned with 𝑡, then setting 𝛼𝑖 →∞ increases the likelihood of the data.\nThis effectively removes that basis function from the model.\n\n7.2.3: RVM for Classification\nRVM extends to classification by replacing the Gaussian likelihood (regression) with a Bernoulli likelihood.\n\nBayesian Inference for RVM Classification\nInstead of solving directly, we approximate the posterior using Laplace’s approximation.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk26",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 26,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "The posterior distribution for w is approximated by a Gaussian:\np(w∣t,X,α)≈N(w∣m,Σ)\nwhere:\nMean: m=argmax p(w∣t,X,α) (found using optimization).\nCovariance:Σ=(A+Φ^T*RΦ)^−1\nA=diag(α) is the prior precision matrix.\nR is a diagonal matrix with elements: 𝑅𝑛𝑛=𝜎(𝑦𝑛)(1−𝜎(𝑦𝑛))\nThis comes from the Hessian of the log-likelihood, ensuring proper variance estimation.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk27",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 27,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Iterative Re-Estimation of Hyperparameters \n𝛼 Just like in RVM regression, the hyperparameters αi\nare updated as:𝛼𝑖new=𝛾𝑖/𝑚𝑖^2\nwhere:𝛾𝑖=1−𝛼𝑖Σ𝑖𝑖\nEffect of α update:\nIf 𝛾𝑖 is small → 𝛼𝑖 grows large → 𝑤𝑖 is forced to zero.\nThis removes irrelevant basis functions → leads to sparsity.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk28",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 28,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  },
  {
    "text": "Making Predictions with RVM Classifier\nAfter training, the predictive probability for a new input 𝑥 is computed as: 𝑝(𝑡=1∣𝑥,𝑡,𝑋,𝛼)=𝜎(𝑚𝑇𝜙(𝑥))\nThis gives a probabilistic output, unlike SVMs, which only provide a hard decision.",
    "embedding": null,
    "source": "Sparse Kernel Machines.txt",
    "chunk_id": "Sparse Kernel Machines_chunk29",
    "meta_data": {
      "filename": "Sparse Kernel Machines.txt",
      "chunk_index": 29,
      "created_time": "2025-06-21T22:46:13.267949"
    }
  }
]