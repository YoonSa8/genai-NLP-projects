5.1. Feed-forward Network Functions
y(x,w) = f
(M
sum wjφj(x))
j=1    
Unlike traditional linear models with fixed basis functions, neural networks allow adaptive basis functions, enabling them to learn from data more effectively.

Neural networks consist of multiple layers of neurons:
Input layer: Receives input features.
Hidden layer(s): Applies nonlinear transformations.
Output layer: Produces the final prediction.
Each layer computes a weighted sum of inputs, followed by a nonlinear activation function.

Activation functions include sigmoid, tanh, and softmax, depending on the problem type.

Properties:
Universal Approximation Theorem: A two-layer neural network with enough hidden units can approximate any continuous function.

5.1.1 Weight-space Symmetries
Weight-space symmetries: Multiple configurations of weights can produce the same network output.

2. Types of Weight-space Symmetries
- Hidden Unit Sign-Flipping Symmetry
If we take a hidden unit j and change the sign of all the weights feeding into it, the activation a_j also flips its sign
However, we can compensate for this sign change by flipping the sign of the corresponding second-layer weights: Wkj(2)→−Wkj(2)
This results in the same final output, meaning that the two sets of weights are functionally equivalent. Since each hidden unit can have its weights flipped independently, there are 2^M equivalent weight configurations for a network with M hidden units.

-Hidden Unit Permutation Symmetry
The order of the hidden units does not matter, as long as the connections are adjusted accordingly.
Suppose we swap the weights of two hidden units, say unit 1 and unit 2:
Swap all first-layer weights connected to these units.
Swap all second-layer weights leading out of these units.
Since the output remains unchanged, we again have an equivalent weight configuration.
With M hidden units, there are M! (M factorial) ways to permute them, meaning an additional M! symmetric weight configurations.

 Why Weight-space Symmetries Matter
(a) Implications for Optimization
Since there are multiple equivalent solutions, gradient-based optimization methods like stochastic gradient descent (SGD) can converge to any of these symmetric minima.
The optimizer may explore different regions of weight-space that correspond to the same function.
(b) Bayesian Neural Networks
In Bayesian approaches (discussed in Section 5.7), symmetries in weight space must be accounted for when computing model evidence.
(c) Pruning and Interpretability
Weight symmetries can make it harder to interpret the learned weights of a network because the same function can be represented in different ways.

5.2: Network Training 
Training a neural network means finding the best weights w to minimize an error function E(w), which measures how well the network’s predictions match the target outputs. This is typically done using gradient-based optimization methods

The choice of error function (or loss function) depends on the type of problem being solved:
-Regression Problems This leads to the sum-of-squares error function
- Classification Problems: 
The choice of error function is based on the likelihood function
Binary Classification : The cross-entropy error function (negative log-likelihood)
Multiclass Classification: The network uses the softmax function for output also Cross-entropy loss

The Error Surface and Local Minima
The error function E(w) defines a surface over weight space.
Training involves finding a minimum of this surface.
Difficulties in optimization:
Local Minima: Some weight settings lead to small but not the absolute lowest error.
Saddle Points: Points where the gradient is zero but it’s neither a minimum nor maximum.
Plateaus and Flat Regions: The error function changes slowly, making learning slow.

Since exactly solving 
∇E(w)=0 is infeasible, we use iterative methods:

(A) Gradient Descent (Steepest Descent)
The simplest method updates weights using the gradient of the error function: w (t+1) =w (t) −η∇E(w)
where η is the learning rate.

(B) Stochastic Gradient Descent (SGD)
Instead of computing the gradient over the entire dataset, we update weights after each data point

(C) Mini-Batch Gradient Descent
Uses a small batch of data points at each step, balancing between stability and efficiency.

(D) More Advanced Methods
Momentum: Helps the network keep moving in the right direction, avoiding slowdowns in plateaus.
Adaptive Learning Rates: Methods like Adam, RMSprop, and Adagrad adjust learning rates dynamically.

 Convergence and Practical Considerations
Weight Initialization: Randomized small values to prevent neurons from becoming identical.
Learning Rate Scheduling: Decreasing 𝜂 over time to stabilize training.
Early Stopping: Stops training when performance stops improving on a validation set to prevent overfitting.
Regularization (Weight Decay): Adds a term like λ∣∣w∣∣2 to the error function to prevent overfitting

5.3: Error Backpropagation
The backpropagation algorithm is a fundamental method for training feed-forward neural networks. It efficiently computes the gradient of the error function with respect to the network's weights by propagating errors backward through the network. This allows us to use gradient-based optimization methods like Stochastic Gradient Descent (SGD) to update the weights.

5.3.1 Evaluation of Error-function Derivatives
The goal is to compute the gradient of the error function with respect to the weights efficiently.
- The gradient can be computed recursively using the chain rule.
- The weight gradient is given by: ∂𝐸/∂𝑤𝑗𝑖=𝛿𝑗𝑧𝑖
Key observation: The weight update depends on the error signal 𝛿𝑗 at the output end and the activation 𝑧𝑖  at the input end.

5.3.2 A Simple Example
To illustrate backpropagation, we consider a two-layer neural network with:
Sigmoid hidden units: h(a)=tanh(a)
Linear output units: y k=ak
​Sum-of-squares error function
Backward Pass
Compute the error term at the output layer: 𝛿𝑘=𝑦𝑘−𝑡𝑘
ompute the error term at the hidden layer:
𝛿𝑗=(1−𝑧𝑗2)∑ 𝑤𝑘𝑗(2)𝛿𝑘
         𝑘 
Compute the weight gradients:
First-layer weights:∂E/∂Wji(1)= 𝛿jxi
Second-layer weights::∂E/∂Wkj(2)= 𝛿kzj

5.3.3 Efficiency of Backpropagation
Backpropagation is computationally very efficient compared to numerical differentiation.
 Computational Complexity O(W)
- Since the network has many weights, backpropagation is much faster.

5.3.4 The Jacobian Matrix
The Jacobian matrix measures how the outputs of a neural network change with respect to inputs.
where each entry represents how much the output 𝑦𝑘 changes when input 𝑥𝑖 is perturbed

5.4: The Hessian Matrix
 It is the second derivative of the error function with respect to the network's weights, providing insights into local minima, saddle points, and convergence speed
Although the Hessian can improve training efficiency and model uncertainty estimation, computing and inverting it directly is expensive. Various approximations exist to make it practical.

5.4.1 Diagonal Approximation
Since the Hessian is a large matrix (𝑊×𝑊, where W is the number of weights), we often use a diagonal approximation, keeping only the second derivatives along the diagonal.
- Off-diagonal terms (which show interactions between weights) are ignored.
- The Hessian is cheap to compute because we only calculate individual second derivatives.
- The inverse of a diagonal matrix is trivial, making this useful for optimization methods like Newton’s method.

5.4.2 Outer Product Approximation (Levenberg-Marquardt Approximation)
This approximation assumes that the Hessian can be estimated using the outer product of gradients, leading to a simplified but effective estimation 
​
5.4.3 Inverse Hessian Approximation
The Hessian’s inverse is important for many applications, including Bayesian inference and optimization. Direct inversion is computationally expensive, so approximations are used.
The inverse Hessian helps determine optimal learning rate adjustments and model uncertainty.

5.4.4 Eigenvalue Analysis of the Hessian
🔹 Why Eigenvalues Matter?
The eigenvalues of the Hessian determine the curvature of the error function.
Large eigenvalues → sharp minima (high curvature) → fast convergence but potential instability.
Small eigenvalues → flat regions (low curvature) → slow convergence.
🔹 Key Properties:
The Hessian is positive definite if all eigenvalues are positive, meaning the point is a local minimum.
The Hessian is negitave definite if all eigenvalues are positive, meaning the point is a local maxima.

If some eigenvalues are negative, the point is a saddle point (not a minimum).
If some eigenvalues are zero, there are flat directions, leading to slow learning.
🔹 Practical Implications:
Large differences between the smallest and largest eigenvalues (high condition number) cause slow optimization.
Eigenvalue analysis helps in selecting adaptive learning rates (larger for flat regions, smaller for sharp minima).

5.4.5 The Hessian and Generalization
The Hessian provides insights into overfitting and model complexity.
🔹 Connection to Model Complexity:
A flat minimum (small eigenvalues) means the model is robust to small changes → better generalization.
A sharp minimum (large eigenvalues) means small weight changes cause large errors → more prone to overfitting.
🔹 Hessian-Based Regularization
Regularization techniques like weight decay or Bayesian learning use Hessian properties to prevent overfitting.
Smaller eigenvalues in the Hessian suggest the network has learned useful features rather than memorizing noise.
🔹 Bayesian View:
In Bayesian inference, the Hessian helps define a Gaussian approximation to the posterior over weights.
This leads to uncertainty quantification, allowing confidence estimates for predictions.

5.4.6 Fast Approximation Methods
Because computing the full Hessian is expensive, several approximations are used in practice.
🔹 Quasi-Newton Methods
BFGS (Broyden-Fletcher-Goldfarb-Shanno) approximates the inverse Hessian iteratively.
Requires less computation and is widely used in optimization.
🔹 Krylov Subspace Methods
Approximates dominant eigenvalues of the Hessian, useful for second-order optimization.
Used in Hessian-free optimization for deep learning.
🔹 Stochastic Estimation of the Hessian
Uses mini-batches to approximate Hessian terms efficiently.
Balances accuracy and computational cost.

5.5: Regularization in Neural Networks
Regularization is a key technique in neural network training to prevent overfitting and improve generalization to unseen data. Overfitting occurs when a model becomes too complex and memorizes the training data rather than learning general patterns. 

5.5.1. Parameter Regularization (Weight Decay)
🔹 Why is Regularization Needed?
Neural networks with many parameters can fit training data well but may perform poorly on new data.
Regularization methods add constraints on the model parameters to prevent overfitting.
🔹 L2 Regularization (Weight Decay)
The most common form of regularization is L2 regularization, which adds a penalty on the squared values of the weights: Ereg =E+ λ/2∑wj2
​λ is the regularization parameter (controls the penalty strength).
L1 Regularization (Sparsity)
Instead of squaring the weights, L1 regularization penalizes their absolute values:
𝐸reg =𝐸+𝜆∑∣𝑤𝑗∣
Effect: Encourages sparse weights, meaning many weights become zero.

5.5.2. Early Stopping
🔹 Idea: Stop Training at the Right Time
Instead of modifying the objective function, early stopping prevents overfitting by stopping training before the model memorizes noise.
The network is trained while monitoring validation loss:
If validation loss starts increasing while training loss keeps decreasing, stop training.
🔹 Practical Implementation
Split data into training set and validation set.
Train the network and track validation loss at each epoch.
Stop training when validation loss starts increasing.

5.5.3. Dataset Augmentation
🔹 Key Idea: Create More Training Data
Overfitting happens when there’s too little data. A simple fix is to artificially generate more training samples.
🔹 Methods for Data Augmentation
For images:
Random cropping, flipping, rotation, zooming, color adjustments.
For text:
Synonym replacement, paraphrasing, back translation.
For speech/audio:
Time stretching, pitch shifting, adding noise.

5.5.3. Dataset Augmentation
🔹 Key Idea: Create More Training Data
Overfitting happens when there’s too little data. A simple fix is to artificially generate more training samples.
🔹 Methods for Data Augmentation
For images:
Random cropping, flipping, rotation, zooming, color adjustments.
For text:
Synonym replacement, paraphrasing, back translation.
For speech/audio:
Time stretching, pitch shifting, adding noise.
🔹 Why It Works?
Helps the model generalize better by learning from a more diverse dataset.
Makes the model invariant to small changes in the input.
5.5.4. Noise in the Inputs and Weights
🔹 Injecting Noise to Improve Generalization
Adding random noise to the inputs or weights forces the model to become robust.
🔹 Methods
Adding noise to inputs (input corruption):
x ′=x+Gaussian noise
Encourages robustness to minor variations in input data.
Similar to data augmentation.
Adding noise to weights:
During training, small random values are added to the weights.
Prevents the model from relying on specific neurons too much.
🔹 Dropout (A Special Case of Noise)
Randomly disables neurons during training.
Prevents neurons from becoming too dependent on each other

5.5.5. Ensemble Methods
- Instead of relying on a single trained model, train multiple models and average their predictions.
Types of Ensemble Methods
Bagging (Bootstrap Aggregating): Train multiple independent networks on slightly different data subsets. Example: Random Forest (for decision trees).
Averaging multiple trained models: Train multiple models and take the average of their predictions.
Dropout as an Ensemble: Dropout can be seen as training many sub-networks, which are then combined.

5.5.6. Convolutional Neural Networks (CNNs) as Regularizers
🔹 CNNs Impose Structural Constraints
Fully connected networks have too many parameters, leading to overfitting.
CNNs naturally reduce the number of parameters by sharing weights in local regions.
🔹 Why CNNs Help?
Local Receptive Fields: Neurons only process small patches of input.
Weight Sharing: The same filter slides across the image, reducing parameters.
Translation Invariance: Helps detect patterns regardless of location.

5.5.7. Tangential Regularization
🔹 Regularization Based on Data Manifold
In real-world data, meaningful variations often lie on a lower-dimensional manifold.
Tangential regularization encourages the network to be invariant to small transformations along this manifold.
🔹 Implementation
Small transformations of the input should not change the output significantly.
Example: If a rotated image still represents the same object, the network should predict the same label.

5.6: Mixture Density Networks (MDNs)
Mixture Density Networks (MDNs) combine neural networks with mixture models to model complex, multimodal conditional probability distributions. Unlike traditional neural networks that predict a single output, MDNs can model situations where multiple outcomes are possible for a given input, making them ideal for tasks with inherent uncertainty or ambiguity.
An MDN predicts a probability distribution over possible outputs rather than a single deterministic output. It consists of:
A neural network that outputs the parameters of a mixture model.
A mixture model, typically a Gaussian Mixture Model (GMM), to represent the conditional probability distribution.
nput Layer: Processes input features as in standard neural networks.
Hidden Layers: Extract features and patterns.
Output Layer: Produces parameters for the mixture model, including:
Mixing coefficients 𝜋𝑘 : Probabilities that sum to 1.
Means 𝜇𝑘 : Centers of the Gaussian components.
Variances 𝜎𝑘2 : Spread of each Gaussian component.
They output a probability distribution rather than a single value, allowing them to handle multimodal data distributions effectively.
Training involves maximizing the likelihood of the observed data under the mixture model, typically using gradient-based methods.


5.7: Bayesian Neural Networks (BNNs)
Bayesian Neural Networks (BNNs) provide a probabilistic approach to neural network learning by placing probability distributions over network weights rather than using fixed values. This helps model uncertainty in predictions, making BNNs useful for tasks requiring confidence estimation, such as medical diagnosis and robotics

5.7.1. Inference in Bayesian Neural Networks
🔹 What is Bayesian Inference in Neural Networks?
Instead of finding a single optimal set of weights w, Bayesian inference treats the weights as random variables with a probability distribution.
The goal is to compute the posterior distribution over weights given the data

p(w) is the prior distribution over weights.
p(D∣w) is the likelihood (how well the network fits the data).
p(D) is the evidence (normalization factor).

5.7.2. Laplace Approximation and Posterior Distribution
🔹 Laplace Approximation: A Simple Bayesian Approach
Instead of computing the exact posterior, we approximate it with a Gaussian centered around the maximum a posteriori (MAP) estimate 𝑤MAP
​Steps in the Laplace Approximation
Find 𝑤MAP , which maximizes the posterior
Compute the Hessian matrix H of the negative log-posterior at 𝑤MAP
​Approximate the posterior as  the posterior distribution is modeled as a Gaussian centered at 𝑤MAP with covariance inversely proportional to the Hessian.
 
​5.7.3. Predictive Distribution and Bayesian Model Averaging
🔹 The Predictive Distribution
Instead of predicting a single output 𝑦∗ for a given input 𝑥∗, we compute the full predictive distribution:

 Bayesian Model Averaging (BMA)
Standard neural networks use one weight configuration.
BNNs average predictions over multiple possible weight configurations, leading to better generalization
​
Summary
BNNs provide uncertainty estimates by treating weights as probability distributions.
Laplace Approximation simplifies Bayesian inference but assumes Gaussian distributions.
Bayesian Model Averaging improves prediction reliability by considering multiple possible weight values.












