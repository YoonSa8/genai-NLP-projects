Sparse Kernel Machines
Kernel-based learning algorithms often require evaluating kernel functions for all training data points, leading to computational inefficiencies. Sparse kernel machines address this by using a subset of data points, reducing computational cost.
The two main methods discussed:
Support Vector Machines (SVMs): Based on maximum margin classification.
Relevance Vector Machines (RVMs): Bayesian formulation that leads to sparser solutions and provides probabilistic outputs.

7.1. Maximum Margin Classifiers
We begin our discussion of support vector machines by returning to the two-class classification problem using linear models of the form y(x) = wTÏ†(x) + b

The training set consists of input vectors xn with corresponding labels ğ‘¡ğ‘›âˆˆ{âˆ’1,1}
The classification decision is based on the sign of ğ‘¦(x).

The goal is to find a hyperplane that maximizes the margin, which is the distance between the closest points (support vectors) and the decision boundary.
- If the data is linearly separable, there exists at least one hyperplane that classifies all points correctly
- To solve this constrained optimization, we introduce Lagrange multipliers Setting the derivatives to zero, Substituting ğ‘¤ into the Lagrangian gives the dual formulation
** Lagrange multipliers if decision boundary is linear then you can call g(x) if you subtract it and solve it and multiply it to the lagrange we will have the solutions 
The final classifier is:ğ‘¦(ğ‘¥)=âˆ‘ğ›¼ğ‘›ğ‘¡ğ‘›ğ‘˜(ğ‘¥,ğ‘¥ğ‘›)+ğ‘
			    ğ‘›âˆˆğ‘†

this means that:
If ğ›¼ğ‘›>0, then ğ‘¡ğ‘›ğ‘¦(ğ‘¥ğ‘›)=1 (these are support vectors).
If ğ›¼ğ‘›=0, then ğ‘¥ğ‘› does not affect the decision boundary

7.1.1 Overlapping class distributions
In real-world cases, classes often overlap.
we introduce slack variables, Î¾n >= 0 where
n = 1, . . . , N, with one slack variable for each training data point 
These are defined by Î¾n = 0 for data points that are on or inside the correct margin boundary and Î¾n = |tn âˆ’ y(xn)| for other points. Thus a data point that is on the decision boundary y(xn) = 0 will have Î¾n = 1,

The new optimization problem is: 
min 1/2âˆ¥ğ‘¤âˆ¥^2+ğ¶âˆ‘ğœ‰ğ‘›
ğ‘¤,ğ‘
The C parameter balances:
Margin size (âˆ¥ğ‘¤âˆ¥^2)
Misclassification penalty (âˆ‘ğœ‰ğ‘›)
The dual problem remains similar, but now:0â‰¤ğ›¼ğ‘›â‰¤ğ¶
Kernel Trick: Instead of explicitly mapping data to high-dimensional feature spaces, SVMs use a kernel function:
ğ‘˜(ğ‘¥,ğ‘¥â€²)=ğœ™(ğ‘¥)^ğ‘‡ğœ™(ğ‘¥â€²) allowing computations to remain in input space.

7.1.2: Relation to Logistic Regression
Error Functions in SVM and Logistic Regression
SVM uses the hinge loss function:
ESV(yt)=[1âˆ’yt]+ 
where [z] +means max(0, z).
If ytâ‰¥1, no loss is incurred.
If yt<1, the loss increases linearly.
Logistic Regression uses the logistic loss function:
ğ¸ğ¿ğ‘…(ğ‘¦ğ‘¡)=ln(1+ğ‘’^âˆ’ğ‘¦ğ‘¡)
This function is smooth and differentiable.
Unlike SVM, it gives probabilistic outputs.
The hinge loss is an approximation of the misclassification error, while logistic loss is a smooth function.

Regularization
Both methods use regularization to prevent overfitting both typically uses L2 regularization

SVM has no Probabilistic Outputs

7.1.3 Multiclass SVMs
Since SVMs are naturally binary classifiers, multiclass classification is handled by:
One-vs-Rest (OvR): Train a separate SVM for each class.
- using the decisions of the individual classifiers can lead to inconsistent results in which an input is assigned to multiple classes simultaneously.
- the training setsare imbalanced.
One-vs-One (OvO): Train an SVM for each pair of classes.
- Another approach is to train K(Kâˆ’1)/2 different 2-class SVMs on all possible pairs of classes, and then to classify test points according to which class has the highest
number of â€˜votesâ€™,
Error-correcting Output Codes: Combine binary classifiers with redundancy.
- The K classes themselves are represented as particular sets of responses from the two-class classifiers chosen, and together with a suitable decoding scheme, this gives robustness to errors and to ambiguity in the outputs of the individual classifiers.

7.1.4 SVMs for regression
Standard Regression: This approach does not enforce sparsity, meaning all data points contribute to the prediction.

SVR Solution: Instead of minimizing squared errors, SVR introduces an Îµ-insensitive loss function, allowing some flexibility in the error

The Îµ-Insensitive Loss Function
In SVR, we define a "tube" of width Îµ around the target values.
Predictions within this Îµ-margin incur zero error.
Only points outside the Îµ-boundary contribute to the loss.
This means:
If the prediction y(x) is within ğœ– of ğ‘¡, no penalty is applied.
If the error exceeds ğœ–, it grows linearly (instead of quadratically like in least squares).


The error function for support vector regression can then be written as
Câˆ‘(Î¾n +Î¾n) +1/2||w||2
n=1
where:
ğœ‰ğ‘› and ğœ‰ğ‘› are slack variables for points above and below the tube.
ğ¶ controls the trade-off between margin width and error penalty

An alternative Î½-SVR method fixes the fraction of points that lie outside the margin instead of setting Îµ manually.
where Î½ controls:
The fraction of support vectors.
The fraction of points violating the Îµ-boundary.

7.1.5 Computational learning theory
Historically, support vector machines have largely been motivated and analysed
using a theoretical framework known as computational learning theory, also sometimes
called statistical learning theory The goal of the PAC framework is to understand how large a data set needs to be in order to give good generalization. It also gives bounds for the computational cost of learning, although we do not consider these here.

Suppose that a data set D of size N is drawn from some joint distribution p(x, t)
where x is the input variable and t represents the class label, and that we restrict
attention to â€˜noise freeâ€™ situations in which the class labels are determined by some
(unknown) deterministic function t = g(x). In PAC learning we say that a function
f (x;D), drawn from a space F of such functions on the basis of the training set
D, has good generalization if its expected error rate is below some pre-specified
threshold , so that
Ex,t [I (f (x;D) != t)] < ğœ–

in other words they strongly over-estimate
the size of data sets required to achieve a given generalization performance. For this
reason, PAC bounds have found few, if any, practical applications.

7.2. Relevance Vector Machines
is a Bayesian sparse kernel technique for regression and classification that shares many of the characteristics of the SVM whilst avoiding its principal limitations. Additionally, it typically leads to much sparser models resulting in correspondingly faster performance on test data whilst maintaining comparable generalization error

7.2.1 RVM for regression
RVM is based on a linear model similar to standard regression, but with a modified prior distribution that enforces sparsity.
p(t|x,w, Î²) = N(t|y(x), Î²^âˆ’1)
where Î² = Ïƒâˆ’2 is the noise precision (inverse noise variance), and the mean is given by a linear model of the form
y(x) =âˆ‘wiÏ†i(x) = wTÏ†(x)
     i=1
with fixed nonlinear basis functions Ï†i(x), which will typically include a constant term so that the corresponding weight parameter represents a â€˜biasâ€™.

Bayesian Framework: Introducing Sparsity
Key idea: Instead of using a fixed regularization term, RVM places a prior on the weights: ğ‘(ğ‘¤âˆ£ğ›¼)=âˆğ‘(ğ‘¤ğ‘–âˆ£0,ğ›¼ğ‘–^âˆ’1)
Each weight ğ‘¤ğ‘– has an individual precision parameter ğ›¼ğ‘–.
This automatically prunes irrelevant basis functions (leading to sparsity).
Many ğ›¼ğ‘– values go to infinity, forcing corresponding ğ‘¤ğ‘– to be zero

Given the prior and the likelihood function, we compute the posterior distribution for the weights
The hyperparameters ğ›¼ and ğ›½ are learned using Type-II Maximum Likelihood (Evidence Approximation).
RVM provides uncertainty estimates, making it useful for probabilistic inference.

7.2.2: Analysis of Sparsity in RVM
The sparsity mechanism in RVM arises due to the Bayesian automatic relevance determination (ARD) framework, which selectively removes basis functions that contribute little to the model.

Why Does Sparsity Occur in RVM?
In SVMs, sparsity occurs because only support vectors have nonzero weights.
In RVMs, sparsity is stronger because the Bayesian framework allows automatic pruning of many weights ğ‘¤ğ‘– to zero.
This happens because each weight ğ‘¤ğ‘– has its own precision parameter ğ›¼ğ‘– n the prior
When optimizing ğ›¼ using evidence maximization, many ğ›¼ğ‘– values become very large (â†’âˆ), forcing the corresponding weights ğ‘¤ğ‘– to zero.

If a basis function ğœ™ğ‘–(ğ‘¥) is poorly aligned with ğ‘¡, then setting ğ›¼ğ‘– â†’âˆ increases the likelihood of the data.
This effectively removes that basis function from the model.

7.2.3: RVM for Classification
RVM extends to classification by replacing the Gaussian likelihood (regression) with a Bernoulli likelihood.

Bayesian Inference for RVM Classification
Instead of solving directly, we approximate the posterior using Laplaceâ€™s approximation.

The posterior distribution for w is approximated by a Gaussian:
p(wâˆ£t,X,Î±)â‰ˆN(wâˆ£m,Î£)
where:
Mean: m=argmax p(wâˆ£t,X,Î±) (found using optimization).
Covariance:Î£=(A+Î¦^T*RÎ¦)^âˆ’1
A=diag(Î±) is the prior precision matrix.
R is a diagonal matrix with elements: ğ‘…ğ‘›ğ‘›=ğœ(ğ‘¦ğ‘›)(1âˆ’ğœ(ğ‘¦ğ‘›))
This comes from the Hessian of the log-likelihood, ensuring proper variance estimation.

Iterative Re-Estimation of Hyperparameters 
ğ›¼ Just like in RVM regression, the hyperparameters Î±i
are updated as:ğ›¼ğ‘–new=ğ›¾ğ‘–/ğ‘šğ‘–^2
where:ğ›¾ğ‘–=1âˆ’ğ›¼ğ‘–Î£ğ‘–ğ‘–
Effect of Î± update:
If ğ›¾ğ‘– is small â†’ ğ›¼ğ‘– grows large â†’ ğ‘¤ğ‘– is forced to zero.
This removes irrelevant basis functions â†’ leads to sparsity.

Making Predictions with RVM Classifier
After training, the predictive probability for a new input ğ‘¥ is computed as: ğ‘(ğ‘¡=1âˆ£ğ‘¥,ğ‘¡,ğ‘‹,ğ›¼)=ğœ(ğ‘šğ‘‡ğœ™(ğ‘¥))
This gives a probabilistic output, unlike SVMs, which only provide a hard decision.

































































































































































