Mixture Models and the EM Algorithm 
This chapter introduces mixture models, a powerful statistical tool used for modeling complex probability distributions. The core idea behind mixture models is the use of latent variables, which allow us to express complicated distributions in terms of simpler components.
Latent variables are variables that can only be inferred indirectly through a mathematical model from other observable variables that can be directly observed or measured.

9.1 K-Means Clustering
1. Introduction to Clustering
The goal of clustering is to group a set of data points into distinct categories or "clusters" based on similarity. this section, introduces K-means clustering, a widely used unsupervised learning algorithm for partitioning data into ğ¾ clusters.
Given Data:
A dataset {x1,x2,...,xN} containing N observations, where each ğ‘¥ğ‘›â€‹ is a D-dimensional Euclidean vector.
The goal is to divide these data points into ğ¾ distinct clusters, assuming ğ¾ is known beforehand.
The assumption is that points in the same cluster should be closer to each other than to points in different clusters.

2. Defining Cluster Centers (ğœ‡ğ‘˜ )
To formalize the clustering concept, the author introduces prototypes or cluster centers (ğœ‡ğ‘˜) for each of the ğ¾ clusters.
The ğœ‡ğ‘˜ values represent the centroids (mean points) of each cluster.
The goal is to assign each data point to the cluster with the nearest ğœ‡ğ‘˜ and adjust ğœ‡ğ‘˜  iteratively to minimize an objective function.

3. Objective Function for K-Means (Distortion Measure)
The objective function in K-means is defined as:
J= âˆ‘  âˆ‘ rnkâˆ¥xnâˆ’Î¼kâˆ¥^2
  n=1k=1
where: 
- ğ‘Ÿğ‘›ğ‘˜âˆˆ{0,1} is a binary assignment variable (1-of-K coding).
- If data point ğ‘¥ğ‘›  belongs to cluster k, then ğ‘Ÿğ‘›ğ‘˜=1, otherwise ğ‘Ÿğ‘›ğ‘˜=0
- The goal of K-means is to minimize ğ½, which represents the total squared Euclidean distance between each data point and its assigned cluster center.

4. The K-Means Algorithm (Two-Step Iterative Process)
1. The K-means algorithm iteratively minimizes ğ½ using two alternating steps:
Assignment Step (E-Step equivalent):
Each data point is assigned to the cluster whose center ğœ‡ğ‘˜ is closest in Euclidean distance
rnk={ 1, ifÂ k=arg min j âˆ¥xnâˆ’Î¼jâˆ¥^2 otherwise 0
This step is equivalent to classifying each data point based on nearest-neighbor rules.
2. Update Step (M-Step equivalent):
The cluster centers ğœ‡ğ‘˜ are updated based on the new assignments by computing the mean of all data points assigned to that cluster
Each cluster center is updated to be the mean (centroid) of all points assigned to it.

5. Convergence and Properties of K-Means
These two steps are repeated iteratively until assignments no longer change (or a maximum number of iterations is reached).
The algorithm is guaranteed to converge because each iteration reduces the objective function J.
However, convergence may be to a local minimum, not necessarily the global optimum.

6. Limitations of K-Means
some key limitations of K-means:
Sensitivity to initialization: Poor initialization can lead to slow convergence or suboptimal solutions.
		      A common heuristic is to initialize as ğœ‡ğ‘˜ a random subset of the data points.

Not robust to outliers: K-means minimizes squared distances, which means it is highly affected by outliers.
			K-medoids is an alternative method that reduces this sensitivity.
K-medoids : 	

Fixed K assumption: The number of clusters ğ¾ must be pre-defined, which may not always be known beforehand.
Elbo method 
Assumes spherical clusters: K-means works well when clusters are compact and well-separated, but it fails for complex cluster shapes.

9.1.1 Image Segmentation and Compression
1. Image Segmentation
Definition: Image segmentation is the process of dividing an image into meaningful regions (e.g., objects or similar textures).
K-means Approach:
Each pixel in an image can be treated as a data point in a 3-dimensional RGB space.
The algorithm clusters pixels based on color similarity.
Pixels in the same cluster are assigned the same mean color (ğœ‡ğ‘˜ ), reducing the number of unique colors in the image.
This allows for segmentation based purely on color intensity, without considering spatial relationships.

2. Image Compression Using K-Means (Vector Quantization)
Another key application of K-means is in lossy data compression.
Traditional Images:
A digital image consists of ğ‘ pixels, each represented by an RGB triplet (24 bits per pixel).

Compression Idea:
Instead of storing all pixel values, we store only the ğ¾ cluster centers and assign each pixel to its closest center.
This reduces the number of unique colors in the image.

Bit Savings Calculation:
Without compression: 24N bits needed.
With K-means: ğ‘log2 K+24K bits needed.
Compression ratio Nlog2 K+24K/24N 
-p.s the log base is 2 
Higher ğ¾ means better quality but lower compression. Optimal ğ¾ balances both.


9.2 Mixtures of Gaussians
1. Introduction to Gaussian Mixture Models (GMMs)
A Gaussian Mixture Model (GMM) is an extension of the single Gaussian distribution that allows modeling complex, multimodal data distributions. Unlike a single Gaussian, which assumes the data is generated from one normal distribution, a GMM assumes that data points are drawn from multiple Gaussian components.
Each Gaussian component is parameterized by:
Mean (ğœ‡ğ‘˜ ): The center of the Gaussian.
Covariance matrix (Î£ğ‘˜ ): Defines the shape and spread of the Gaussian distribution.
Mixing coefficient (ğœ‹ğ‘˜): The probability that a randomly chosen data point belongs to component ğ‘˜â€‹

2. Role of Latent Variables in GMMs
To formalize the mixture model, we introduce latent variables ğ‘§, which indicate which Gaussian component generated each data point.
ğ‘§ follows a categorical distribution (one-hot encoded), meaning: ğ‘(ğ‘§ğ‘˜=1)=ğœ‹ğ‘˜
The conditional distribution of ğ‘¥ given ğ‘§ğ‘˜ =1 is Gaussian: p(xâˆ£zk=1)=N(xâˆ£Î¼k,Î£k)
The joint distribution of ğ‘¥ and ğ‘§ is: p(x,z)=p(z)p(xâˆ£z)
The marginal distribution of ğ‘¥ is obtained by summing over all possible ğ‘§
 
3. Responsibilities in GMMs
The responsibility ğ›¾(ğ‘§ğ‘˜) is the posterior probability that a data point ğ‘¥ belongs to Gaussian component ğ‘˜, given by Bayes' theorem
This term plays a key role in the Expectation-Maximization (EM) algorithm, as it determines the degree to which each data point influences each Gaussian component.

9.2.1 Maximum Likelihood Estimation (MLE) for GMMs
Singularities in Maximum Likelihood for GMMs
Unlike a single Gaussian, maximizing the likelihood for a GMM presents a major issue: singularities.
If one Gaussian collapses onto a single data point, its variance Î£ğ‘˜ approaches zero, causing the likelihood to diverge to infinity.
This happens because the probability density of a Gaussian is inversely proportional to its variance, meaning as Î£k â†’0, p(x) explodes.
To prevent this, heuristics such as regularization or Bayesian priors are used.
 Identifiability Issue in GMMs
Since GMMs allow permutation of components, any set of parameters can be arranged in K! different ways that produce the same likelihood.
This makes parameter interpretation ambiguous, though it does not affect the quality of density estimation.

9.2.2 Expectation-Maximization (EM) for Gaussian Mixtures
1. Why Use EM?
The log-likelihood function contains a log-sum, making direct maximization intractable.
Instead, EM iteratively improves the likelihood in two steps:
E-step: Compute the expected complete-data log-likelihood.
M-step: Maximize this expectation with respect to model parameters.

2. E-Step (Expectation Step)
Using the current parameter estimates (ğœ‹ğ‘˜,ğœ‡ğ‘˜,Î£ğ‘˜), compute the responsibilities This represents the soft assignment of each data point to each Gaussian component.

3. M-Step (Maximization Step)
Given the responsibilities ğ›¾(ğ‘§ğ‘›ğ‘˜), update the parameters:
Mean update (weighted average of data points assigned to cluster ğ‘˜)
Covariance update (weighted covariance matrix)
Mixing coefficient update (fraction of data points assigned to cluster ğ‘˜)

4. Convergence of EM
EM guarantees an increase in log-likelihood at each step.
The algorithm stops when the change in log-likelihood is below a threshold.
However, EM only finds local optima, meaning different initializations can lead to different results.

9.3 An Alternative View of the EM
The EM algorithm maximizes the likelihood function when the model contains unobserved (latent) variables.
Instead of working with the marginal likelihood p(Xâˆ£Î¸), which involves a log-sum, EM considers the complete-data likelihood p(X,Zâˆ£Î¸), where ğ‘ represents latent variables.

2. General Framework of EM
Given:
Observed data: X={x1,x2,...,xN}
Latent variables:  Z={z1,z2,...,zN}
Model parameters: ğœƒ

The log-likelihood function is:
ln p(Xâˆ£Î¸)=ln âˆ‘p(X,Zâˆ£Î¸)
Since summation is inside the log, direct maximization is difficult. Instead, EM works as follows:

E-Step (Expectation Step)
Compute the expected log-likelihood over the latent variables, given the current estimate of the parameters: Q(Î¸,Î¸old)= âˆ‘p(Zâˆ£X,Î¸old)lnp(X,Zâˆ£Î¸)
where p(Zâˆ£X,Î¸old) is the posterior probability of the latent variables.

M-Step (Maximization Step)
Find the new parameters that maximize the expected log-likelihood:
Î¸new =arg maxQ(Î¸,Î¸,old)
Each iteration ensures that the likelihood never decreases.

EM for Maximum A Posteriori (MAP) Estimation
If a prior p(Î¸) is introduced, EM can be used for MAP estimation instead of MLE.
The M-step maximizes: Q(Î¸,Î¸old)+lnp(Î¸)
	This avoids overfitting and prevents singularities in GMMs.

9.3.1 Gaussian Mixtures Revisited
Here, the author re-examines Gaussian Mixture Models (GMMs) using the alternative EM interpretation.

Complete-Data Log-Likelihood
If we had full knowledge of which Gaussian component generated each data point, the log-likelihood would simplify to: ln p(X,Z|Î¼,Î£,Ï€) =âˆ‘ âˆ‘ znk {ln Ï€k + lnN(xn|Î¼k,Î£k)}
					n=1k=1
where ğ‘§ğ‘›ğ‘˜ is a binary variable indicating the component assignment

Expectation Step (E-Step)
Compute the expected value of ğ‘§ğ‘›ğ‘˜
â€‹
Maximization Step (M-Step)
The new parameter estimates Î¼k, âˆ‘k, Ï€k

Key Takeaways
EM is equivalent to MLE for GMMs, but the use of latent variables simplifies the optimization.
The responsibilities softly assign data points to Gaussian components


9.3.2 Relation to K-Means
 compares K-means and EM, highlighting their similarities and differences:
1. Similarities
Both algorithms assign data points to clusters.
Both involve iterative optimization with alternating steps.
2. Differences
K-means uses hard assignments (ğ‘Ÿğ‘›ğ‘˜âˆˆ{0,1}), while EM uses soft assignments (ğ›¾(ğ‘§ğ‘›ğ‘˜).

K-means minimizes squared Euclidean distance, while EM maximizes log-likelihood.

K-means assumes equal variance for clusters, while EM estimates full covariance matrices.

3. K-Means as a Special Case of EM
If we assume Gaussian clusters with equal, small variances, EM reduces to K-means.
ğ›¾(ğ‘§ğ‘›ğ‘˜)â†’1(forÂ closestÂ cluster)
Thus, K-means is a limit case of EM for Gaussian mixtures.

9.3.3 Mixtures of Bernoulli Distributions
extends mixture models to binary data, using Bernoulli Mixture Models.
Each data point ğ‘¥ğ‘› is a binary vector (ğ‘¥ğ‘–âˆˆ{0,1}).
The EM algorithm remains the same but with Bernoulli parameters instead of Gaussian ones.

9.3.4 EM for Bayesian Linear Regression
The author applies EM to Bayesian regression, where model parameters have priors.
Latent variable: The regression weights ğ‘¤.
E-step: Compute posterior mean and covariance of ğ‘¤.
M-step: Maximize the marginal likelihood to update hyperparameters ğ›¼,ğ›½

9.4: The EM Algorithm in General
1. General Formulation of the EM Algorithm
- The EM algorithm is a general optimization technique used to find maximum likelihood (ML) estimates for models with latent (unobserved) variables.
- The goal is to maximize the marginal likelihood
2. The EM Algorithm: Two-Step Iterative Approach
The EM algorithm circumvents the difficulty of optimizing the marginal likelihood by instead maximizing an expected complete-data log-likelihood in two alternating steps:

Step 1: Expectation Step (E-Step)
Compute the expected complete-data log-likelihood under the posterior distribution of the latent variables p(Zâˆ£X,Î¸old):
ğ‘„(Î¸,Î¸old)= âˆ‘p(Zâˆ£X,Î¸old)ln p(X,Zâˆ£Î¸)
This step fills in the missing information using the posterior over ğ‘.
Step 2: Maximization Step (M-Step)
Find the new parameter estimates by maximizing ğ‘„(ğœƒ,ğœƒold)
Î¸ new=arg max Q (Î¸,Î¸ old)
This step updates ğœƒ in a way that increases the likelihood.
Iterate Until Convergence
- The E-step and M-step are repeated until convergence, meaning the parameters ğœƒ stop changing significantly or the log-likelihood stabilizes.

3. Why EM Works: Theoretical Justification
To understand why EM increases the log-likelihood, the author introduces the Jensenâ€™s Inequality and the variational lower bound.

1. Jensenâ€™s Inequality and the Log-Sum Problem
The difficulty in directly maximizing the log-likelihood comes from the log-sum structure
Since the log function is concave, we can apply Jensenâ€™s inequality:
lnâˆ‘ğ‘¤ğ‘– ğ‘“ğ‘–â‰¥âˆ‘ğ‘¤ğ‘– ln ğ‘“ğ‘– ,forÂ weightsÂ wiÂ summingÂ toÂ 1
This provides a lower bound on the log-likelihood that EM maximizes.

2. Lower Bound Interpretation (Variational View)
Define a distribution ğ‘(ğ‘) over latent variables. 
lnp(Xâˆ£Î¸)= âˆ‘q(Z)ln p(X,Zâˆ£Î¸)/q(Z) + âˆ‘ q(Z)ln q(Z)/p(Zâˆ£X,Î¸)
â€‹-The first term is the variational lower bound (which EM maximizes).
- The second term is the KL divergence between q(Z) and the true posterior ğ‘(ğ‘âˆ£ğ‘‹,ğœƒ), which is always non-negative.

Thus, EM guarantees non-decreasing likelihood because each iteration tightens this lower bound.
 
4. EM Convergence and Guarantees
1. EM Increases the Likelihood at Each Iteration
- At each iteration, the M-step maximizes ğ‘„(Î¸,Î¸old), which is a lower bound on the true log-likelihood.
- This ensures: ln p(Xâˆ£Î¸new )â‰¥ln p(Xâˆ£Î¸ old) meaning EM never decreases the likelihood.

2. EM Does Not Guarantee a Global Maximum
- The log-likelihood often has multiple local maxima.
- EM may get stuck in a local maximum depending on initialization.

3. Stopping Criteria for EM
The algorithm is considered converged when:
- The log-likelihood changes very little between iterations.
- The parameter updates become small.
- A fixed number of iterations is reached.

5. Extensions and Applications of EM
1. Generalized EM (GEM)
- In the standard EM algorithm, the M-step fully maximizes Q(Î¸).
- In Generalized EM (GEM), we increase (but do not fully maximize) Q(Î¸).
- GEM is useful when exact maximization is difficult, such as in variational inference.

2. EM in Bayesian Inference
EM can also be used in Bayesian models, where instead of MLE, we maximize a posterior:
Q(Î¸,Î¸old)+lnp(Î¸)
- This regularizes the estimates and prevents overfitting.

3. Applications of EM
The EM algorithm is widely used in machine learning and statistics, including:
- Gaussian Mixture Models (GMMs) (soft clustering)
- Hidden Markov Models (HMMs) (speech recognition)
- Topic modeling (Latent Dirichlet Allocation, LDA)
- Missing data imputation



